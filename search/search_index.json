{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the NOMAD documentation for Computational Materials Scientists, where you can find information about how to benefit from using NOMAD to upload and publish data, find and download data, perform complex queries and apply Data Science techniques, as well as develop the current infrastructure for parsers, normalizers, filters and searches, and visualizations. Each main section on the left is self-contained, so that it can independently be browsed through to learn one of the aforementioned aspects. </p> <p>NOMAD is a free open-source data management platform for Materials Science which follows the F.A.I.R. (Findable, Accessible, Interoperable, and Reusable) principles [@Wilkinson2016;@goFairWeb]. This documentation page is a part of the more general NOMAD documentation. We will widely refer to it for some specific terms and information. </p> <p>Main contributors</p> <p>Dr. Nathan Daelman, ndaelman@physik.hu-berlin.de</p> <p>Dr. Alvin Noe Ladines, alvin.noe.ladines@physik.hu-berlin.de</p> <p>Dr. Jos\u00e9 M. Pizarro, jose.pizarro@physik.hu-berlin</p> <p>Dr. Joseph F. Rudzinski, joseph.rudzinski@physik.hu-berlin.de</p>"},{"location":"contact/","title":"Contact","text":"<p>NOMAD is an open source project that warmly welcomes community projects, contributions, suggestions, bug fixes, and constructive feedback. The contributors to this documentation page are part of the FAIRmat Area C - Theory and Computations team.</p> <p>You can reach us by different channels. You can send as directly an email to the main contributors list:</p> <p>Main contributors</p> Name E-mail Topics Dr. Nathan Daelman nathan.daelman@physik.hu-berlin.de DFT, parsers, normalizers Dr. Alvin Noe Ladines alvin.noe.ladines@physik.hu-berlin.de Parsers, workflows Dr. Jos\u00e9 M. Pizarro jose.pizarro@physik.hu-berlin.de GW, DMFT, BSE, parsers, workflows, normalizers Dr. Joseph F. Rudzinski (Coordinator) joseph.rudzinski@physik.hu-berlin.de MD, parsers, workflows, normalizers <p>Alternatively, you can also:</p> <ul> <li>Open an issue in the general NOMAD Github project, or in one of the sub-projects related with specific parsers. Our Github profile tags are @ndaelman-hu, @ladinesa, @JosePizarro3, and @JFRudzinski.</li> <li>Write us in the NOMAD MatSci forum. Our tags there are @NateD, @ladinesa, @JosePizarro, and @JFRudzinski. </li> <li>Send an email to support@nomad-lab.eu. Please, add in the subject \"ATTN - Area C\".</li> </ul>"},{"location":"current_and_planned_features/overview/","title":"Overview","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"custom_schemas/overview/","title":"Overview","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"custom_schemas/overview/#workflows-in-nomad","title":"Workflows in NOMAD","text":"<p>Workflows are an important aspect of data management as they enable a systematic organization of the tasks performed during any Materials Science research project. We refer to a workflow as a series of experiments or simulations composed of inputs, outputs, and tasks performed either in serial or in parallel. Each entry in NOMAD has a workflow section, describing how the (meta)data within the entry was generated. Additionally, an \"overarching\" workflow can be generated within its own entry, to define connections between multiple entries (and subworkflows) via references to the corresponding entries and sections.</p> <p>The general schema for a workflow in NOMAD (found under <code>nomad.datamodel.metainfo.workflow</code>) can be represented with the following graph:</p> <p> </p> <p>The NOMAD workflow (blue section in the above image) is section of an entry in the NOMAD Archive. The workflow subsection<code>Task</code> contains information about each of the tasks performed within the workflow. The workflow subsection <code>TaskReference</code> allows to reference other tasks or workflows. Finally, the workflow subsection <code>Link</code> allows to link between tasks and sections within the NOMAD Archive.</p> <p>This documentation will show you:  </p> <ul> <li>A simple tutorial to understand the managing and definition of custom workflows in NOMAD.</li> <li>...</li> </ul>"},{"location":"custom_schemas/overview/#introduction","title":"Introduction","text":"<p>We will use a ficticious example of a simulation workflow, where the files and folder structure is:</p> <p><pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u2514\u2500\u2500 pressure2\n \u00a0\u00a0 \u251c\u2500\u2500 temperature1\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 temperature2\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n \u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n \u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n \u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n \u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n</code></pre> Each of the mainfiles represent an electronic-structure calculation (either DFT, TB, or DMFT) which in turn is then parsed into a singular entry in NOMAD. When dragged into the NOMAD Upload page, these files should generate 8 entries in total. This folder structure presents a typical workflow calculation which can be represented as a provenance graph: <pre><code>graph LR;\n    A((Inputs)) --&gt; B1[DFT];\n    A((Inputs)) --&gt; B2[DFT];\n    subgraph pressure P&lt;sub&gt;2&lt;/sub&gt;\n    B2[DFT] --&gt; C2[TB];\n    C2[TB] --&gt; D21[DMFT at T&lt;sub&gt;1&lt;/sub&gt;];\n    C2[TB] --&gt; D22[DMFT at T&lt;sub&gt;2&lt;/sub&gt;];\n    end\n    D21[DMFT at T&lt;sub&gt;1&lt;/sub&gt;] --&gt; E21([Output calculation P&lt;sub&gt;2&lt;/sub&gt;, T&lt;sub&gt;1&lt;/sub&gt;])\n    D22[DMFT at T&lt;sub&gt;2&lt;/sub&gt;] --&gt; E22([Output calculation P&lt;sub&gt;2&lt;/sub&gt;, T&lt;sub&gt;2&lt;/sub&gt;])\n    subgraph pressure P&lt;sub&gt;1&lt;/sub&gt;\n    B1[DFT] --&gt; C1[TB];\n    C1[TB] --&gt; D11[DMFT at T&lt;sub&gt;1&lt;/sub&gt;];\n    C1[TB] --&gt; D12[DMFT at T&lt;sub&gt;2&lt;/sub&gt;];\n    end\n    D11[DMFT at T&lt;sub&gt;1&lt;/sub&gt;] --&gt; E11([Output calculation P&lt;sub&gt;1&lt;/sub&gt;, T&lt;sub&gt;1&lt;/sub&gt;])\n    D12[DMFT at T&lt;sub&gt;2&lt;/sub&gt;] --&gt; E12([Output calculation P&lt;sub&gt;1&lt;/sub&gt;, T&lt;sub&gt;2&lt;/sub&gt;])</code></pre> Here, \"Input\" refers to the all input information given to perform the calculation (e.g., atom positions, model parameters, experimental initial conditions, etc.). \"DFT\", \"TB\" and \"DMFT\" refer to individual tasks of the workflow, which each correspond to a SinglePoint entry in NOMAD. \"Output calculation\" refers to the output data of each of the final DMFT tasks.</p> <p>The goal of this tutorial is to set up the following workflows:</p> <ol> <li>A <code>SinglePoint</code> workflow for one of the calculations (e.g., the DFT one) in the <code>pressure1</code> subfolder.</li> <li>An overarching workflow entry for each pressure P<sub>i=1,2</sub>, grouping all <code>SinglePoint</code> \"DFT\", \"TB\", \"DMFT at T<sub>1</sub>\", and \"DMFT at T<sub>2</sub>\" tasks.</li> <li>A top level workflow entry, grouping together all pressure calculations.</li> </ol>"},{"location":"custom_schemas/overview/#starting-example-singlepoint-workflow","title":"Starting example: SinglePoint workflow","text":"<p>NOMAD is able to recognize certain workflows in an automatic way, such as the <code>SinglePoint</code> case mentioned above. However, to showcase how to the use workflows in NOMAD, we will \"manually\" construct the SinglePoint workflow, represented by the following provenance graph: <pre><code>graph LR;\n    subgraph SinglePoint\n    A((Input structure)) --&gt; B[DFT];\n    B[DFT] --&gt; C([Output calculation]);\n    end</code></pre></p> <p>To define a workflow manually in NOMAD, we must add a <code>YAML</code> file to the upload folder that contains the relevant input, output, and task information. This file should be named <code>&lt;filename&gt;.archive.yaml</code><sup>1</sup>. In this case, we include the file <code>single_point.archive.yaml</code> with the following content:</p> <pre><code>workflow2:\nname: SinglePoint\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\ntasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at Pressure P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>We can note several things about the content of this file:</p> <ol> <li><code>name</code> keys are optional.</li> <li>The root path of the upload can be referenced with <code>../upload/archive/mainfile/</code>. Starting from there, the original directory tree structure of the upload is maintained.</li> <li><code>inputs</code> reference the section containing inputs of the whole workflow. In this case this is the section <code>run[0].system[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>outputs</code> reference the section containing outputs of the whole workflow. In this case this is the section <code>run[0].calculation[-1]</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>tasks</code> reference the section containing tasks of each step in the workflow. These must also contain <code>inputs</code> and <code>outputs</code> properly referencing the corresponding sections; this will then link inputs/outputs/tasks in the NOMAD Archive. In this case this is a <code>TaskReference</code> to the section <code>workflow2</code> parsed from the mainfile in the path <code>pressure1/dft_p1.xml</code>.</li> <li><code>section</code> reference to the uploaded mainfile specific section. The left side of the <code>#</code> symbol contains the path to the mainfile, while the right contains the path to the section.</li> </ol> <p>This will produce an extra entry with the following Overview content:</p> <p> </p> <p>Note that we are referencing sections which are lists. Thus, in each case we have to be careful to reference the correct section for inputs and outputs (example: a <code>GeometryOptimization</code> workflow calculation will have the \"Input structure\" as <code>run[0].system[0]</code>, while the \"Output calculation\" would also contain <code>run[0].system[-1]</code>, and all intermediate steps must input/output the corresponding section system).</p> <p>We can extend the workflow meta-information by adding the metholodogical input parameters. These are stored in NOMAD in the section path <code>run[0].method[-1]</code>. The new <code>single_point.archive.yaml</code> will be:</p> <pre><code>workflow2:\nname: SinglePoint\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input methodology parameters\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\ntasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at Pressure P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input methodology parameters\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/method/-1'\noutputs:\n- name: Output calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n</code></pre> <p>which in turn produces a similar workflow than before, but with an extra input node:</p> <p> </p>"},{"location":"custom_schemas/overview/#pressure-workflows","title":"Pressure workflows","text":"<p>Now that we know the basics of the workflow <code>YAML</code> schema, let's try to define an overarching workflow for each of the pressures. For this section, we will show the case of P<sub>1</sub>; the extension for P<sub>2</sub> is then a matter of changing names and paths in the <code>YAML</code> files. For simplicity, we will skip referencing to methodologies.</p> <p>Thus, the <code>inputs</code> can be defined as: <pre><code>workflow2:\nname: DFT+TB+DMFT at P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n</code></pre> and there are two <code>outputs</code>, one for each of the DMFT calculations at distinct temperatures: <pre><code>  outputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Now, <code>tasks</code> are defined for each of the methodologies performed (each corresponding to an underlying SinglePoint workflow). To define a valid workflow, each task must contain an input that corresponds to one of the outputs of the previous task. Moreover, the first task should take as input the overall input of the workflow, and the final task should also have as an output the overall workflow output. Then: <pre><code>  tasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/dft_p1.xml#/workflow2'\nname: DFT at P1\ninputs:\n- name: Input structure\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output DFT at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/tb_p1.wout#/workflow2'\nname: TB at P1\ninputs:\n- name: Input DFT at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/calculation/-1'\noutputs:\n- name: Output TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\nname: DMFT at P1 and T1\ninputs:\n- name: Input TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\noutputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/workflow2'\nname: DMFT at P1 and T2\ninputs:\n- name: Input TB at P1 calculation\nsection: '../upload/archive/mainfile/pressure1/tb_p1.wout#/run/0/calculation/-1'\noutputs:\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n</code></pre> We can note here:</p> <ul> <li>The <code>inputs</code> for each subsequent step are the <code>outputs</code> of the previous step.</li> <li>The final two <code>outputs</code> coincide with the <code>workflow2</code> <code>outputs</code>.</li> </ul> <p>This workflow (<code>pressure1.archive.yaml</code>) file will then produce an entry with the following Overview page:</p> <p> </p> <p>Similarly, for P<sub>2</sub> we can upload a new <code>pressure2.archive.yaml</code> file with the same content, except when substituting 'pressure1' and 'p1' by their counterparts. This will produce a similar graph than the one showed before but for 'P2'.</p>"},{"location":"custom_schemas/overview/#the-top-level-workflow","title":"The top-level workflow","text":"<p>After adding the workflow YAML files, our upload folder directory now looks like: <pre><code>.\n\u251c\u2500\u2500 pressure1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p1_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p1.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p1.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure1.archive.yaml\n\u251c\u2500\u2500 pressure2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t1.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 temperature2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dmft_p2_t2.hdf5\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 ...extra auxiliary files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dft_p2.xml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tb_p2.wout\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...extra auxiliary files\n\u251c\u2500\u2500 pressure2.archive.yaml\n\u2514\u2500\u2500 single_point.archive.yaml\n</code></pre> In order to define the general workflow that groups all pressure calculations, we can reference directly the previous <code>pressureX.archive.yaml</code> files as tasks. Still, <code>inputs</code> and <code>outputs</code> must be referenced to their corresponding mainfile and section paths.</p> <p>We then create a new <code>fullworkflow.archive.yaml</code> file with the <code>inputs</code>: <pre><code>workflow2:\nname: Full calculation at different pressures for SrVO3\ninputs:\n- name: Input structure at P1\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\n- name: Input structure at P2\nsection: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\n</code></pre> And <code>outputs</code>: <pre><code>  outputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T1 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T2 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre> Finally, <code>tasks</code> references the previous YAML schemas as follows: <pre><code>  tasks:\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure1.archive.yaml#/workflow2'\nname: DFT+TB+DMFT at P1\ninputs:\n- name: Input structure at P1\nsection: '../upload/archive/mainfile/pressure1/dft_p1.xml#/run/0/system/-1'\noutputs:\n- name: Output DMFT at P1, T1 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature1/dmft_p1_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P1, T2 calculation\nsection: '../upload/archive/mainfile/pressure1/temperature2/dmft_p1_t2.hdf5#/run/0/calculation/-1'\n- m_def: nomad.datamodel.metainfo.workflow2.TaskReference\ntask: '../upload/archive/mainfile/pressure2.archive.yaml#/workflow2'\nname: DFT+TB+DMFT at P2\ninputs:\n- name: Input structure at P2\nsection: '../upload/archive/mainfile/pressure2/dft_p2.xml#/run/0/system/-1'\noutputs:\n- name: Output DMFT at P2, T1 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature1/dmft_p2_t1.hdf5#/run/0/calculation/-1'\n- name: Output DMFT at P2, T2 calculation\nsection: '../upload/archive/mainfile/pressure2/temperature2/dmft_p2_t2.hdf5#/run/0/calculation/-1'\n</code></pre></p> <p>This will produce the following entry and its Overview page:</p> <p> </p>"},{"location":"custom_schemas/overview/#automatic-workflows","title":"Automatic workflows","text":"<p>There are some cases where the NOMAD infrastructure is able to recognize certain workflows automatically when processing the uploaded files. The simplest example is any <code>SinglePoint</code> calculation, as explained above. Other examples include <code>GeometryOptimization</code>, <code>Phonons</code>, <code>GW</code>, and <code>MolecularDynamics</code>. Automated workflow detection may require your folder structure to fulfill certain conditions.</p> <p>Here are some general guidelines for preparing your upload folder in order to make it easier for the automatic workflow recognition to work:</p> <ul> <li>Always organize your files in an upwards-downwards structure, i.e., the initial tasks should be upper in the directory tree, while the later tasks lower on it.</li> <li>Avoid having to go up and down between folders if some properties are derived between these files. These situations are very complicated to predict for the current NOMAD infrastructure.</li> <li>Avoid duplication of files in subfolders. If initially you do a calculation A from which a later calculation B is derived and you want to store B in a subfolder, there is no need to copy the A files inside the subfolder B.</li> </ul> <p>The folder structure used throughout this Tutorial is a good example of a clean upload which is friendly and easy to work with when defining NOMAD workflows.</p> <ol> <li> <p><code>&lt;filename&gt;</code> can be any custom name defined by the user, but the file must keep the extension <code>.archive.yaml</code> at the end.\u00a0\u21a9</p> </li> </ol>"},{"location":"custom_schemas/h5md/connectivity/","title":"The connectivity group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of \"connectivity\" information, e.g., to be used in conjunction with a molecular mechanics force field. The connectivity information is stored as tuples in the group <code>/connectivity</code>. The tuples are pairs, triples, etc. as needed and may be either time-independent or time-dependent. As with other elements, connectivity elements can be defined for particular particle groups. However, H5MD-NOMAD focuses on the storage of connectivity elements for the entire system (i.e., the <code>all</code> particles group).</p>"},{"location":"custom_schemas/h5md/connectivity/#standardized-h5md-nomad-connectivity","title":"Standardized H5MD-NOMAD connectivity","text":"<p>The general structure of the <code>connectivity</code> group is as follows:</p> <pre><code>connectivity\n \\-- (bonds): Integer[N_part][2]\n \\-- (angles): Integer[N_part][3]\n \\-- (dihedrals): Integer[N_part][4]\n \\-- (impropers): Integer[N_part][4]\n \\-- (&lt;custom_interaction&gt;): Integer[N_part][m]\n \\-- (particles_group)\n      \\-- ...\n</code></pre> <p><code>N_part</code> corresponds to the number of particles stored in the <code>particles/all</code> group.</p> <ul> <li> <p><code>bonds</code> : a list of 2-tuples specifying the indices of particles containing a \"bond interaction\".</p> </li> <li> <p><code>angles</code> : a list of 3-tuples specifying the indices of particles containing an \"angle interaction\".</p> </li> <li> <p><code>dihedrals</code> : a list of 4-tuples specifying the indices of particles containing a \"dihedral interaction\".</p> </li> <li> <p><code>impropers</code> : a list of 4-tuples specifying the indices of particles containing an \"improper dihedral interaction\".</p> </li> <li> <p><code>&lt;custom_interaction&gt;</code> : a list of m-tuples specifying the indices of particles containing an arbitrary interaction. <code>m</code> denotes the number of particles involved in the interaction.</p> </li> <li> <p><code>particles_group</code> : See below.</p> </li> </ul> <p> Currently only time-independent connectivity elements are supported.</p>"},{"location":"custom_schemas/h5md/connectivity/#the-particles_group-subgroup","title":"The particles_group subgroup","text":"<p>Despite not fully utilizing the organization of arbitrary groups of particles within the <code>particles</code> group, H5MD-NOMAD allows for the user to provide an arbitrary hierarchy of particle groupings, also referred to as a \"topology\", within the <code>connectivity</code> subgroup called <code>particles_group</code>. This information will be used by NOMAD to facilitate visualizations of the system, through the \"topology bar\" in the overview page. The general structure of the topology group is as follows:</p> <pre><code>connectivity\n \\-- particles_group\n      \\-- &lt;group_1&gt;\n      |    \\-- (type): String[]\n      |    \\-- (formula): String[]\n      |    \\-- indices: Integer[]\n      |    \\-- (is_molecule): Bool\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n      |    \\-- (particles_group):\n      |        \\-- ...\n      \\-- &lt;group_2&gt;\n          \\-- ...\n</code></pre> <p>The initial <code>particles_group</code> subgroup, directly under <code>connectivity</code>, is a container for the entire topology. <code>particles_group</code> contains a series of subgroups with arbitrary names, which denote the first level of organization within the topology. The name of each subgroup will become the group label within the NOMAD metadata. Each of these subgroups then contain a series of datasets:</p> <ul> <li> <p><code>type</code> : describes the type of particle group. There exists a list of standardized types: <code>molecule_group</code>, <code>molecule</code>, <code>monomer_group</code>, <code>monomer</code>. However, arbitrary types can be given. We suggest that you 1. use the standardized types when appropriate (note that protein residues should be generically typed as <code>monomer</code>) and 2. use the general format <code>&lt;type&gt;_group</code> for groups of a distinct type (see further description of suggested hierarchy below).</p> </li> <li> <p><code>formula</code> : a \"chemical-like\" formula that describes the particle group with respect to its underlying components. The format for the formula is <code>&lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...</code>, where <code>&lt;child_x&gt;</code> is the name/label of the underlying component, and <code>n_child_x</code> is the number of such components found within this particle group. Example: A particles group containing 100 water molecules named <code>water</code> has the formula <code>water(100)</code>, whereas each underlying water molecule has the standard chemical formula <code>H2O</code>.</p> </li> <li> <p><code>indices</code> : a list of integer indices corresponding to all particles belonging to this group. Indices should correspond to the list of particles stored in the <code>particles/all</code> group.</p> </li> <li> <p><code>is_molecule</code> : indicator of individual molecules (typically with respect to the bond connections defined by a force field).</p> </li> <li> <p><code>custom_dataset</code> : arbitrary additional metadata for this particle group may be given.</p> </li> </ul> <p>Each subgroup may also contain a (nested) <code>particles_group</code> subgroup, in order to subdivide the group of particles into an organizational hierarchy. As with the overall <code>particles_group</code> container, the groups contained within <code>particles_group</code> must not partition the particles within this group (i.e., overlapping or non-complete groupings are allowed). However, particle groups must contain particles already contained within the parent <code>particles_group</code> (i.e., subgroups must be a subset of the grouping at the previous level of the hierarchy).</p> <p>Note that typically the <code>particles_group</code> hierarchy ends at the level of individual particles (i.e., individual particles are not stored, since this information is already contained within the <code>particles</code> group).</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":"<ul> <li>H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD<ul> <li>Overview</li> <li>Introduction to the H5MD storage format<ul> <li>File format</li> <li>Notation and naming</li> <li>Time-dependent data</li> <li>Time-independent data</li> <li>Storage order of arrays</li> <li>Storage of particles and tuples lists</li> </ul> </li> <li>The root level</li> <li>The H5MD Group</li> <li>The particles group<ul> <li>Standardized H5MD elements for particles group</li> </ul> </li> </ul> </li> <li>TODO can we make these admonitions indented somehow or more obviously connected with the members of this list?<ul> <li>Standardized H5MD-NOMAD elements for particles group</li> <li>Non-standard elements in particles group</li> <li>The simulation box subgroup</li> <li>The observables group<ul> <li>H5MD-NOMAD observables</li> </ul> </li> <li>The connectivity group<ul> <li>Standardized H5MD-NOMAD connectivity</li> <li>The particles_group subgroup</li> </ul> </li> <li>The parameters group<ul> <li>Force calculations</li> <li>The molecular dynamics workflow</li> </ul> </li> <li>Units</li> </ul> </li> </ul>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#overview","title":"Overview","text":"<p>Most computational data in NOMAD is harvested with code-specific parsers that recognize the output files from a particular software and retrieve the appropriate (meta)data accordingly. However, this approach is not possible for many modern molecular simulation engines that use fully-flexible scriptable input and non-fixed output files. \"HDF5 for molecular data\" (H5MD) is a data schema for storage of molecular simulation data, based on the HDF5 file format. This page describes an extension of the H5MD schema, denoted H5MD-NOMAD, which adds specificity to several of the H5MD guidelines while also retaining reasonable flexibility. This enables simulation data stored according to the H5MD-NOMAD schema to be stored in the NOMAD.</p> <p>Due to the nature of extending upon the original H5MD schema, portions of this doc page was duplicated, extended, or summarized from the H5MD webpage.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#introduction-to-the-h5md-storage-format","title":"Introduction to the H5MD storage format","text":"<p>H5MD was originally proposed by P. de Buyl, P. H. Colberg and F. H\u00f6fling in H5MD: A structured, efficient, and portable file format for molecular data, Comp. Phys. Comm. 185, 1546\u20131553 (2014) [arXiv:1308.6382]. The schema is maintained, along with associated tools, in a GitHub repository: H5MD GitHub.</p> <p>This section provides the basic nomenclature of the H5MD schema relevant for understanding H5MD-NOMAD, and was duplicated or summarized from the H5MD webpage.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#file-format","title":"File format","text":"<p>H5MD structures are stored in the HDF5 file format version 0 or later. It is recommended to use the HDF5 file format version 2, which includes the implicit tracking of the creation and modification times of the file and of each of its objects.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#notation-and-naming","title":"Notation and naming","text":"<p>HDF5 files are organized into groups and datasets, summarized as objects, which form a tree structure with the datasets as leaves. Attributes can be attached to each object. The H5MD specification adopts this naming and uses the following notation to depict the tree or its subtrees:</p> <p><code>\\-- item</code> :   An object within a group, that is either a dataset or a group. If it is a     group itself, the objects within the group are indented by five spaces with     respect to the group name.</p> <p><code>+-- attribute</code> :   An attribute, that relates either to a group or a dataset.</p> <p><code>\\-- data: &lt;type&gt;[dim1][dim2]</code> :   A dataset with array dimensions <code>dim1</code> by <code>dim2</code> and of type <code>&lt;type&gt;</code>. The     type is taken from <code>Enumeration</code>, <code>Integer</code>, <code>Float</code> or <code>String</code> and follows     the HDF5 Datatype classes. If the type is not mandated by H5MD, <code>&lt;type&gt;</code> is     indicated. A scalar dataspace is indicated by <code>[]</code>.</p> <p><code>(identifier)</code> :   An optional item.</p> <p><code>&lt;identifier&gt;</code> :   An optional item with unspecified name.</p> <p>H5MD defines a structure called H5MD element (or element whenever there is no confusion). An element is either a time-dependent group or a single dataset (see time-dependent data below), depending on the situation.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#time-dependent-data","title":"Time-dependent data","text":"<p>Time-dependent data consists of a series of samples (or frames) referring to multiple time steps. Such data are found inside a single dataset and are accessed via dataset slicing. In order to link the samples to the time axis of the simulation, H5MD defines a time-dependent H5MD element as a group that contains, in addition to the actual data, information on the corresponding integer time step and on the physical time. The structure of such a group is:</p> <pre><code>&lt;element&gt;\n \\-- step\n \\-- (time)\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <p><code>value</code> :   A dataset that holds the data of the time series. It uses a simple     dataspace whose rank is given by 1 plus the tensor rank of the data stored.     Its shape is the shape of a single data item prepended by a <code>[variable]</code>     dimension that allows the accumulation of samples during the course of     time. For instance, the data shape of scalars has the form <code>[variable]</code>,     <code>D</code>-dimensional vectors use <code>[variable][D]</code>, etc. The first dimension of     <code>value</code> must match the unique dimension of <code>step</code> and <code>time</code>.</p> <p>If several H5MD elements are sampled at equal times, <code>step</code> and <code>time</code> of one element may be hard links to the <code>step</code> and <code>time</code> datasets of a different element. If two elements are sampled at different times (for instance, if one needs the positions more frequently than the velocities), <code>step</code> and <code>time</code> are unique to each of them.</p> <p>The storage of step and time information follows one of the two modes below, depending on the dataset layout of <code>step</code>.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#explicit-step-and-time-storage","title":"Explicit step and time storage","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[variable]\n \\-- (time: type[variable])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <p><code>step</code> :   A dataset with dimensions <code>[variable]</code> that contains the time steps at     which the corresponding data were sampled. It is of <code>Integer</code> type to allow     exact temporal matching of data from one H5MD element to another. The     values of the dataset are in monotonically increasing order.</p> <p><code>time</code> :   An optional dataset that is the same as the <code>step</code> dataset, except it is     <code>Float</code> or <code>Integer</code>-valued and contains the simulation time in physical units. The     values of the dataset are in monotonically increasing order.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#fixed-step-and-time-storage-currently-not-supported-in-h5md-nomad","title":"Fixed step and time storage (currently not supported in H5MD-NOMAD)","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[]\n     +-- (offset: type[])\n \\-- (time: type[])\n     +-- (offset: type[])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <p><code>step</code> :   A scalar dataset of <code>Integer</code> type that contains the increment of the     time step between two successive rows of data in <code>value</code>.</p> <pre><code>`offset`\n: A scalar attribute of type `Integer` corresponding to the first sampled\nvalue of `step`.\n</code></pre> <p><code>time</code> :   An optional scalar dataset that is the same as the <code>step</code> dataset, except that     it is <code>Float</code> or <code>Integer</code>-valued and contains the increment in simulation     time, in physical units.</p> <p><code>offset</code>     : A scalar attribute of the same type as <code>time</code> corresponding to the first     sampled value of <code>time</code>.</p> <p>For this storage mode, the explicit value \\(s(i)\\) of the step corresponding to the \\(i\\)-th row of the dataset <code>value</code> is \\(s(i) = i\\times\\mathrm{step} + \\mathrm{offset}\\) where \\(\\mathrm{offset}\\) is set to zero if absent. The corresponding formula for the time \\(t(i)\\) is identical: \\(t(i) = i\\times\\mathrm{time} + \\mathrm{offset}\\). The index \\(i\\) is zero-based.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#time-independent-data","title":"Time-independent data","text":"<p>H5MD defines a time-independent H5MD element as a dataset. As for the <code>value</code> dataset in the case of time-dependent data, data type and array shape are implied by the stored data, where the <code>[variable]</code> dimension is omitted.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#storage-order-of-arrays","title":"Storage order of arrays","text":"<p>All arrays are stored in C-order as enforced by the HDF5 file format. A C or C++ program may thus declare <code>r[N][D]</code> for the array of particle coordinates while the Fortran program will declare a <code>r(D,N)</code> array (appropriate index ordering for a system of <code>N</code> particles in <code>D</code> spatial dimensions), and the HDF5 file will be the same.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#storage-of-particles-and-tuples-lists","title":"Storage of particles and tuples lists","text":""},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#storage-of-a-list-of-particles","title":"Storage of a list of particles","text":"<p>A list of particles is an H5MD element:</p> <pre><code>&lt;list_name&gt;: Integer[N]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>list_name</code> is a dataset of <code>Integer</code> type and dimensions <code>[N]</code>, N being the number of particle indices stored in the list. <code>particles_group</code> is an attribute containing an HDF5 Object Reference as defined by the HDF5 file format. <code>particles_group</code> must refer to one of the groups in <code>/particles</code>.</p> <p>If a fill value is defined for <code>list_name</code>, the particles indices in <code>list_name</code> set to this value are ignored.</p> <p>If the corresponding <code>particles_group</code> does not possess the <code>id</code> element, the values in <code>list_name</code> correspond to the indexing of the elements in <code>particles_group</code>. Else, the values in <code>list_name</code> must be put in correspondence with the equal values in the <code>id</code> element.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#storage-of-tuples","title":"Storage of tuples","text":"<p>A list of tuples is an H5MD element:</p> <pre><code>&lt;tuples_list_name&gt;: Integer[N,T]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>N</code> is the length of the list and <code>T</code> is the size of the tuples.  Both <code>N</code> and <code>T</code> may indicate variable dimensions. <code>particles_group</code> is an attribute containing an HDF5 Object Reference, obeying the same rules as for the lists of particles.</p> <p>The interpretation of the values stored within the tuples is done as for a list of particles.</p> <p>If a fill value is defined, tuples with at least one entry set to this value are ignored.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#time-dependence-time-dependent-particle-lists-currently-not-supported-in-h5md-nomad","title":"Time-dependence (time-dependent particle lists currently not supported in H5MD-NOMAD)","text":"<p>As the lists of particles and tuples above are H5MD elements, they can be stored either as time-dependent groups or time-independent datasets.</p> <p>As an example, a time-dependent list of pairs is stored as:</p> <pre><code>&lt;pair_list_name&gt;\n   +-- particles_group: Object reference\n   \\-- value: Integer[variable,N,2]\n   \\-- step: Integer[variable]\n</code></pre> <p>The dimension denoted by <code>N</code> may be variable.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-root-level","title":"The root level","text":"<p>The root level of H5MD-NOMAD structure is organized as follows (identical to the original H5MD specification):</p> <pre><code>H5MD-NOMAD root\n \\-- h5md\n \\-- (particles)\n \\-- (observables)\n \\-- (connectivity)\n \\-- (parameters)\n</code></pre> <p><code>h5md</code> :   A group that contains metadata and information on the H5MD structure     itself. It is the only mandatory group at the root level of H5MD.</p> <p><code>particles</code> :   An optional group that contains information on each particle in the system,     e.g., a snapshot of the positions or the full trajectory in phase space.</p> <p><code>observables</code> :   An optional group that contains other quantities of interest, e.g.,     physical observables that are derived from the system state at given points     in time.</p> <p><code>connectivity</code> :   An optional group that contains information about the connectivity between particles.</p> <p><code>parameters</code> :   An optional group that contains application-specific (meta)data such as     control parameters or simulation scripts.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-h5md-group","title":"The H5MD Group","text":"<p>A set of global metadata describing the H5MD structure is stored in the <code>h5md</code> group as attributes. The contents of the group are:</p> <pre><code>h5md\n +-- version: Integer[2]\n \\-- author\n |    +-- name: String[]\n |    +-- (email: String[])\n \\-- creator\n |    +-- name: String[]\n |    +-- version: String[]\n \\-- program\n      +-- name: String[]\n      +-- version: String[]\n</code></pre> <p><code>version</code> :   An attribute, of <code>Integer</code> datatype and of simple dataspace of rank 1 and     size 2, that contains the major version number and the minor version number     of the H5MD specification the H5MD structure conforms to.</p> <p>The version x.y.z of the H5MD specification follows semantic versioning: A change of the major version number x indicates backward-incompatible changes to the file structure. A change of the minor version number y indicates backwards-compatible changes to the file structure. A change of the patch version number z indicates changes that have no effect on the file structure and serves to allow for clarifications or minor text editing of the specification.</p> <p>As the z component has no impact on the content of an H5MD file, the <code>version</code> attribute contains only x and y.</p> <p><code>author</code> :   A group that contains metadata on the person responsible for the simulation     (or the experiment) as follows:</p> <ul> <li> <p><code>name</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that holds the author's real name.</p> </li> <li> <p><code>email</code> :   An optional attribute, of fixed-length string datatype and     of scalar dataspace, that holds the author's email address of     the form <code>email@domain.tld</code>.</p> </li> </ul> <p><code>creator</code> :   A group that contains metadata on the program that created the H5MD     structure as follows:</p> <ul> <li> <p><code>name</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that stores the name of the program.</p> </li> <li> <p><code>version</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that yields the version of the program.</p> </li> </ul> <p><code>program</code> :   A group that contains metadata on the code/package that created the simulation data contained within this H5MD structure:</p> <ul> <li> <p><code>name</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that stores the name of the program.</p> </li> <li> <p><code>version</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that yields the version of the program.</p> </li> </ul>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#modules-currently-unused-in-h5md-nomad","title":"Modules (currently unused in H5MD-NOMAD)","text":"<p>The original H5MD specification allowed the definition of modules under the h5md group. Such modules are currently ignored when uploading to NOMAD, although they of course will remain present in the raw uploaded hdf5 file.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-particles-group","title":"The particles group","text":"<p>Particle attributes, i.e., information about each particle in the system, are stored within the <code>particles</code> group. According to the original H5MD schema, the <code>particles</code> group is a container for subgroups that represent different subsets of the system under consideration. For simplicity of parsing, H5MD-NOMAD currently requires one such group, labeled <code>all</code>, to contain all the particles and corresponding attributes to be stored in the NOMAD archive. Additional particle groups will be ignored.</p> <p>For each dataset, the ordering of indices (whenever relevant) is as follows: frame index, particle index, dimension index. Thus, the contents of the <code>particles</code> group for a trajectory with <code>N_frames</code> frames and <code>N_part</code> particles in a <code>D</code>-dimensional space can be represented:</p> <pre><code>particles\n \\-- all\n |    \\-- box\n |    \\-- (&lt;time-dependent_vector_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part][D]\n |    \\-- (&lt;time-dependent_scalar_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part]\n |    \\-- (&lt;time-independent_vector_attribute&gt;): &lt;type&gt;[N_part][D]\n |    \\-- (&lt;time-independent_scalar_attribute&gt;): &lt;type&gt;[N_part]\n |    \\-- ...\n \\-- &lt;group2&gt;\n      \\-- ...\n</code></pre>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#standardized-h5md-elements-for-particles-group","title":"Standardized H5MD elements for particles group","text":"<p><code>position</code> :   (required for parsing other particle attributes) An element that describes the particle positions as coordinate vectors of <code>Float</code> or <code>Integer</code> type.</p> <p><code>velocity</code> :   An element that contains the velocities for each particle as a vector of     <code>Float</code> or <code>Integer</code> type.</p> <p><code>force</code> :   An element that contains the total forces (i.e., the accelerations     multiplied by the particle mass) for each particle as a vector of <code>Float</code>     or <code>Integer</code> type.</p> <p><code>mass</code> :   An element that holds the mass for each particle as a scalar of <code>Float</code>     type.</p> <ul> <li><code>image</code> :   (currently unused in H5MD-NOMAD)</li> </ul>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#todo-can-we-make-these-admonitions-indented-somehow-or-more-obviously-connected-with-the-members-of-this-list","title":"TODO can we make these admonitions indented somehow or more obviously connected with the members of this list?","text":"Details <p>An element that represents periodic images of the box as coordinate vectors of <code>Float</code> or <code>Integer</code> type and allows one to compute for each particle its absolute position in space. If <code>image</code> is present, <code>position</code> must be present as well. For time-dependent data, the <code>step</code> and <code>time</code> datasets of <code>image</code> must equal those of <code>position</code>, which must be accomplished by hard-linking the respective datasets.</p> <p><code>species</code> :   (currently unused in H5MD-NOMAD)</p> Details <p>An element that describes the species for each particle, i.e., its atomic or chemical identity, as a scalar of <code>Enumeration</code> or <code>Integer</code> data type. Particles of the same species are assumed to be identical with respect to their properties and unbonded interactions.</p> <p><code>id</code> : (currently unused in H5MD-NOMAD)</p> Details <p>An element that holds a scalar identifier for each particle of <code>Integer</code> type, which is unique within the given particle subgroup. The <code>id</code> serves to identify particles over the course of the simulation in the case when the order of the particles changes, or when new particles are inserted and removed. If <code>id</code> is absent, the identity of the particles is given by their index in the <code>value</code> datasets of the elements within the same subgroup.</p> <p><code>charge</code> :   An element that contains the charge associated to each particle as a     scalar, of <code>Integer</code> or <code>Float</code> type.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#standardized-h5md-nomad-elements-for-particles-group","title":"Standardized H5MD-NOMAD elements for particles group","text":"<p><code>species_label</code> :    An element that holds a label (fixed-length string datatype) for each particle. This label denotes the fundamental species type of the particle (e.g., the chemical element label for atoms), regardless of its given interactions within the model. Both time-independent and time-dependent <code>species_label</code> elements are supported.</p> <p><code>model_label</code> :   An element that holds a label (fixed-length string datatype) for each particle. This label denotes the type of particle with respect to the given interactions within the model (e.g., force field) Currently only time-independent species labels are supported.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#non-standard-elements-in-particles-group","title":"Non-standard elements in particles group","text":"<p>All non-standard elements within the particles group are currently ignored by the NOMAD H5MD parser. In principle, one can store additional custom attributes as configuration-specific observables (see The observables group).</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-simulation-box-subgroup","title":"The simulation box subgroup","text":"<p>Information about the simulation box is stored in a subgroup named <code>box</code>, within the relevant particles group (<code>all</code> in our case). Both time-independent and time-dependent box information are supported (i.e. via the <code>edges</code> element). Because the <code>box</code> group is specific to a particle group of particles, time-dependent boxes must contain <code>step</code> and <code>time</code> datasets that exactly match those of the corresponding <code>position</code> group. In principal, this should be accomplished by hard-linking the respective datasets. In practice, H5MD-NOMAD currently assumes that this is the case (i.e., the box group <code>step</code> and <code>time</code> information is unused), and simply checks that <code>edges.value</code> has the same leading dimension as <code>position</code>.</p> <p>The structure of the <code>box</code> group is as follows:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- (edges)\n</code></pre> <p><code>dimension</code> :   An attribute that stores the spatial dimension <code>D</code> of the simulation box     and is of <code>Integer</code> datatype and scalar dataspace.</p> <p><code>boundary</code> :    An attribute, of boolean datatype (changed from string to boolean in H5MD-NOMAD) and of simple dataspace of rank 1 and size <code>D</code>, that specifies the boundary condition of the box along each dimension, i.e., <code>True</code> implies periodic boundaries are applied in the corresponding dimension. If all values in <code>boundary</code> are <code>False</code>, <code>edges</code> may be omitted.</p> <p><code>edges</code> :   A <code>D</code>-dimensional vector or a <code>D</code> \u00d7 <code>D</code> matrix, depending on the geometry of the box, of <code>Float</code> or <code>Integer</code> type. Only cuboid and triclinic boxes are allowed. If <code>edges</code> is a vector, it specifies the space diagonal of a cuboid-shaped box. If <code>edges</code> is a matrix, the box is of triclinic shape with the edge vectors given by the rows of the matrix. For a time-dependent box, a cuboid geometry is encoded by a dataset <code>value</code> (within the H5MD element) of rank 2 (1 dimension for the time and 1 for the vector) and a triclinic geometry by a dataset <code>value</code> of rank 3 (1 dimension for the time and 2 for the matrix). For a time-independent box, a cuboid geometry is encoded by a dataset <code>edges</code> of rank 1 and a triclinic geometry by a dataset of rank 2.</p> <p>For instance, a cuboid box that changes in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges\n                \\-- step: Integer[variable]\n                \\-- time: Float[variable]\n                \\-- value: &lt;type&gt;[variable][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>. A triclinic box that is fixed in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges: &lt;type&gt;[D][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-observables-group","title":"The observables group","text":"<p>The initial H5MD proposed a simple and flexible schema for the general storage of observable info, defined roughly as \"macroscopic observables\" or \"averages of a property over many particles\", as H5MD elements:</p> <pre><code>observables\n \\-- &lt;observable1&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames]\n \\-- &lt;observable2&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames][D]\n \\-- &lt;group1&gt;\n |    \\-- &lt;observable3&gt;\n |         \\-- step: Integer[N_frames]\n |         \\-- time: Float[N_frames]\n |         \\-- value: &lt;type&gt;[N_frames][D][D]\n \\-- &lt;observable4&gt;: &lt;type&gt;[]\n \\-- ...\n</code></pre> <p></p> <p>As depicted above, observables representing only a subset of the particles may be stored in appropriate subgroups similar to the <code>particles</code> tree. H5MD-NOMAD does support the organization of observables into subgroups (as discussed in more detail below). However, grouping by particle groups is not fully supported in the sense that there is currently no metadata storing the corresponding indices of the relevant particles subgroup. Additionally, since only the <code>all</code> particles group is parsed, information about the named subgroup will not be stored anywhere in the archive. Thus, we recommend for now that only observables relevant to the <code>all</code> particles subgroup are stored within this section.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#h5md-nomad-observables","title":"H5MD-NOMAD observables","text":"<p>H5MD-NOMAD extends H5MD observable storage by 1. specifying standard observable types with associated metadata and 2. providing standardized specifications for some common observables. In contrast to the schema above, a more restrictive structure is required:</p> <pre><code>observables\n \\-- &lt;observable_type_1&gt;\n |    \\-- &lt;observable_1_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n \\-- &lt;observable_type_2&gt;\n |    \\-- &lt;observable_2_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- &lt;observable_2_label_2&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- ...\n \\-- ...\n</code></pre> <p>Here, each <code>observable_type</code> corresponds to a particular group of observables, e.g., to be plotted together in a single plot. The given name for this group could be generic, e.g., <code>radial distribution function</code>, or more specific, e.g., <code>molecular radial distribution function for solvents</code>. The latter may be useful in case multiple groupings of a single type of observable are needed. Each <code>observable_label</code> then corresponds to a specific name for an individual instance of this observable type. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>observable_label</code> might be set to <code>A-B</code>.</p> <p>Finally, H5MD-NOMAD has added the observable <code>type</code> as an attribute of each observable: The following observable types are supported:</p> <p></p> <p><code>configurational</code> :   An observable that is computed for each individual configuration, with the following general structure:</p> <pre><code>observables\n \\-- &lt;configurational_subgroup&gt;\n |    \\-- &lt;label_1&gt;\n |    |    +-- type: \"configurational\"\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][M]\n |    \\-- ...\n \\-- ...\n</code></pre> <p>where <code>M</code> is the dimension of the observable. This section may also be used to store per-particle quantities/attributes that are not currently supported as standardized H5MD-NOMAD elements for particles group, in which case <code>value</code> will have dimensions <code>[N_frames][N_part][M]</code>.</p> <p></p> <p><code>ensemble_average</code> :   An observable that is computed by averaging over multiple configurations, with the following generic structure:</p> <pre><code>observables\n \\-- &lt;ensemble_average_subgroup&gt;\n |    \\-- &lt;label_1&gt;\n |    |    +-- type: \"ensemble_average\"\n |    |    \\-- (n_variables): Integer\n |    |    \\-- (variables_name): String[n_variables][]\n |    |    \\-- (n_bins): Integer[]\n |    |    \\-- bins: Float[n_bins][]\n |    |    \\-- value: &lt;type&gt;[n_bins][]\n |    |    \\-- (frame_start): Integer\n |    |    \\-- (frame_end): Integer\n |    |    \\-- (n_smooth): Integer\n |    |    \\-- (type): String[]\n |    |    \\-- (error_type): String[]\n |    |    \\-- (errors): Float[n_bins]\n |    |    \\-- (error_labels): String[]\n |    |    \\-- (frame_end): Integer\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n |    \\-- ...\n \\-- ...\n</code></pre> <ul> <li> <p><code>n_variables</code> :   dimensionality of the observable. Can also be inferred from leading dimension of <code>bins</code>.</p> </li> <li> <p><code>variables_name</code> :   name/description of the independent variables along which the observable is defined.</p> </li> <li> <p><code>n_bins</code> :   number of bins along each dimension of the observable. Either single Integer for 1-D observables, or a list of Integers for multi-dimensional observable. Can also be inferred from dimensions of <code>bins</code>.</p> </li> <li> <p><code>bins</code> :   value of the bins used for calculating the observable along each dimension of the observable.</p> </li> <li> <p><code>value</code> :   value of the calculated ensemble average at each bin.</p> </li> <li> <p><code>frame_start</code> :   trajectory frame index at which the averaging begins. This index must correspond to the list of steps and times in <code>particles.all.position</code>.</p> </li> <li> <p><code>frame_end</code> :   trajectory frame index at which the averaging ends. This index must correspond to the list of steps and times in <code>particles.all.position</code>.</p> </li> <li> <p><code>n_smooth</code> :   number of bins over which the running average was computed for <code>value</code>.</p> </li> <li> <p><code>type</code> :   Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level.</p> </li> </ul> <ul> <li> <p><code>error_type</code> :   describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>.</p> </li> <li> <p><code>errors</code> :   value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>.</p> </li> <li> <p><code>error_labels</code> :   describes the error along individual dimensions for multi-D errors.</p> </li> <li> <p><code>&lt;custom_dataset&gt;</code> :   additional metadata may be given as necessary.</p> </li> </ul> <p></p> <p><code>time_correlation</code> :   An observable that is computed by calculating correlations between configurations in time, with the following general structure:</p> <pre><code>observables\n \\-- &lt;time_correlation_subgroup&gt;\n |    \\-- &lt;label_1&gt;\n |    |    +-- type: \"time_correlation\"\n |    |    \\-- (direction): String[]\n |    |    \\-- (n_times): Integer[]\n |    |    \\-- times: Float[n_times][]\n |    |    \\-- value: &lt;type&gt;[n_bins][]\n |    |    \\-- (type): String[]\n |    |    \\-- (error_type): String[]\n |    |    \\-- (errors): Float[n_bins]\n |    |    \\-- (error_labels): String[]\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n |    \\-- ...\n \\-- ...\n</code></pre> <ul> <li> <p><code>label</code> :   describes the particles involved in determining the property. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>label</code> might be set to <code>A-B</code></p> </li> <li> <p><code>direction</code> :   allowed values of <code>x</code>, <code>y</code>, <code>z</code>, <code>xy</code>, <code>yz</code>, <code>xz</code>, <code>xyz</code>. The direction/s used for calculating the correlation function.</p> </li> <li> <p><code>n_times</code> :   number of times windows for the calculation of the correlation function. Can also be inferred from dimensions of <code>times</code>.</p> </li> <li> <p><code>times</code> :   time values used for calculating the correlation function (i.e., \u0394t values).</p> </li> <li> <p><code>value</code> :   value of the calculated correlation function at each time.</p> </li> <li> <p><code>type</code> :   Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level.</p> </li> </ul> <ul> <li> <p><code>error_type</code> :   describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>.</p> </li> <li> <p><code>errors</code> :   value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>.</p> </li> <li> <p><code>error_labels</code> :   describes the error along individual dimensions for multi-D errors.</p> </li> <li> <p><code>&lt;custom_dataset&gt;</code> :   additional metadata may be given as necessary.</p> </li> </ul> <p>A list of standardized observables can be found in Reference - H5MD-NOMAD &gt; Standardized observables in H5MD-NOMAD.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-connectivity-group","title":"The connectivity group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of \"connectivity\" information, e.g., to be used in conjunction with a molecular mechanics force field. The connectivity information is stored as tuples in the group <code>/connectivity</code>. The tuples are pairs, triples, etc. as needed and may be either time-independent or time-dependent. As with other elements, connectivity elements can be defined for particular particle groups. However, H5MD-NOMAD focuses on the storage of connectivity elements for the entire system (i.e., the <code>all</code> particles group).</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#standardized-h5md-nomad-connectivity","title":"Standardized H5MD-NOMAD connectivity","text":"<p>The general structure of the <code>connectivity</code> group is as follows:</p> <pre><code>connectivity\n \\-- (bonds): Integer[N_part][2]\n \\-- (angles): Integer[N_part][3]\n \\-- (dihedrals): Integer[N_part][4]\n \\-- (impropers): Integer[N_part][4]\n \\-- (&lt;custom_interaction&gt;): Integer[N_part][m]\n \\-- (particles_group)\n      \\-- ...\n</code></pre> <p><code>N_part</code> corresponds to the number of particles stored in the <code>particles/all</code> group.</p> <ul> <li> <p><code>bonds</code> : a list of 2-tuples specifying the indices of particles containing a \"bond interaction\".</p> </li> <li> <p><code>angles</code> : a list of 3-tuples specifying the indices of particles containing an \"angle interaction\".</p> </li> <li> <p><code>dihedrals</code> : a list of 4-tuples specifying the indices of particles containing a \"dihedral interaction\".</p> </li> <li> <p><code>impropers</code> : a list of 4-tuples specifying the indices of particles containing an \"improper dihedral interaction\".</p> </li> <li> <p><code>&lt;custom_interaction&gt;</code> : a list of m-tuples specifying the indices of particles containing an arbitrary interaction. <code>m</code> denotes the number of particles involved in the interaction.</p> </li> <li> <p><code>particles_group</code> : See below.</p> </li> </ul> <p> Currently only time-independent connectivity elements are supported.</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-particles_group-subgroup","title":"The particles_group subgroup","text":"<p>Despite not fully utilizing the organization of arbitrary groups of particles within the <code>particles</code> group, H5MD-NOMAD allows for the user to provide an arbitrary hierarchy of particle groupings, also referred to as a \"topology\", within the <code>connectivity</code> subgroup called <code>particles_group</code>. This information will be used by NOMAD to facilitate visualizations of the system, through the \"topology bar\" in the overview page. The general structure of the topology group is as follows:</p> <pre><code>connectivity\n \\-- particles_group\n      \\-- &lt;group_1&gt;\n      |    \\-- (type): String[]\n      |    \\-- (formula): String[]\n      |    \\-- indices: Integer[]\n      |    \\-- (is_molecule): Bool\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n      |    \\-- (particles_group):\n      |        \\-- ...\n      \\-- &lt;group_2&gt;\n          \\-- ...\n</code></pre> <p>The initial <code>particles_group</code> subgroup, directly under <code>connectivity</code>, is a container for the entire topology. <code>particles_group</code> contains a series of subgroups with arbitrary names, which denote the first level of organization within the topology. The name of each subgroup will become the group label within the NOMAD metadata. Each of these subgroups then contain a series of datasets:</p> <ul> <li> <p><code>type</code> : describes the type of particle group. There exists a list of standardized types: <code>molecule_group</code>, <code>molecule</code>, <code>monomer_group</code>, <code>monomer</code>. However, arbitrary types can be given. We suggest that you 1. use the standardized types when appropriate (note that protein residues should be generically typed as <code>monomer</code>) and 2. use the general format <code>&lt;type&gt;_group</code> for groups of a distinct type (see further description of suggested hierarchy below).</p> </li> <li> <p><code>formula</code> : a \"chemical-like\" formula that describes the particle group with respect to its underlying components. The format for the formula is <code>&lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...</code>, where <code>&lt;child_x&gt;</code> is the name/label of the underlying component, and <code>n_child_x</code> is the number of such components found within this particle group. Example: A particles group containing 100 water molecules named <code>water</code> has the formula <code>water(100)</code>, whereas each underlying water molecule has the standard chemical formula <code>H2O</code>.</p> </li> <li> <p><code>indices</code> : a list of integer indices corresponding to all particles belonging to this group. Indices should correspond to the list of particles stored in the <code>particles/all</code> group.</p> </li> <li> <p><code>is_molecule</code> : indicator of individual molecules (typically with respect to the bond connections defined by a force field).</p> </li> <li> <p><code>custom_dataset</code> : arbitrary additional metadata for this particle group may be given.</p> </li> </ul> <p>Each subgroup may also contain a (nested) <code>particles_group</code> subgroup, in order to subdivide the group of particles into an organizational hierarchy. As with the overall <code>particles_group</code> container, the groups contained within <code>particles_group</code> must not partition the particles within this group (i.e., overlapping or non-complete groupings are allowed). However, particle groups must contain particles already contained within the parent <code>particles_group</code> (i.e., subgroups must be a subset of the grouping at the previous level of the hierarchy).</p> <p>Note that typically the <code>particles_group</code> hierarchy ends at the level of individual particles (i.e., individual particles are not stored, since this information is already contained within the <code>particles</code> group).</p>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-parameters-group","title":"The parameters group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of general \"parameter\" information within the <code>parameters</code> group, with the following structure:</p> <pre><code>parameters\n +-- &lt;user_attribute1&gt;\n \\-- &lt;user_data1&gt;\n \\-- &lt;user_group1&gt;\n |    \\-- &lt;user_data2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>In contrast, the H5MD-NOMAD schema calls for very specific structures to be used when storing parameter information. While the previous groups have attempted to stay away from enforcing NOMAD-specific data structures on the user, instead opting for more intuitive and generally-convenient structures, the <code>parameters</code> group utilizes already-existing metadata and structures within NOMAD to efficiently import simulation parameters in a way that is searchable and comparable to simulations performed by other users.</p> <p>In this way, the H5MD-NOMAD <code>parameters</code> group has the following structure:</p> <pre><code>parameters\n \\-- &lt;parameter_subgroup_1&gt;\n |    \\-- ...\n \\-- &lt;parameter_subgroup_2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>The subgroups <code>force_calculations</code> and <code>workflow</code> are supported. The following describes the detailed data structures for these subgroups, using the NOMAD MetaInfo definitions for each underlying <code>Quantity</code>. Please note that:</p> <ol> <li> <p>Quantities with <code>type=MEnum()</code> are restricted to the provided allowed values.</p> </li> <li> <p>The unit given in the MetaInfo definition does not have to be used within the H5MD-NOMAD file, however, the dimensionality of the unit should match.</p> </li> </ol>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#force-calculations","title":"Force calculations","text":"<p>The <code>force_calculations</code> group contains the parameters for force calculations according to the force field during a molecular dynamics run.</p> <p></p> <p>The following json template illustrates the structure of the <code>force_calculations</code> group, with example values for clarity:</p> <pre><code>{\n\"vdw_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n\"coulomb_type\": \"particle_mesh_ewald\",\n\"coulomb_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n\"neighbor_searching\": {\n\"neighbor_update_frequency\": 1,\n\"neighbor_update_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"}\n}\n}\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>vdw_cutoff</code> :</p> <pre><code>Quantity(\n        type=np.float64,\n        shape=[],\n        unit='m',\n        description='''\n        Cutoff for calculating VDW forces.\n        ''')\n</code></pre> </li> <li> <p><code>coulomb_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('cutoff', 'ewald', 'multilevel_summation', 'particle_mesh_ewald',\n            'particle_particle_particle_mesh', 'reaction_field'),\n    shape=[],\n    description='''\n    Method used for calculating long-ranged Coulomb forces.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"Cutoff\"`          | Simple cutoff scheme. |\n\n    | `\"Ewald\"` | Standard Ewald summation as described in any solid-state physics text. |\n\n    | `\"Multi-Level Summation\"` |  D. Hardy, J.E. Stone, and K. Schulten,\n    [Parallel. Comput. **35**, 164](https://doi.org/10.1016/j.parco.2008.12.005)|\n\n    | `\"Particle-Mesh-Ewald\"`        | T. Darden, D. York, and L. Pedersen,\n    [J. Chem. Phys. **98**, 10089 (1993)](https://doi.org/10.1063/1.464397) |\n\n    | `\"Particle-Particle Particle-Mesh\"` | See e.g. Hockney and Eastwood, Computer Simulation Using Particles,\n    Adam Hilger, NY (1989). |\n\n    | `\"Reaction-Field\"` | J.A. Barker and R.O. Watts,\n    [Mol. Phys. **26**, 789 (1973)](https://doi.org/10.1080/00268977300102101)|\n    ''')\n</code></pre> </li> <li> <p><code>coulomb_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    Cutoff for calculating short-ranged Coulomb forces.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_searching</code> : Section containing the parameters for neighbor searching/lists during a molecular dynamics run.</p> </li> <li> <p><code>neighbor_update_frequency</code> :</p> <pre><code>Quantity(\n    type=int,\n    shape=[],\n    description='''\n    Number of timesteps between updating the neighbor list.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_update_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    The distance cutoff for determining the neighbor list.\n    ''')\n</code></pre> </li> </ul>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#the-molecular-dynamics-workflow","title":"The molecular dynamics workflow","text":"<p>The <code>workflow</code> group contains the parameters for any type of workflow. Here we describe the specific case of the well-defined <code>molecular_dynamics</code> workflow. Custom workflows are described in detail in Workflows in NOMAD.</p> <p></p> <p>The following json template illustrates the structure of the <code>molecular_dynamics</code> subsection of the <code>workflow</code> group, with example values for clarity:</p> <pre><code>{\n\"molecular_dynamics\": {\n\"thermodynamic_ensemble\": \"NPT\",\n\"integrator_type\": \"langevin_leap_frog\",\n\"integration_timestep\": {\"value\": 2e-15, \"unit\": \"ps\"},\n\"n_steps\": 20000000,\n\"coordinate_save_frequency\": 10000,\n\"velocity_save_frequency\": null,\n\"force_save_frequency\": null,\n\"thermodynamics_save_frequency\": null,\n\"thermostat_parameters\": {\n\"thermostat_type\": \"langevin_leap_frog\",\n\"reference_temperature\": {\"value\": 300.0, \"unit\": \"kelvin\"},\n\"coupling_constant\": {\"value\": 1.0, \"unit\": \"ps\"}},\n\"barostat_parameters\": {\n\"barostat_type\": \"berendsen\",\n\"coupling_type\": \"isotropic\",\n\"reference_pressure\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], \"unit\": \"bar\"},\n\"coupling_constant\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]},\n\"compressibility\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]}\n}\n}\n}\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>thermodynamic_ensemble</code> :</p> <pre><code>Quantity(\n    type=MEnum('NVE', 'NVT', 'NPT', 'NPH'),\n    shape=[],\n    description='''\n    The type of thermodynamic ensemble that was simulated.\n\n    Allowed values are:\n\n    | Thermodynamic Ensemble          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"NVE\"`           | Constant number of particles, volume, and energy |\n\n    | `\"NVT\"`           | Constant number of particles, volume, and temperature |\n\n    | `\"NPT\"`           | Constant number of particles, pressure, and temperature |\n\n    | `\"NPH\"`           | Constant number of particles, pressure, and enthalpy |\n    ''')\n</code></pre> </li> <li> <p><code>integrator_type</code> :         Quantity(             type=MEnum(                 'brownian', 'conjugant_gradient', 'langevin_goga',                 'langevin_schneider', 'leap_frog', 'rRESPA_multitimescale', 'velocity_verlet'             ),             shape=[],             description='''             Name of the integrator.</p> <pre><code>    Allowed values are:\n\n    | Integrator Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"leap_frog\"`          | R.W. Hockney, S.P. Goel, and J. Eastwood,\n    [J. Comp. Phys. **14**, 148 (1974)](https://doi.org/10.1016/0021-9991(74)90010-2) |\n\n    | `\"velocity_verlet\"` | W.C. Swope, H.C. Andersen, P.H. Berens, and K.R. Wilson,\n    [J. Chem. Phys. **76**, 637 (1982)](https://doi.org/10.1063/1.442716) |\n\n    | `\"rRESPA_multitimescale\"` | M. Tuckerman, B. J. Berne, and G. J. Martyna\n    [J. Chem. Phys. **97**, 1990 (1992)](https://doi.org/10.1063/1.463137) |\n    ''')\n</code></pre> </li> <li> <p><code>integration_timestep</code> :         Quantity(             type=np.float64,             shape=[],             unit='s',             description='''             The timestep at which the numerical integration is performed.             ''')</p> </li> <li> <p><code>n_steps</code> :         Quantity(             type=int,             shape=[],             description='''             Number of timesteps performed.             ''')</p> </li> <li> <p><code>coordinate_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the coordinates.             ''')</p> </li> <li> <p><code>velocity_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the velocities.             ''')</p> </li> <li> <p><code>force_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the forces.             ''')</p> </li> <li> <p><code>thermodynamics_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the thermodynamic quantities.             ''')</p> </li> <li> <p><code>thermostat_parameters</code> :  Section containing the parameters pertaining to the thermostat for a molecular dynamics run.</p> </li> <li> <p><code>thermostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('andersen', 'berendsen', 'brownian', 'langevin_goga', 'langevin_schneider', 'nose_hoover', 'velocity_rescaling',\n            'velocity_rescaling_langevin'),\n    shape=[],\n    description='''\n    The name of the thermostat used for temperature control. If skipped or an empty string is used, it\n    means no thermostat was applied.\n\n    Allowed values are:\n\n    | Thermostat Name        | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"andersen\"`           | H.C. Andersen, [J. Chem. Phys.\n    **72**, 2384 (1980)](https://doi.org/10.1063/1.439486) |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"brownian\"`           | Brownian Dynamics |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"velocity_rescaling\"` | G. Bussi, D. Donadio, and M. Parrinello,\n    [J. Chem. Phys. **126**, 014101 (2007)](https://doi.org/10.1063/1.2408420) |\n\n    | `\"velocity_rescaling_langevin\"` | G. Bussi and M. Parrinello,\n    [Phys. Rev. E **75**, 056707 (2007)](https://doi.org/10.1103/PhysRevE.75.056707) |\n    ''')\n</code></pre> </li> <li> <p><code>reference_temperature</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kelvin',\n    description='''\n    The target temperature for the simulation.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='s',\n    description='''\n    The time constant for temperature coupling. Need to describe what this means for the various\n    thermostat options...\n    ''')\n</code></pre> </li> <li> <p><code>effective_mass</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kilogram',\n    description='''\n    The effective or fictitious mass of the temperature resevoir.\n    ''')\n</code></pre> </li> <li> <p><code>barostat_parameters</code> : Section containing the parameters pertaining to the barostat for a molecular dynamics run.</p> </li> <li> <p><code>barostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('berendsen', 'martyna_tuckerman_tobias_klein', 'nose_hoover', 'parrinello_rahman', 'stochastic_cell_rescaling'),\n    shape=[],\n    description='''\n    The name of the barostat used for temperature control. If skipped or an empty string is used, it\n    means no barostat was applied.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"martyna_tuckerman_tobias_klein\"` | G.J. Martyna, M.E. Tuckerman, D.J. Tobias, and M.L. Klein,\n    [Mol. Phys. **87**, 1117 (1996)](https://doi.org/10.1080/00268979600100761);\n    M.E. Tuckerman, J. Alejandre, R. L\u00f3pez-Rend\u00f3n, A.L. Jochim, and G.J. Martyna,\n    [J. Phys. A. **59**, 5629 (2006)](https://doi.org/10.1088/0305-4470/39/19/S18)|\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"parrinello_rahman\"`        | M. Parrinello and A. Rahman,\n    [J. Appl. Phys. **52**, 7182 (1981)](https://doi.org/10.1063/1.328693);\n    S. Nos\u00e9 and M.L. Klein, [Mol. Phys. **50**, 1055 (1983) |\n\n    | `\"stochastic_cell_rescaling\"` | M. Bernetti and G. Bussi,\n    [J. Chem. Phys. **153**, 114107 (2020)](https://doi.org/10.1063/1.2408420) |\n    ''')\n</code></pre> </li> <li> <p><code>coupling_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('isotropic', 'semi_isotropic', 'anisotropic'),\n    shape=[],\n    description='''\n    Describes the symmetry of pressure coupling. Specifics can be inferred from the `coupling constant`\n\n    | Type          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `isotropic`          | Identical coupling in all directions. |\n\n    | `semi_isotropic` | Identical coupling in 2 directions. |\n\n    | `anisotropic`        | General case. |\n    ''')\n</code></pre> </li> <li> <p><code>reference_pressure</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='pascal',\n    description='''\n    The target pressure for the simulation, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='s',\n    description='''\n    The time constants for pressure coupling, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. 0 values along the off-diagonal\n    indicate no-coupling between these directions.\n    ''')\n</code></pre> </li> <li> <p><code>compressibility</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='1 / pascal',\n    description='''\n    An estimate of the system's compressibility, used for box rescaling, stored in a 3x3 matrix indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. If None, it may indicate that these values\n    are incorporated into the coupling_constant, or simply that the software used uses a fixed value that is not available in\n    the input/output files.\n    ''')\n</code></pre> </li> </ul>"},{"location":"custom_schemas/h5md/explanation-H5MD-NOMAD/#units","title":"Units","text":"<p>In the original H5MD schema, units were given as string attributes of datasets, e.g., <code>60 m s-2</code>. H5MD-NOMAD amends the treatment of units in 2 ways:</p> <ol> <li> <p>If needed, the leading prefactor is stored as a separate attribute of <code>float</code> datatype called <code>unit_factor</code>.</p> </li> <li> <p>The string that describes the unit should be compatible with the <code>UnitRegistry</code> class of the <code>pint</code> python module.</p> </li> </ol> <p>Generic representation of unit storage in H5MD-NOMAD:</p> <pre><code>&lt;group&gt;\n    \\-- &lt;dataset&gt;\n        +-- (unit: String[])\n        +-- (unit_factor: Float)\n</code></pre>"},{"location":"custom_schemas/h5md/h5md/","title":"The H5MD Group","text":"<p>A set of global metadata describing the H5MD structure is stored in the <code>h5md</code> group as attributes. The contents of the group are:</p> <pre><code>h5md\n +-- version: Integer[2]\n \\-- author\n |    +-- name: String[]\n |    +-- (email: String[])\n \\-- creator\n |    +-- name: String[]\n |    +-- version: String[]\n \\-- program\n      +-- name: String[]\n      +-- version: String[]\n</code></pre> <p><code>version</code> :   An attribute, of <code>Integer</code> datatype and of simple dataspace of rank 1 and     size 2, that contains the major version number and the minor version number     of the H5MD specification the H5MD structure conforms to.</p> <p>The version x.y.z of the H5MD specification follows semantic versioning: A change of the major version number x indicates backward-incompatible changes to the file structure. A change of the minor version number y indicates backwards-compatible changes to the file structure. A change of the patch version number z indicates changes that have no effect on the file structure and serves to allow for clarifications or minor text editing of the specification.</p> <p>As the z component has no impact on the content of an H5MD file, the <code>version</code> attribute contains only x and y.</p> <p><code>author</code> :   A group that contains metadata on the person responsible for the simulation     (or the experiment) as follows:</p> <ul> <li> <p><code>name</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that holds the author's real name.</p> </li> <li> <p><code>email</code> :   An optional attribute, of fixed-length string datatype and     of scalar dataspace, that holds the author's email address of     the form <code>email@domain.tld</code>.</p> </li> </ul> <p><code>creator</code> :   A group that contains metadata on the program that created the H5MD     structure as follows:</p> <ul> <li> <p><code>name</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that stores the name of the program.</p> </li> <li> <p><code>version</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that yields the version of the program.</p> </li> </ul> <p><code>program</code> :   A group that contains metadata on the code/package that created the simulation data contained within this H5MD structure:</p> <ul> <li> <p><code>name</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that stores the name of the program.</p> </li> <li> <p><code>version</code> :   An attribute, of fixed-length string datatype and of scalar     dataspace, that yields the version of the program.</p> </li> </ul>"},{"location":"custom_schemas/h5md/h5md/#modules-currently-unused-in-h5md-nomad","title":"Modules (currently unused in H5MD-NOMAD)","text":"<p>The original H5MD specification allowed the definition of modules under the h5md group. Such modules are currently ignored when uploading to NOMAD, although they of course will remain present in the raw uploaded hdf5 file.</p>"},{"location":"custom_schemas/h5md/h5md_overview/","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":"<p>Most computational data in NOMAD is harvested with code-specific parsers that recognize the output files from a particular software and retrieve the appropriate (meta)data accordingly. However, this approach is not possible for many modern molecular simulation engines that use fully-flexible scriptable input and non-fixed output files. \"HDF5 for molecular data\" (H5MD) is a data schema for storage of molecular simulation data, based on the HDF5 file format. The following pages describe an extension of the H5MD schema, denoted H5MD-NOMAD, which adds specificity to several of the H5MD guidelines while also retaining reasonable flexibility. This enables simulation data stored according to the H5MD-NOMAD schema to be parsed and normalized by NOMAD, while also allowing the user some freedom for customization.</p> <p>Due to the new nature of extending upon the original H5MD schema, portions of these doc pages were duplicated, extended, or summarized from the H5MD webpage.</p>"},{"location":"custom_schemas/h5md/h5md_overview/#introduction-to-the-h5md-storage-format","title":"Introduction to the H5MD storage format","text":"<p>H5MD was originally proposed by P. de Buyl, P. H. Colberg and F. H\u00f6fling in H5MD: A structured, efficient, and portable file format for molecular data, Comp. Phys. Comm. 185, 1546\u20131553 (2014) [arXiv:1308.6382]. The schema is maintained, along with associated tools, in a GitHub repository: H5MD GitHub.</p> <p>The basic nomenclature of the H5MD schema relevant for understanding H5MD-NOMAD can be found here: Quick Start - H5MD basics. Moreover, many of the details of the H5MD structure will be necessarily covered through the explanation of H5MD-NOMAD.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/","title":"How to work with the H5MD-NOMAD schema","text":"<ul> <li>How to work with the H5MD-NOMAD schema<ul> <li>Writing an HDF5 file according to H5MD-NOMAD with python<ul> <li>Imports</li> <li>Example Data</li> <li>H5MD Group</li> <li>Particles Group</li> <li>Connectivity Group</li> <li>Observables Group</li> <li>Parameter Group</li> </ul> </li> <li>Accessing an H5MD-NOMAD file</li> <li>Creating a topology (particles_group)<ul> <li>Standard topology structure for bonded force fields</li> <li>Creating the standard hierarchy from an MDAnalysis universe</li> <li>Writing the topology to an H5MD-NOMAD file</li> </ul> </li> </ul> </li> </ul>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#writing-an-hdf5-file-according-to-h5md-nomad-with-python","title":"Writing an HDF5 file according to H5MD-NOMAD with python","text":"<p>You can write to an HDF5 file via a python interface, using the h5py package. This section walks you through the creation of each section of the H5MD-NOMAD schema, using practical examples to help you get started.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#imports","title":"Imports","text":"<pre><code>import numpy as np\nimport json\nimport h5py\nimport parmed as chem\nimport MDAnalysis as mda\nfrom pint import UnitRegistry\nureg = UnitRegistry()\n</code></pre> <p>h5py : module for reading and writing HDF5 files.</p> <p>UnitRegistry : object from the pint package that provides assistance for working with units. We suggest using this package for easiest compatibility with NOMAD. If you have <code>nomad-lab</code> installed, you can alternatively import <code>ureg</code> with <code>from nomad.units import ureg</code>.</p> <p>MDAnalysis : a library to analyze trajectories from molecular dynamics simulations stored in various formats.</p> <p>ParmEd : a tool for aiding in investigations of biomolecular systems using popular molecular simulation packages.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#example-data","title":"Example Data","text":"<p>For concreteness, we consider a fictitious set of \"vanilla\" molecular dynamics simulations, run with the OpenMM software. The following definitions set the dimensionality, periodicity, and the units for this simulation.</p> <pre><code>dimension = 3\nperiodicity = [True, True, True]\ntime_unit = 1.0 * ureg.picosecond\nlength_unit = 1.0 * ureg.angstrom\nenergy_unit = 1000. * ureg.joule\nmass_unit = 1.0 * ureg.amu\ncharge_unit = 1.0 * ureg.e\ntemperature_unit = 1.0 * ureg.K\ncustom_unit = 1.0 * ureg.newton / length_unit**2\nacceleration_unit = 1.0 * length_unit / time_unit**2\n</code></pre> <p>In this example, we will assume that the relevant simulation data is compatible with MDAnalysis, such that a universe containing the trajectory and topology information can be created.</p> <p>Note</p> <p>Knowledge of the MDAnalysis package is not necessary for understanding this example. The dimensions of the supplied quantities will be made clear in each case.</p> <p>Create a universe by supplying a <code>pdb</code> structure file and corresponding <code>dcd</code> trajectory file (MDAnalysis supports many different file formats): <pre><code>universe = mda.Universe('initial_structure.pdb', 'trajectory.dcd')\nn_frames = len(universe.trajectory)\nn_atoms = universe.trajectory[0].n_atoms\n</code></pre> Some topologies can be loaded directly into MDAnalysis. However, for simulations from OpenMM, one can read the topology using <code>parmed</code> and then import it to MDanalysis: <pre><code>pdb = app.PDBFile('initial_structure.pdb')\nforcefield = app.ForceField('force_field.xml')\nsystem = forcefield.createSystem(pdb.topology)\nstruct = chem.openmm.load_topology(pdb.topology, system)\nuniverse_toponly = mda.Universe(struct)\n</code></pre></p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#h5md-group","title":"H5MD Group","text":"<p>Create an HDF5 file called <code>test_h5md-nomad.h5</code> and create the group <code>h5md</code> under <code>root</code>: <pre><code>h5_write = h5py.File('test_h5md-nomad.h5', 'w')\nh5md = h5_write.create_group('h5md')\n</code></pre></p> <p>Add the h5md version (1.0.x in this case) as an attribute of the <code>h5md</code> group: <pre><code>h5md.attrs['version'] = [1, 0]\n</code></pre></p> <p>Create the <code>author</code> group and add the associated metadata: <pre><code>author = h5md.create_group('author')\nauthor.attrs['name'] = 'author name'\nauthor.attrs['email'] = 'author-name@example-domain.edu'\n</code></pre></p> <p>Create the <code>program</code> group and add the associated metadata: <pre><code>program = h5md.create_group('program')\nprogram.attrs['name'] = 'OpenMM'\nprogram.attrs['version'] = '7.7.0'\n</code></pre></p> <p>Create the <code>creator</code> group and add the associated metadata: <pre><code>program = h5md.create_group('creator')\nprogram.attrs['name'] = h5py.__name__\nprogram.attrs['version'] = str(h5py.__version__)\n</code></pre></p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#particles-group","title":"Particles Group","text":"<p>Create the <code>particles</code> group and the underlying <code>all</code> group to hold the relevant particle data: <pre><code>particles = h5_write.create_group('particles')\nparticles_group_all = particles.create_group('all')\n</code></pre></p> <p>Get the steps, times, positions, and lattice vectors (i.e., box dimensions) from the MDA universe: <pre><code># quantities extracted from MDAnalysis\nsteps = []\ntimes = []\npositions = []\nlattice_vectors = []\nfor i_frame, frame in enumerate(universe.trajectory):\ntimes.append(frame.time)\nsteps.append(frame.frame)\npositions.append(frame.positions)\nlattice_vectors.append(frame.triclinic_dimensions)\n</code></pre></p> <p>Set the positions and corresponding metadata: <pre><code>position_group_all = particles_group_all.create_group('position')\nposition_group_all['step'] = steps  # shape = (n_frames)\nposition_group_all['time'] = times  # shape = (n_frames)\nposition_group_all['time'].attrs['unit'] = str(time_unit.units)\nposition_group_all['time'].attrs['unit_factor'] = time_unit.magnitude\nposition_group_all['value'] = positions  # shape = (n_frames, n_atoms, dimension)\nposition_group_all['value'].attrs['unit'] = str(length_unit.units)\nposition_group_all['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p> <p>Set the particle-specific metadata: <pre><code>particles_group_all['species_label'] = universe_toponly.atoms.types  # shape = (n_atoms)\nparticles_group_all['force_field_label'] = universe_toponly.atoms.names  # shape = (n_atoms)\nparticles_group_all['mass'] = universe_toponly.atoms.masses  # shape = (n_atoms)\nparticles_group_all['mass'].attrs['unit'] = str(mass_unit.units)\nparticles_group_all['mass'].attrs['unit_factor'] = mass_unit.magnitude\nparticles_group_all['charge'] = universe_toponly.atoms.charges  # shape = (n_atoms)\nparticles_group_all['charge'].attrs['unit'] = str(charge_unit.units)\nparticles_group_all['charge'].attrs['unit_factor'] = charge_unit.magnitude\n</code></pre></p> <p>Create the <code>box</code> group under <code>particles.all</code> and write corresponding data: <pre><code>box_group = particles_group_all.create_group('box')\nbox_group.attrs['dimension'] = dimension\nbox_group.attrs['boundary'] = periodicity\nedges = box_group.create_group('edges')\nedges['step'] = steps\nedges['time'] = times\nedges['time'].attrs['unit'] = str(time_unit.units)\nedges['time'].attrs['unit_factor'] = time_unit.magnitude\nedges['value'] = lattice_vectors\nedges['value'].attrs['unit'] = str(length_unit.units)\nedges['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#connectivity-group","title":"Connectivity Group","text":"<p>Create the <code>connectivity</code> group under <code>root</code> and add the tuples of bonds, angles, and dihedrals: <pre><code>connectivity = h5_write.create_group('connectivity')\nconnectivity['bonds'] = universe_toponly.bonds._bix  # shape = (n_bonds, 2)\nconnectivity['angles'] = universe_toponly.angles._bix  # shape = (n_angles, 3)\nconnectivity['dihedrals'] = universe_toponly.dihedrals._bix  # shape = (n_dihedrals, 4)\nconnectivity['impropers'] = universe_toponly.impropers._bix  # shape = (n_impropers, 4)\n</code></pre> Here <code>n_bonds</code>, <code>n_angles</code>, <code>n_dihedrals</code>, and <code>n_impropers</code> represent the corresponding number of instances of each interaction within the force field.</p> <p>You can read more about the creation of the hierarchical <code>particles_group</code> in Creating a topology.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#observables-group","title":"Observables Group","text":"<p>For this section, we will consider sets of fabricated observable data for clarity. First, create the <code>observables</code> group under root: <pre><code>observables = h5_write.create_group('observables')\n</code></pre></p> <p>There are 3 types of support observables: <pre><code>types = ['configurational', 'ensemble_average', 'correlation_function']\n</code></pre></p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#configurational-observables","title":"Configurational Observables","text":"<p>Fabricated data: <pre><code>temperatures = 300. * np.ones(n_frames)\npotential_energies = 1.0 * np.ones(n_frames)\nkinetic_energies = 2.0 * np.ones(n_frames)\n</code></pre></p> <p>Create a <code>temperature</code> group and populate the associated metadata:</p> <pre><code>temperature = observables.create_group('temperature')\ntemperature.attrs['type'] = types[0]\ntemperature['step'] = steps\ntemperature['time'] = times\ntemperature['time'].attrs['unit'] = str(time_unit.units)\ntemperature['time'].attrs['unit_factor'] = time_unit.magnitude\ntemperature['value'] = temperatures\ntemperature['value'].attrs['unit'] = str(temperature_unit.units)\ntemperature['value'].attrs['unit_factor'] = temperature_unit.magnitude\n</code></pre> <p>Create an <code>energy</code> group to hold various types of energies. Add : <pre><code>energies = observables.create_group('energy')\npotential_energy = energies.create_group('potential')\npotential_energy.attrs['type'] = types[0]\npotential_energy['step'] = steps\npotential_energy['time'] = times\npotential_energy['time'].attrs['unit'] = str(time_unit.units)\npotential_energy['time'].attrs['unit_factor'] = time_unit.magnitude\npotential_energy['value'] = potential_energies\npotential_energy['value'].attrs['unit'] = str(energy_unit.units)\npotential_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\nkinetic_energy = energies.create_group('kinetic')\nkinetic_energy.attrs['type'] = types[0]\nkinetic_energy['step'] = steps\nkinetic_energy['time'] = times\nkinetic_energy['time'].attrs['unit'] = str(time_unit.units)\nkinetic_energy['time'].attrs['unit_factor'] = time_unit.magnitude\nkinetic_energy['value'] = kinetic_energies\nkinetic_energy['value'].attrs['unit'] = str(energy_unit.units)\nkinetic_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\n</code></pre></p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#ensemble-average-observables","title":"Ensemble Average Observables","text":"<p>Fabricated data - the following represents radial distribution function (rdf) data calculated between molecule types <code>X</code> and <code>Y</code>, stored in <code>rdf_MOLX-MOLY.xvg</code>: <pre><code>      0.24 0.000152428\n     0.245 0.00457094\n      0.25  0.0573499\n     0.255   0.284764\n      0.26   0.842825\n     0.265    1.64705\n      0.27    2.37243\n     0.275    2.77916\n      0.28    2.80622\n     0.285    2.60082\n      0.29    2.27182\n      ...\n</code></pre></p> <p>Store the rdf data in a dictionary along with some relevant metadata:</p> <pre><code>rdf_XX = np.loadtxt('rdf_MOLX-MOLX.xvg')\nrdf_XY = np.loadtxt('rdf_MOLX-MOLY.xvg')\nrdf_YY = np.loadtxt('rdf_MOLY-MOLY.xvg')\nrdfs = {\n'MOLX-MOLX': {\n'n_bins': len(rdf_XX[:, 0]),\n'bins': rdf_XX[:, 0],\n'value': rdf_XX[:, 1],\n'type': 'molecular',\n'frame_start': 0,\n'frame_end': n_frames-1\n},\n'MOLX-MOLY': {\n'n_bins': len(rdf_XY[:, 0]),\n'bins': rdf_XY[:, 0],\n'value': rdf_XY[:, 1],\n'type': 'molecular',\n'frame_start': 0,\n'frame_end': n_frames-1\n},\n'MOLY-MOLY': {\n'n_bins': len(rdf_YY[:, 0]),\n'bins': rdf_YY[:, 0],\n'value': rdf_YY[:, 1],\n'type': 'molecular',\n'frame_start': 0,\n'frame_end': n_frames-1\n}\n}\n</code></pre> <p>Now create the <code>radial_distribution_functions</code> group under <code>observables</code> and store each imported rdf: <pre><code>radial_distribution_functions = observables.create_group('radial_distribution_functions')\nfor key in rdfs.keys():\nrdf = radial_distribution_functions.create_group(key)\nrdf.attrs['type'] = types[1]\nrdf['type'] = rdfs[key]['type']\nrdf['n_bins'] = rdfs[key]['n_bins']\nrdf['bins'] = rdfs[key]['bins']\nrdf['bins'].attrs['unit'] = str(length_unit.units)\nrdf['bins'].attrs['unit_factor'] = length_unit.magnitude\nrdf['value'] = rdfs[key]['value']\nrdf['frame_start'] = rdfs[key]['frame_start']\nrdf['frame_end'] = rdfs[key]['frame_end']\n</code></pre></p> <p>We can also store scalar ensemble average observables. Let's consider some fabricated diffusion constant data: <pre><code>Ds = {\n'MOLX': {\n'value': 1.0,\n'error_type': 'Pearson_correlation_coefficient',\n'errors': 0.98\n},\n'MOLY': {\n'value': 2.0,\n'error_type': 'Pearson_correlation_coefficient',\n'errors': 0.95\n}\n}\n</code></pre></p> <p>Create the <code>diffusion constants</code> group under <code>observables</code> and store the correspond (meta)data:</p> <pre><code>diffusion_constants = observables.create_group('diffusion_constants')\nfor key in Ds.keys():\ndiffusion_constant = diffusion_constants.create_group(key)\ndiffusion_constant.attrs['type'] = types[1]\ndiffusion_constant['value'] = Ds[key]['value']\ndiffusion_constant['value'].attrs['unit'] = str(diff_unit.units)\ndiffusion_constant['value'].attrs['unit_factor'] = diff_unit.magnitude\ndiffusion_constant['error_type'] = Ds[key]['error_type']\ndiffusion_constant['errors'] = Ds[key]['errors']\n</code></pre>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#time-correlation-observables","title":"Time Correlation Observables","text":"<p>Fabricated data - the following represents mean squared displacement (msd) data calculated for molecule type <code>X</code>, stored in <code>msd_MOLX.xvg</code>: <pre><code>         0           0\n         2   0.0688769\n         4    0.135904\n         6    0.203573\n         8    0.271162\n        10    0.339284\n        12    0.410115\n        14    0.477376\n        16    0.545184\n        18     0.61283\n        ...\n</code></pre></p> <p>Store the msd data in a dictionary along with some relevant metadata:</p> <pre><code>msd_X = np.loadtxt('msd_MOLX.xvg')\nmsd_Y = np.loadtxt('msd_MOLY.xvg')\nmsds = {\n'MOLX': {\n'n_times': len(msd_X[:, 0]),\n'times': msd_X[:, 0],\n'value': msd_X[:, 1],\n'type': 'molecular',\n'direction': 'xyz',\n'error_type': 'standard_deviation',\n'errors': np.zeros(len(msd_X[:, 0])),\n},\n'MOLY': {\n'n_times': len(msd_Y[:, 0]),\n'times': msd_Y[:, 0],\n'value': msd_Y[:, 1],\n'type': 'molecular',\n'direction': 'xyz',\n'error_type': 'standard_deviation',\n'errors': np.zeros(len(msd_Y[:, 0])),\n}\n}\n</code></pre> <p>Now create the <code>mean_squared_displacements</code> group under <code>observables</code> and store each imported rdf:</p> <pre><code>mean_squared_displacements = observables.create_group('mean_squared_displacements')\nmsd_unit = length_unit * length_unit\ndiff_unit = msd_unit / time_unit\nfor key in msds.keys():\nmsd = mean_squared_displacements.create_group(key)\nmsd.attrs['type'] = types[2]\nmsd['type'] = msds[key]['type']\nmsd['direction'] = msds[key]['direction']\nmsd['error_type'] = msds[key]['error_type']\nmsd['n_times'] = msds[key]['n_times']\nmsd['times'] = msds[key]['times']\nmsd['times'].attrs['unit'] = str(time_unit.units)\nmsd['times'].attrs['unit_factor'] = time_unit.magnitude\nmsd['value'] = msds[key]['value']\nmsd['value'].attrs['unit'] = str(msd_unit.units)\nmsd['value'].attrs['unit_factor'] = msd_unit.magnitude\nmsd['errors'] = msds[key]['errors']\n</code></pre>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#parameter-group","title":"Parameter Group","text":"<p>Using the json templates for force calculations and molecular dynamics workflows, the (meta)data can be written to the H5MD-NOMAD file using the following code:</p> <p>First, import the data extracted from the JSON templates: <pre><code>with open('force_calculations_metainfo.json') as json_file:\nforce_calculation_parameters = json.load(json_file)\nwith open('workflow_metainfo.json') as json_file:\nworkflow_parameters = json.load(json_file)\n</code></pre></p> <p>Then, create the appropriate container groups: <pre><code>parameters = h5_write.create_group('parameters')\nforce_calculations = parameters.create_group('force_calculations')\nworkflow = parameters.create_group('workflow')\n</code></pre></p> <p>Now, recursively write the (meta)data: <pre><code>def get_parameters_recursive(parameter_group, parameter_dict):\n# Store the parameters from parameter dict into an hdf5 file\nfor key, val in parameter_dict.items():\nif type(val) == dict:\nparam = val.get('value')\nif param is not None:\nparameter_group[key] = param\nunit = val.get('unit')\nif unit is not None:\nparameter_group[key].attrs['unit'] = unit\nelse:  # This is a subsection\nsubsection = parameter_group.create_group(key)\nsubsection = get_parameters_recursive(subsection, val)\nelse:\nif val is not None:\nparameter_group[key] = val\nreturn parameter_group\nforce_calculations = get_parameters_recursive(force_calculations, force_calculation_parameters)\nworkflow = get_parameters_recursive(workflow, workflow_parameters)\n</code></pre></p> <p>It's as simple as that! Now, we can upload our H5MD-NOMAD file directly to NOMAD and all the written (meta)data will be stored according to the standard NOMAD schema.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#accessing-an-h5md-nomad-file","title":"Accessing an H5MD-NOMAD file","text":"<p>The following functions are useful for accessing data from your H5MD-NOMAD file: <pre><code>def apply_unit(quantity, unit, unit_factor):\nfrom pint import UnitRegistry\nureg = UnitRegistry()\nif quantity is None:\nreturn\nif unit:\nunit = ureg(unit)\nunit *= unit_factor\nquantity *= unit\nreturn quantity\ndef decode_hdf5_bytes(dataset):\nif dataset is None:\nreturn\nelif type(dataset).__name__ == 'ndarray':\nif dataset == []:\nreturn dataset\ndataset = np.array([val.decode(\"utf-8\") for val in dataset]) if type(dataset[0]) == bytes else dataset\nelse:\ndataset = dataset.decode(\"utf-8\") if type(dataset) == bytes else dataset\nreturn dataset\ndef hdf5_attr_getter(source, path, attribute, default=None):\n'''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\nsection_segments = path.split('.')\nfor section in section_segments:\ntry:\nvalue = source.get(section)\nsource = value[-1] if isinstance(value, list) else value\nexcept Exception:\nreturn\nvalue = source.attrs.get(attribute)\nsource = value[-1] if isinstance(value, list) else value\nsource = decode_hdf5_bytes(source) if source is not None else default\nreturn source\ndef hdf5_getter(source, path, default=None):\n'''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\nsection_segments = path.split('.')\nfor section in section_segments:\ntry:\nvalue = source.get(section)\nunit = hdf5_attr_getter(source, section, 'unit')\nunit_factor = hdf5_attr_getter(source, section, 'unit_factor', default=1.0)\nsource = value[-1] if isinstance(value, list) else value\nexcept Exception:\nreturn\nif source is None:\nsource = default\nelif type(source) == h5py.Dataset:\nsource = source[()]\nsource = apply_unit(source, unit, unit_factor)\nreturn decode_hdf5_bytes(source)\n</code></pre></p> <p>Open your H5MD-NOMAD file with <code>h5py</code>: <pre><code>import h5py\nh5_read = h5py.File('test_h5md-nomad.h5', 'r')\n</code></pre></p> <p>Access a particular data set: <pre><code>potential_energies = h5_read['observables']['energy']['potential']['value']\nprint(potential_energies[()])\n</code></pre> result: <pre><code>array([1., 1., 1., 1., 1.])\n</code></pre></p> <p>Get the unit information for this quantity: <pre><code>unit = potential_energies.attrs['unit']\nunit_factor = potential_energies.attrs['unit_factor']\nprint(unit)\nprint(unit_factor)\n</code></pre></p> <p>results: <pre><code>joule\n1000.0\n</code></pre></p> <p>Alternatively, the above functions will return the dataset as python arrays, i.e., already applying <code>[()]</code> to the HDF5 element, and also apply the appropriate units where applicable: <pre><code>potential_energies = hdf5_getter(h5_read, 'observables.energy.potential.value')\nprint(potential_energies)\n</code></pre></p> <p>result: <pre><code>Magnitude\n[1000.0 1000.0 1000.0 1000.0 1000.0]\nUnits   joule\n</code></pre></p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#creating-a-topology-particles_group","title":"Creating a topology (<code>particles_group</code>)","text":"<p>This page demonstrates how to create a \"standard\" topology in H5MD-NOMAD. The demonstrated organization of molecules and monomers is identical to what other NOMAD parsers do to create a topology from native simulation files (e.g., outputs from GROMACS or LAMMPS). However, the user is free to deviate from this standard to create arbitrary organizations of particles, as described in Connectivity.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#standard-topology-structure-for-bonded-force-fields","title":"Standard topology structure for bonded force fields","text":"<p><pre><code>topology\n\u251c\u2500\u2500 molecule_group_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502       \u251c\u2500\u2500 monomer_1\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_1\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 monomer_2\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_2\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 molecule_group_2\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> Here, the first level of organization is the \"molecule group\". Molecule groups contain molecules of the same type. In other words, <code>molecule_group_1</code> and <code>molecule_group_2</code> represent distinct molecule types. At the next level of the hierarchy, each molecule within this group is stored (i.e., <code>molecule_1</code>, <code>molecule_2</code>, etc.). In the above example, <code>molecule_group_1</code> represents a polymer (or protein). Thus, below the molecule level, there is a \"monomer group level\". Similar to the molecule group, the monomer group organizes all monomers (of the parent molecule) that are of the same type. Thus, for <code>molecule_1</code> of <code>molecule_group_1</code>, <code>monomer_group_1</code> and <code>monomer_group_2</code> represent distinct types of monomers existing within the polymer. Then, below <code>monomer_group_1</code>, each monomer within this group is stored. Finally, beneath these individual monomers, only the metadata for that monomer is stored (i.e., no further organization levels). Note however, that metadata can be (and is) stored at each level of the hierarchy, but is left out of the illustration for clarity. Notice also that <code>molecule_group_2</code> is not a polymer. Thus, each molecule within this group stores only the corresponding metadata, and no further levels of organization.</p>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#creating-the-standard-hierarchy-from-an-mdanalysis-universe","title":"Creating the standard hierarchy from an MDAnalysis universe","text":"<p>We start from the perspective of the Writing an HDF5 file according to H5MD-NOMAD with python section, with identical imports and assuming that an MDAnalysis <code>universe</code> is already instantiated from the raw simulation files. As in the previous example, the <code>universe</code> containing the topology information is called <code>universe_topology</code>.</p> <p>The following functions will be useful for creating the topology:</p> <pre><code>def get_composition(children_names):\n'''\n    Given a list of children, return a compositional formula as a function of\n    these children. The format is &lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...\n    '''\nchildren_count_tup = np.unique(children_names, return_counts=True)\nformula = ''.join([f'{name}({count})' for name, count in zip(*children_count_tup)])\nreturn formula\ndef get_molecules_from_bond_list(n_particles: int, bond_list: List[int], particle_types: List[str] = None, particles_typeid=None):\n'''\n    Returns a dictionary with molecule info from the list of bonds\n    '''\nimport networkx\nsystem_graph = networkx.empty_graph(n_particles)\nsystem_graph.add_edges_from([(i[0], i[1]) for i in bond_list])\nmolecules = [system_graph.subgraph(c).copy() for c in networkx.connected_components(system_graph)]\nmol_dict = []\nfor i_mol, mol in enumerate(molecules):\nmol_dict.append({})\nmol_dict[i_mol]['indices'] = np.array(mol.nodes())\nmol_dict[i_mol]['bonds'] = np.array(mol.edges())\nmol_dict[i_mol]['type'] = 'molecule'\nmol_dict[i_mol]['is_molecule'] = True\nif particles_typeid is None and len(particle_types) == n_particles:\nmol_dict[i_mol]['names'] = [particle_types[int(x)] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\nif particle_types is not None and particles_typeid is not None:\nmol_dict[i_mol]['names'] = [particle_types[particles_typeid[int(x)]] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\nmol_dict[i_mol]['formula'] = get_composition(mol_dict[i_mol]['names'])\nreturn mol_dict\ndef is_same_molecule(mol_1: dict, mol_2: dict):\n'''\n    Checks whether the 2 input molecule dictionaries represent the same\n    molecule type, i.e., same particle types and corresponding bond connections.\n    '''\nif sorted(mol_1['names']) == sorted(mol_2['names']):\nmol_1_shift = np.min(mol_1['indices'])\nmol_2_shift = np.min(mol_2['indices'])\nmol_1_bonds_shift = mol_1['bonds'] - mol_1_shift\nmol_2_bonds_shift = mol_2['bonds'] - mol_2_shift\nbond_list_1 = [sorted((mol_1['names'][i], mol_1['names'][j])) for i, j in mol_1_bonds_shift]\nbond_list_2 = [sorted((mol_2['names'][i], mol_2['names'][j])) for i, j in mol_2_bonds_shift]\nbond_list_names_1, bond_list_counts_1 = np.unique(bond_list_1, axis=0, return_counts=True)\nbond_list_names_2, bond_list_counts_2 = np.unique(bond_list_2, axis=0, return_counts=True)\nbond_list_dict_1 = {bond[0] + '-' + bond[1]: bond_list_counts_1[i_bond] for i_bond, bond in enumerate(bond_list_names_1)}\nbond_list_dict_2 = {bond[0] + '-' + bond[1]: bond_list_counts_2[i_bond] for i_bond, bond in enumerate(bond_list_names_2)}\nif bond_list_dict_1 == bond_list_dict_2:\nreturn True\nreturn False\nreturn False\n</code></pre> <p>Then, we can create the topology structure from the MDAnalysis universe:</p> <pre><code>bond_list = universe_toponly.bonds._bix\nmolecules = get_molecules_from_bond_list(n_atoms, bond_list, particle_types=universe_toponly.atoms.types, particles_typeid=None)\n# create the topology\nmol_groups = []\nmol_groups.append({})\nmol_groups[0]['molecules'] = []\nmol_groups[0]['molecules'].append(molecules[0])\nmol_groups[0]['type'] = 'molecule_group'\nmol_groups[0]['is_molecule'] = False\nfor mol in molecules[1:]:\nflag_mol_group_exists = False\nfor i_mol_group in range(len(mol_groups)):\nif is_same_molecule(mol, mol_groups[i_mol_group]['molecules'][0]):\nmol_groups[i_mol_group]['molecules'].append(mol)\nflag_mol_group_exists = True\nbreak\nif not flag_mol_group_exists:\nmol_groups.append({})\nmol_groups[-1]['molecules'] = []\nmol_groups[-1]['molecules'].append(mol)\nmol_groups[-1]['type'] = 'molecule_group'\nmol_groups[-1]['is_molecule'] = False\nfor i_mol_group, mol_group in enumerate(mol_groups):\nmol_groups[i_mol_group]['formula'] = molecule_labels[i_mol_group] + '(' + str(len(mol_group['molecules'])) + ')'\nmol_groups[i_mol_group]['label'] = 'group_' + str(molecule_labels[i_mol_group])\nmol_group_indices = []\nfor i_molecule, molecule in enumerate(mol_group['molecules']):\nmolecule['label'] = molecule_labels[i_mol_group]\nmol_indices = molecule['indices']\nmol_group_indices.append(mol_indices)\nmol_resids = np.unique(universe_toponly.atoms.resindices[mol_indices])\nif mol_resids.shape[0] == 1:\ncontinue\nres_dict = []\nfor i_resid, resid in enumerate(mol_resids):\nres_dict.append({})\nres_dict[i_resid]['indices'] = np.where( universe_toponly.atoms.resindices[mol_indices] == resid)[0]\nres_dict[i_resid]['label'] = universe_toponly.atoms.resnames[res_dict[i_resid]['indices'][0]]\nres_dict[i_resid]['formula'] = get_composition(universe_toponly.atoms.names[res_dict[i_resid]['indices']])\nres_dict[i_resid]['is_molecule'] = False\nres_dict[i_resid]['type'] = 'monomer'\nres_groups = []\nres_groups.append({})\nres_groups[0]['residues'] = []\nres_groups[0]['residues'].append(res_dict[0])\nres_groups[0]['label'] = 'group_' + res_dict[0]['label']\nres_groups[0]['type'] = 'monomer_group'\nres_groups[0]['is_molecule'] = False\nfor res in res_dict[1:]:\nflag_res_group_exists = False\nfor i_res_group in range(len(res_groups)):\nif res['label'] == res_groups[i_res_group]['label']:\nres_groups[i_res_group]['residues'].append(res)\nflag_res_group_exists = True\nbreak\nif not flag_res_group_exists:\nres_groups.append({})\nres_groups[-1]['residues'] = []\nres_groups[-1]['residues'].append(res)\nres_groups[-1]['label'] = 'group_' + res['label']\nres_groups[-1]['formula'] = get_composition(universe_toponly.atoms.names[res['indices']])\nres_groups[-1]['type'] = 'monomer_group'\nres_groups[-1]['is_molecule'] = False\nmolecule['formula'] = ''\nfor res_group in res_groups:\nres_group['formula'] = res_group['residues'][0]['label'] + '(' + str(len(res_group['residues'])) + ')'\nmolecule['formula'] += res_group['formula']\nres_group_indices = []\nfor res in res_group['residues']:\nres_group_indices.append(res['indices'])\nres_group['indices'] = np.concatenate(res_group_indices)\nmol_group['indices'] = np.concatenate(mol_group_indices)\nmolecule['residue_groups'] = res_groups\n</code></pre>"},{"location":"custom_schemas/h5md/howto-H5MD-NOMAD/#writing-the-topology-to-an-h5md-nomad-file","title":"Writing the topology to an H5MD-NOMAD file","text":"<p>Here we assume an H5MD-NOMAD file has already been created, as demonstrated on the Writing an HDF5 file according to H5MD-NOMAD with python section, and that the <code>connectivity</code> group was created under the root level.</p> <p>Now, create the <code>particles_group</code> group under <code>connectivity</code> within our HDF5-NOMAD file: <pre><code>topology_keys = ['type', 'formula', 'particles_group', 'label', 'is_molecule', 'indices']\ncustom_keys = ['molecules', 'residue_groups', 'residues']\ntopology = connectivity.create_group('particles_group')\nfor i_mol_group, mol_group in enumerate(mol_groups):\nhdf5_mol_group = topology.create_group('group_' + molecule_labels[i_mol_group])\nfor mol_group_key in mol_group.keys():\nif mol_group_key not in topology_keys + custom_keys:\ncontinue\nif mol_group_key != 'molecules':\nhdf5_mol_group[mol_group_key] = mol_group[mol_group_key]\nelse:\nhdf5_molecules = hdf5_mol_group.create_group('particles_group')\nfor i_molecule, molecule in enumerate(mol_group[mol_group_key]):\nhdf5_mol = hdf5_molecules.create_group('molecule_' + str(i_molecule))\nfor mol_key in molecule.keys():\nif mol_key not in topology_keys + custom_keys:\ncontinue\nif mol_key != 'residue_groups':\nhdf5_mol[mol_key] = molecule[mol_key]\nelse:\nhdf5_residue_groups = hdf5_mol.create_group('particles_group')\nfor i_res_group, res_group in enumerate(molecule[mol_key]):\nhdf5_res_group = hdf5_residue_groups.create_group('residue_group_' + str(i_res_group))\nfor res_group_key in res_group.keys():\nif res_group_key not in topology_keys + custom_keys:\ncontinue\nif res_group_key != 'residues':\nhdf5_res_group[res_group_key] = res_group[res_group_key]\nelse:\nhdf5_residues = hdf5_res_group.create_group('particles_group')\nfor i_res, res in enumerate(res_group[res_group_key]):\nhdf5_res = hdf5_residues.create_group('residue_' + str(i_res))\nfor res_key in res.keys():\nif res_key not in topology_keys:\ncontinue\nif res[res_key] is not None:\nhdf5_res[res_key] = res[res_key]\n</code></pre></p>"},{"location":"custom_schemas/h5md/observables/","title":"The observables group","text":"<p>The initial H5MD proposed a simple and flexible schema for the general storage of observable info, defined roughly as \"macroscopic observables\" or \"averages of a property over many particles\", as H5MD elements:</p> <pre><code>observables\n \\-- &lt;observable1&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames]\n \\-- &lt;observable2&gt;\n |    \\-- step: Integer[N_frames]\n |    \\-- time: Float[N_frames]\n |    \\-- value: &lt;type&gt;[N_frames][D]\n \\-- &lt;group1&gt;\n |    \\-- &lt;observable3&gt;\n |         \\-- step: Integer[N_frames]\n |         \\-- time: Float[N_frames]\n |         \\-- value: &lt;type&gt;[N_frames][D][D]\n \\-- &lt;observable4&gt;: &lt;type&gt;[]\n \\-- ...\n</code></pre> <p></p> <p>As depicted above, observables representing only a subset of the particles may be stored in appropriate subgroups similar to the <code>particles</code> tree. H5MD-NOMAD does support the organization of observables into subgroups (as discussed in more detail below). However, grouping by particle groups is not fully supported in the sense that there is currently no metadata storing the corresponding indices of the relevant particles subgroup. Additionally, since only the <code>all</code> particles group is parsed, information about the named subgroup will not be stored anywhere in the archive. Thus, we recommend for now that only observables relevant to the <code>all</code> particles subgroup are stored within this section.</p>"},{"location":"custom_schemas/h5md/observables/#h5md-nomad-observables","title":"H5MD-NOMAD observables","text":"<p>H5MD-NOMAD extends H5MD observable storage by 1. specifying standard observable types with associated metadata and 2. providing standardized specifications for some common observables. In contrast to the schema above, a more restrictive structure is required:</p> <pre><code>observables\n \\-- &lt;observable_type_1&gt;\n |    \\-- &lt;observable_1_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n \\-- &lt;observable_type_2&gt;\n |    \\-- &lt;observable_2_label_1&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- &lt;observable_2_label_2&gt;\n |    |    +-- type: String[]\n |    |    \\-- ...\n |    \\-- ...\n \\-- ...\n</code></pre> <p>Here, each <code>observable_type</code> corresponds to a particular group of observables, e.g., to be plotted together in a single plot. The given name for this group could be generic, e.g., <code>radial distribution function</code>, or more specific, e.g., <code>molecular radial distribution function for solvents</code>. The latter may be useful in case multiple groupings of a single type of observable are needed. Each <code>observable_label</code> then corresponds to a specific name for an individual instance of this observable type. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>observable_label</code> might be set to <code>A-B</code>.</p> <p>Finally, H5MD-NOMAD has added the observable <code>type</code> as an attribute of each observable: The following observable types are supported:</p> <p></p> <p><code>configurational</code> :   An observable that is computed for each individual configuration, with the following general structure:</p> <pre><code>observables\n \\-- &lt;configurational_subgroup&gt;\n |    \\-- &lt;label_1&gt;\n |    |    +-- type: \"configurational\"\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][M]\n |    \\-- ...\n \\-- ...\n</code></pre> <p>where <code>M</code> is the dimension of the observable. This section may also be used to store per-particle quantities/attributes that are not currently supported as standardized H5MD-NOMAD elements for particles group, in which case <code>value</code> will have dimensions <code>[N_frames][N_part][M]</code>.</p> <p></p> <p><code>ensemble_average</code> :   An observable that is computed by averaging over multiple configurations, with the following generic structure:</p> <pre><code>observables\n \\-- &lt;ensemble_average_subgroup&gt;\n |    \\-- &lt;label_1&gt;\n |    |    +-- type: \"ensemble_average\"\n |    |    \\-- (n_variables): Integer\n |    |    \\-- (variables_name): String[n_variables][]\n |    |    \\-- (n_bins): Integer[]\n |    |    \\-- bins: Float[n_bins][]\n |    |    \\-- value: &lt;type&gt;[n_bins][]\n |    |    \\-- (frame_start): Integer\n |    |    \\-- (frame_end): Integer\n |    |    \\-- (n_smooth): Integer\n |    |    \\-- (type): String[]\n |    |    \\-- (error_type): String[]\n |    |    \\-- (errors): Float[n_bins]\n |    |    \\-- (error_labels): String[]\n |    |    \\-- (frame_end): Integer\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n |    \\-- ...\n \\-- ...\n</code></pre> <ul> <li> <p><code>n_variables</code> :   dimensionality of the observable. Can also be inferred from leading dimension of <code>bins</code>.</p> </li> <li> <p><code>variables_name</code> :   name/description of the independent variables along which the observable is defined.</p> </li> <li> <p><code>n_bins</code> :   number of bins along each dimension of the observable. Either single Integer for 1-D observables, or a list of Integers for multi-dimensional observable. Can also be inferred from dimensions of <code>bins</code>.</p> </li> <li> <p><code>bins</code> :   value of the bins used for calculating the observable along each dimension of the observable.</p> </li> <li> <p><code>value</code> :   value of the calculated ensemble average at each bin.</p> </li> <li> <p><code>frame_start</code> :   trajectory frame index at which the averaging begins. This index must correspond to the list of steps and times in <code>particles.all.position</code>.</p> </li> <li> <p><code>frame_end</code> :   trajectory frame index at which the averaging ends. This index must correspond to the list of steps and times in <code>particles.all.position</code>.</p> </li> <li> <p><code>n_smooth</code> :   number of bins over which the running average was computed for <code>value</code>.</p> </li> <li> <p><code>type</code> :   Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level.</p> </li> </ul> <ul> <li> <p><code>error_type</code> :   describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>.</p> </li> <li> <p><code>errors</code> :   value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>.</p> </li> <li> <p><code>error_labels</code> :   describes the error along individual dimensions for multi-D errors.</p> </li> <li> <p><code>&lt;custom_dataset&gt;</code> :   additional metadata may be given as necessary.</p> </li> </ul> <p></p> <p><code>time_correlation</code> :   An observable that is computed by calculating correlations between configurations in time, with the following general structure:</p> <pre><code>observables\n \\-- &lt;time_correlation_subgroup&gt;\n |    \\-- &lt;label_1&gt;\n |    |    +-- type: \"time_correlation\"\n |    |    \\-- (direction): String[]\n |    |    \\-- (n_times): Integer[]\n |    |    \\-- times: Float[n_times][]\n |    |    \\-- value: &lt;type&gt;[n_bins][]\n |    |    \\-- (type): String[]\n |    |    \\-- (error_type): String[]\n |    |    \\-- (errors): Float[n_bins]\n |    |    \\-- (error_labels): String[]\n |    |    \\-- (&lt;custom_dataset&gt;): &lt;type&gt;[]\n |    \\-- ...\n \\-- ...\n</code></pre> <ul> <li> <p><code>label</code> :   describes the particles involved in determining the property. For example, for a radial distribution function between particles of type <code>A</code> and <code>B</code>, <code>label</code> might be set to <code>A-B</code></p> </li> <li> <p><code>direction</code> :   allowed values of <code>x</code>, <code>y</code>, <code>z</code>, <code>xy</code>, <code>yz</code>, <code>xz</code>, <code>xyz</code>. The direction/s used for calculating the correlation function.</p> </li> <li> <p><code>n_times</code> :   number of times windows for the calculation of the correlation function. Can also be inferred from dimensions of <code>times</code>.</p> </li> <li> <p><code>times</code> :   time values used for calculating the correlation function (i.e., \u0394t values).</p> </li> <li> <p><code>value</code> :   value of the calculated correlation function at each time.</p> </li> <li> <p><code>type</code> :   Allowed values of <code>molecular</code> or <code>atomic</code>. Categorizes if the observable is calculated at the molecular or atomic level.</p> </li> </ul> <ul> <li> <p><code>error_type</code> :   describes the type of error reported for this observable. Examples: <code>Pearson correlation coefficient</code>, <code>mean squared error</code>.</p> </li> <li> <p><code>errors</code> :   value of the error at each bin. Can be multidimensional with corresponding label stored in <code>error_labels</code>.</p> </li> <li> <p><code>error_labels</code> :   describes the error along individual dimensions for multi-D errors.</p> </li> <li> <p><code>&lt;custom_dataset&gt;</code> :   additional metadata may be given as necessary.</p> </li> </ul> <p>A list of standardized observables can be found HERE.</p>"},{"location":"custom_schemas/h5md/parameters/","title":"The parameters group","text":"<p>The initial H5MD proposed a simple and flexible schema for the storage of general \"parameter\" information within the <code>parameters</code> group, with the following structure:</p> <pre><code>parameters\n +-- &lt;user_attribute1&gt;\n \\-- &lt;user_data1&gt;\n \\-- &lt;user_group1&gt;\n |    \\-- &lt;user_data2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>In contrast, the H5MD-NOMAD schema calls for very specific structures to be used when storing parameter information. While the previous groups have attempted to stay away from enforcing NOMAD-specific data structures on the user, instead opting for more intuitive and generally-convenient structures, the <code>parameters</code> group utilizes already-existing metadata and structures within NOMAD to efficiently import simulation parameters in a way that is searchable and comparable to simulations performed by other users.</p> <p>In this way, the H5MD-NOMAD <code>parameters</code> group has the following structure:</p> <pre><code>parameters\n \\-- &lt;parameter_subgroup_1&gt;\n |    \\-- ...\n \\-- &lt;parameter_subgroup_2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre> <p>The subgroups <code>force_calculations</code> and <code>workflow</code> are supported. The following pages describe the detailed data structures for these subgroups, using the NOMAD MetaInfo definitions for each underlying <code>Quantity</code>. Please note that:</p> <ol> <li> <p>Quantities with <code>type=MEnum()</code> are restricted to the provided allowed values.</p> </li> <li> <p>The unit given in the MetaInfo definition does not have to be used within the H5MD-NOMAD file, however, the dimensionality of the unit should match.</p> </li> </ol>"},{"location":"custom_schemas/h5md/parameters/#force-calculations","title":"Force calculations","text":"<p>The <code>force_calculations</code> group contains the parameters for force calculations according to the force field during a molecular dynamics run.</p> <p></p> <p>The following json template illustrates the structure of the <code>force_calculations</code> group, with example values for clarity:</p> <pre><code>{\n\"vdw_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n\"coulomb_type\": \"particle_mesh_ewald\",\n\"coulomb_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"},\n\"neighbor_searching\": {\n\"neighbor_update_frequency\": 1,\n\"neighbor_update_cutoff\": {\"value\": 1.2, \"unit\": \"nm\"}\n}\n}\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>vdw_cutoff</code> :</p> <pre><code>Quantity(\n        type=np.float64,\n        shape=[],\n        unit='m',\n        description='''\n        Cutoff for calculating VDW forces.\n        ''')\n</code></pre> </li> <li> <p><code>coulomb_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('cutoff', 'ewald', 'multilevel_summation', 'particle_mesh_ewald',\n            'particle_particle_particle_mesh', 'reaction_field'),\n    shape=[],\n    description='''\n    Method used for calculating long-ranged Coulomb forces.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"Cutoff\"`          | Simple cutoff scheme. |\n\n    | `\"Ewald\"` | Standard Ewald summation as described in any solid-state physics text. |\n\n    | `\"Multi-Level Summation\"` |  D. Hardy, J.E. Stone, and K. Schulten,\n    [Parallel. Comput. **35**, 164](https://doi.org/10.1016/j.parco.2008.12.005)|\n\n    | `\"Particle-Mesh-Ewald\"`        | T. Darden, D. York, and L. Pedersen,\n    [J. Chem. Phys. **98**, 10089 (1993)](https://doi.org/10.1063/1.464397) |\n\n    | `\"Particle-Particle Particle-Mesh\"` | See e.g. Hockney and Eastwood, Computer Simulation Using Particles,\n    Adam Hilger, NY (1989). |\n\n    | `\"Reaction-Field\"` | J.A. Barker and R.O. Watts,\n    [Mol. Phys. **26**, 789 (1973)](https://doi.org/10.1080/00268977300102101)|\n    ''')\n</code></pre> </li> <li> <p><code>coulomb_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    Cutoff for calculating short-ranged Coulomb forces.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_searching</code> : Section containing the parameters for neighbor searching/lists during a molecular dynamics run.</p> </li> <li> <p><code>neighbor_update_frequency</code> :</p> <pre><code>Quantity(\n    type=int,\n    shape=[],\n    description='''\n    Number of timesteps between updating the neighbor list.\n    ''')\n</code></pre> </li> <li> <p><code>neighbor_update_cutoff</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='m',\n    description='''\n    The distance cutoff for determining the neighbor list.\n    ''')\n</code></pre> </li> </ul>"},{"location":"custom_schemas/h5md/parameters/#the-molecular-dynamics-workflow","title":"The molecular dynamics workflow","text":"<p>The <code>workflow</code> group contains the parameters for any type of workflow. Here we describe the specific case of the well-defined <code>molecular_dynamics</code> workflow. Custom workflows are described in detail HERE.</p> <p></p> <p>The following json template illustrates the structure of the <code>molecular_dynamics</code> subsection of the <code>workflow</code> group, with example values for clarity:</p> <pre><code>{\n\"molecular_dynamics\": {\n\"thermodynamic_ensemble\": \"NPT\",\n\"integrator_type\": \"langevin_leap_frog\",\n\"integration_timestep\": {\"value\": 2e-15, \"unit\": \"ps\"},\n\"n_steps\": 20000000,\n\"coordinate_save_frequency\": 10000,\n\"velocity_save_frequency\": null,\n\"force_save_frequency\": null,\n\"thermodynamics_save_frequency\": null,\n\"thermostat_parameters\": {\n\"thermostat_type\": \"langevin_leap_frog\",\n\"reference_temperature\": {\"value\": 300.0, \"unit\": \"kelvin\"},\n\"coupling_constant\": {\"value\": 1.0, \"unit\": \"ps\"}},\n\"barostat_parameters\": {\n\"barostat_type\": \"berendsen\",\n\"coupling_type\": \"isotropic\",\n\"reference_pressure\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], \"unit\": \"bar\"},\n\"coupling_constant\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]},\n\"compressibility\": {\"value\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]}\n}\n}\n}\n</code></pre> <p>In the following, we provide the NOMAD definitions for each of these quantities:</p> <ul> <li> <p><code>thermodynamic_ensemble</code> :</p> <pre><code>Quantity(\n    type=MEnum('NVE', 'NVT', 'NPT', 'NPH'),\n    shape=[],\n    description='''\n    The type of thermodynamic ensemble that was simulated.\n\n    Allowed values are:\n\n    | Thermodynamic Ensemble          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"NVE\"`           | Constant number of particles, volume, and energy |\n\n    | `\"NVT\"`           | Constant number of particles, volume, and temperature |\n\n    | `\"NPT\"`           | Constant number of particles, pressure, and temperature |\n\n    | `\"NPH\"`           | Constant number of particles, pressure, and enthalpy |\n    ''')\n</code></pre> </li> <li> <p><code>integrator_type</code> :         Quantity(             type=MEnum(                 'brownian', 'conjugant_gradient', 'langevin_goga',                 'langevin_schneider', 'leap_frog', 'rRESPA_multitimescale', 'velocity_verlet'             ),             shape=[],             description='''             Name of the integrator.</p> <pre><code>    Allowed values are:\n\n    | Integrator Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"leap_frog\"`          | R.W. Hockney, S.P. Goel, and J. Eastwood,\n    [J. Comp. Phys. **14**, 148 (1974)](https://doi.org/10.1016/0021-9991(74)90010-2) |\n\n    | `\"velocity_verlet\"` | W.C. Swope, H.C. Andersen, P.H. Berens, and K.R. Wilson,\n    [J. Chem. Phys. **76**, 637 (1982)](https://doi.org/10.1063/1.442716) |\n\n    | `\"rRESPA_multitimescale\"` | M. Tuckerman, B. J. Berne, and G. J. Martyna\n    [J. Chem. Phys. **97**, 1990 (1992)](https://doi.org/10.1063/1.463137) |\n    ''')\n</code></pre> </li> <li> <p><code>integration_timestep</code> :         Quantity(             type=np.float64,             shape=[],             unit='s',             description='''             The timestep at which the numerical integration is performed.             ''')</p> </li> <li> <p><code>n_steps</code> :         Quantity(             type=int,             shape=[],             description='''             Number of timesteps performed.             ''')</p> </li> <li> <p><code>coordinate_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the coordinates.             ''')</p> </li> <li> <p><code>velocity_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the velocities.             ''')</p> </li> <li> <p><code>force_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the forces.             ''')</p> </li> <li> <p><code>thermodynamics_save_frequency</code> :         Quantity(             type=int,             shape=[],             description='''             The number of timesteps between saving the thermodynamic quantities.             ''')</p> </li> <li> <p><code>thermostat_parameters</code> :  Section containing the parameters pertaining to the thermostat for a molecular dynamics run.</p> </li> <li> <p><code>thermostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('andersen', 'berendsen', 'brownian', 'langevin_goga', 'langevin_schneider', 'nose_hoover', 'velocity_rescaling',\n            'velocity_rescaling_langevin'),\n    shape=[],\n    description='''\n    The name of the thermostat used for temperature control. If skipped or an empty string is used, it\n    means no thermostat was applied.\n\n    Allowed values are:\n\n    | Thermostat Name        | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"andersen\"`           | H.C. Andersen, [J. Chem. Phys.\n    **72**, 2384 (1980)](https://doi.org/10.1063/1.439486) |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"brownian\"`           | Brownian Dynamics |\n\n    | `\"langevin_goga\"`           | N. Goga, A. J. Rzepiela, A. H. de Vries,\n    S. J. Marrink, and H. J. C. Berendsen, [J. Chem. Theory Comput. **8**, 3637 (2012)]\n    (https://doi.org/10.1021/ct3000876) |\n\n    | `\"langevin_schneider\"`           | T. Schneider and E. Stoll,\n    [Phys. Rev. B **17**, 1302](https://doi.org/10.1103/PhysRevB.17.1302) |\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"velocity_rescaling\"` | G. Bussi, D. Donadio, and M. Parrinello,\n    [J. Chem. Phys. **126**, 014101 (2007)](https://doi.org/10.1063/1.2408420) |\n\n    | `\"velocity_rescaling_langevin\"` | G. Bussi and M. Parrinello,\n    [Phys. Rev. E **75**, 056707 (2007)](https://doi.org/10.1103/PhysRevE.75.056707) |\n    ''')\n</code></pre> </li> <li> <p><code>reference_temperature</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kelvin',\n    description='''\n    The target temperature for the simulation.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='s',\n    description='''\n    The time constant for temperature coupling. Need to describe what this means for the various\n    thermostat options...\n    ''')\n</code></pre> </li> <li> <p><code>effective_mass</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[],\n    unit='kilogram',\n    description='''\n    The effective or fictitious mass of the temperature resevoir.\n    ''')\n</code></pre> </li> <li> <p><code>barostat_parameters</code> : Section containing the parameters pertaining to the barostat for a molecular dynamics run.</p> </li> <li> <p><code>barostat_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('berendsen', 'martyna_tuckerman_tobias_klein', 'nose_hoover', 'parrinello_rahman', 'stochastic_cell_rescaling'),\n    shape=[],\n    description='''\n    The name of the barostat used for temperature control. If skipped or an empty string is used, it\n    means no barostat was applied.\n\n    Allowed values are:\n\n    | Barostat Name          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `\"\"`                   | No thermostat               |\n\n    | `\"berendsen\"`          | H. J. C. Berendsen, J. P. M. Postma,\n    W. F. van Gunsteren, A. DiNola, and J. R. Haak, [J. Chem. Phys.\n    **81**, 3684 (1984)](https://doi.org/10.1063/1.448118) |\n\n    | `\"martyna_tuckerman_tobias_klein\"` | G.J. Martyna, M.E. Tuckerman, D.J. Tobias, and M.L. Klein,\n    [Mol. Phys. **87**, 1117 (1996)](https://doi.org/10.1080/00268979600100761);\n    M.E. Tuckerman, J. Alejandre, R. L\u00f3pez-Rend\u00f3n, A.L. Jochim, and G.J. Martyna,\n    [J. Phys. A. **59**, 5629 (2006)](https://doi.org/10.1088/0305-4470/39/19/S18)|\n\n    | `\"nose_hoover\"`        | S. Nos\u00e9, [Mol. Phys. **52**, 255 (1984)]\n    (https://doi.org/10.1080/00268978400101201); W.G. Hoover, [Phys. Rev. A\n    **31**, 1695 (1985) |\n\n    | `\"parrinello_rahman\"`        | M. Parrinello and A. Rahman,\n    [J. Appl. Phys. **52**, 7182 (1981)](https://doi.org/10.1063/1.328693);\n    S. Nos\u00e9 and M.L. Klein, [Mol. Phys. **50**, 1055 (1983) |\n\n    | `\"stochastic_cell_rescaling\"` | M. Bernetti and G. Bussi,\n    [J. Chem. Phys. **153**, 114107 (2020)](https://doi.org/10.1063/1.2408420) |\n    ''')\n</code></pre> </li> <li> <p><code>coupling_type</code> :</p> <pre><code>Quantity(\n    type=MEnum('isotropic', 'semi_isotropic', 'anisotropic'),\n    shape=[],\n    description='''\n    Describes the symmetry of pressure coupling. Specifics can be inferred from the `coupling constant`\n\n    | Type          | Description                               |\n\n    | ---------------------- | ----------------------------------------- |\n\n    | `isotropic`          | Identical coupling in all directions. |\n\n    | `semi_isotropic` | Identical coupling in 2 directions. |\n\n    | `anisotropic`        | General case. |\n    ''')\n</code></pre> </li> <li> <p><code>reference_pressure</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='pascal',\n    description='''\n    The target pressure for the simulation, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal.\n    ''')\n</code></pre> </li> <li> <p><code>coupling_constant</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='s',\n    description='''\n    The time constants for pressure coupling, stored in a 3x3 matrix, indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. 0 values along the off-diagonal\n    indicate no-coupling between these directions.\n    ''')\n</code></pre> </li> <li> <p><code>compressibility</code> :</p> <pre><code>Quantity(\n    type=np.float64,\n    shape=[3, 3],\n    unit='1 / pascal',\n    description='''\n    An estimate of the system's compressibility, used for box rescaling, stored in a 3x3 matrix indicating the values for individual directions\n    along the diagonal, and coupling between directions on the off-diagonal. If None, it may indicate that these values\n    are incorporated into the coupling_constant, or simply that the software used uses a fixed value that is not available in\n    the input/output files.\n    ''')\n</code></pre> </li> </ul>"},{"location":"custom_schemas/h5md/particles/","title":"The particles group","text":"<p>Particle attributes, i.e., information about each particle in the system, are stored within the <code>particles</code> group. According to the original H5MD schema, the <code>particles</code> group is a container for subgroups that represent different subsets of the system under consideration. For simplicity of parsing, H5MD-NOMAD currently requires one such group, labeled <code>all</code>, to contain all the particles and corresponding attributes to be stored in the NOMAD archive. Additional particle groups will be ignored.</p> <p>For each dataset, the ordering of indices (whenever relevant) is as follows: frame index, particle index, dimension index. Thus, the contents of the <code>particles</code> group for a trajectory with <code>N_frames</code> frames and <code>N_part</code> particles in a <code>D</code>-dimensional space can be represented:</p> <pre><code>particles\n \\-- all\n |    \\-- box\n |    \\-- (&lt;time-dependent_vector_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part][D]\n |    \\-- (&lt;time-dependent_scalar_attribute&gt;)\n |    |    \\-- step: Integer[N_frames]\n |    |    \\-- time: Float[N_frames]\n |    |    \\-- value: &lt;type&gt;[N_frames][N_part]\n |    \\-- (&lt;time-independent_vector_attribute&gt;): &lt;type&gt;[N_part][D]\n |    \\-- (&lt;time-independent_scalar_attribute&gt;): &lt;type&gt;[N_part]\n |    \\-- ...\n \\-- &lt;group2&gt;\n      \\-- ...\n</code></pre>"},{"location":"custom_schemas/h5md/particles/#standardized-h5md-elements-for-particles-group","title":"Standardized H5MD elements for particles group","text":"<p><code>position</code> :   (required for parsing other particle attributes) An element that describes the particle positions as coordinate vectors of <code>Float</code> or <code>Integer</code> type.</p> <p><code>velocity</code> :   An element that contains the velocities for each particle as a vector of     <code>Float</code> or <code>Integer</code> type.</p> <p><code>force</code> :   An element that contains the total forces (i.e., the accelerations     multiplied by the particle mass) for each particle as a vector of <code>Float</code>     or <code>Integer</code> type.</p> <p><code>mass</code> :   An element that holds the mass for each particle as a scalar of <code>Float</code>     type.</p> <p><code>image</code> :   (currently unused in H5MD-NOMAD)</p> Details <p>An element that represents periodic images of the box as coordinate vectors of <code>Float</code> or <code>Integer</code> type and allows one to compute for each particle its absolute position in space. If <code>image</code> is present, <code>position</code> must be present as well. For time-dependent data, the <code>step</code> and <code>time</code> datasets of <code>image</code> must equal those of <code>position</code>, which must be accomplished by hard-linking the respective datasets.</p> <p><code>species</code> :   (currently unused in H5MD-NOMAD)</p> Details <p>An element that describes the species for each particle, i.e., its atomic or chemical identity, as a scalar of <code>Enumeration</code> or <code>Integer</code> data type. Particles of the same species are assumed to be identical with respect to their properties and unbonded interactions.</p> <p><code>id</code> : (currently unused in H5MD-NOMAD)</p> Details <p>An element that holds a scalar identifier for each particle of <code>Integer</code> type, which is unique within the given particle subgroup. The <code>id</code> serves to identify particles over the course of the simulation in the case when the order of the particles changes, or when new particles are inserted and removed. If <code>id</code> is absent, the identity of the particles is given by their index in the <code>value</code> datasets of the elements within the same subgroup.</p> <p><code>charge</code> :   An element that contains the charge associated to each particle as a     scalar, of <code>Integer</code> or <code>Float</code> type.</p>"},{"location":"custom_schemas/h5md/particles/#standardized-h5md-nomad-elements-for-particles-group","title":"Standardized H5MD-NOMAD elements for particles group","text":"<p><code>species_label</code> :    An element that holds a label (fixed-length string datatype) for each particle. This label denotes the fundamental species type of the particle (e.g., the chemical element label for atoms), regardless of its given interactions within the model. Both time-independent and time-dependent <code>species_label</code> elements are supported.</p> <p><code>model_label</code> :   An element that holds a label (fixed-length string datatype) for each particle. This label denotes the type of particle with respect to the given interactions within the model (e.g., force field) Currently only time-independent species labels are supported.</p>"},{"location":"custom_schemas/h5md/particles/#non-standard-elements-in-particles-group","title":"Non-standard elements in particles group","text":"<p>All non-standard elements within the particles group are currently ignored by the NOMAD H5MD parser. In principle, one can store additional custom attributes as configuration-specific observables (see The observables group).</p>"},{"location":"custom_schemas/h5md/particles/#the-simulation-box-subgroup","title":"The simulation box subgroup","text":"<p>Information about the simulation box is stored in a subgroup named <code>box</code>, within the relevant particles group (<code>all</code> in our case). Both time-independent and time-dependent box information are supported (i.e. via the <code>edges</code> element). Because the <code>box</code> group is specific to a particle group of particles, time-dependent boxes must contain <code>step</code> and <code>time</code> datasets that exactly match those of the corresponding <code>position</code> group. In principal, this should be accomplished by hard-linking the respective datasets. In practice, H5MD-NOMAD currently assumes that this is the case (i.e., the box group <code>step</code> and <code>time</code> information is unused), and simply checks that <code>edges.value</code> has the same leading dimension as <code>position</code>.</p> <p>The structure of the <code>box</code> group is as follows:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- (edges)\n</code></pre> <p><code>dimension</code> :   An attribute that stores the spatial dimension <code>D</code> of the simulation box     and is of <code>Integer</code> datatype and scalar dataspace.</p> <p><code>boundary</code> :    An attribute, of boolean datatype (changed from string to boolean in H5MD-NOMAD) and of simple dataspace of rank 1 and size <code>D</code>, that specifies the boundary condition of the box along each dimension, i.e., <code>True</code> implies periodic boundaries are applied in the corresponding dimension. If all values in <code>boundary</code> are <code>False</code>, <code>edges</code> may be omitted.</p> <p><code>edges</code> :   A <code>D</code>-dimensional vector or a <code>D</code> \u00d7 <code>D</code> matrix, depending on the geometry of the box, of <code>Float</code> or <code>Integer</code> type. Only cuboid and triclinic boxes are allowed. If <code>edges</code> is a vector, it specifies the space diagonal of a cuboid-shaped box. If <code>edges</code> is a matrix, the box is of triclinic shape with the edge vectors given by the rows of the matrix. For a time-dependent box, a cuboid geometry is encoded by a dataset <code>value</code> (within the H5MD element) of rank 2 (1 dimension for the time and 1 for the vector) and a triclinic geometry by a dataset <code>value</code> of rank 3 (1 dimension for the time and 2 for the matrix). For a time-independent box, a cuboid geometry is encoded by a dataset <code>edges</code> of rank 1 and a triclinic geometry by a dataset of rank 2.</p> <p>For instance, a cuboid box that changes in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges\n                \\-- step: Integer[variable]\n                \\-- time: Float[variable]\n                \\-- value: &lt;type&gt;[variable][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>. A triclinic box that is fixed in time would appear as:</p> <pre><code>particles\n \\-- all\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges: &lt;type&gt;[D][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/","title":"Quick Start - H5MD basics","text":"<p>The following was duplicated or summarized from the H5MD webpage.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#file-format","title":"File format","text":"<p>H5MD structures are stored in the HDF5 file format version 0 or later. It is recommended to use the HDF5 file format version 2, which includes the implicit tracking of the creation and modification times of the file and of each of its objects.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#notation-and-naming","title":"Notation and naming","text":"<p>HDF5 files are organized into groups and datasets, summarized as objects, which form a tree structure with the datasets as leaves. Attributes can be attached to each object. The H5MD specification adopts this naming and uses the following notation to depict the tree or its subtrees:</p> <p><code>\\-- item</code> :   An object within a group, that is either a dataset or a group. If it is a     group itself, the objects within the group are indented by five spaces with     respect to the group name.</p> <p><code>+-- attribute</code> :   An attribute, that relates either to a group or a dataset.</p> <p><code>\\-- data: &lt;type&gt;[dim1][dim2]</code> :   A dataset with array dimensions <code>dim1</code> by <code>dim2</code> and of type <code>&lt;type&gt;</code>. The     type is taken from <code>Enumeration</code>, <code>Integer</code>, <code>Float</code> or <code>String</code> and follows     the HDF5 Datatype classes. If the type is not mandated by H5MD, <code>&lt;type&gt;</code> is     indicated. A scalar dataspace is indicated by <code>[]</code>.</p> <p><code>(identifier)</code> :   An optional item.</p> <p><code>&lt;identifier&gt;</code> :   An optional item with unspecified name.</p> <p>H5MD defines a structure called H5MD element (or element whenever there is no confusion). An element is either a time-dependent group or a single dataset (see time-dependent data below), depending on the situation.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#time-dependent-data","title":"Time-dependent data","text":"<p>Time-dependent data consists of a series of samples (or frames) referring to multiple time steps. Such data are found inside a single dataset and are accessed via dataset slicing. In order to link the samples to the time axis of the simulation, H5MD defines a time-dependent H5MD element as a group that contains, in addition to the actual data, information on the corresponding integer time step and on the physical time. The structure of such a group is:</p> <pre><code>&lt;element&gt;\n \\-- step\n \\-- (time)\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <p><code>value</code> :   A dataset that holds the data of the time series. It uses a simple     dataspace whose rank is given by 1 plus the tensor rank of the data stored.     Its shape is the shape of a single data item prepended by a <code>[variable]</code>     dimension that allows the accumulation of samples during the course of     time. For instance, the data shape of scalars has the form <code>[variable]</code>,     <code>D</code>-dimensional vectors use <code>[variable][D]</code>, etc. The first dimension of     <code>value</code> must match the unique dimension of <code>step</code> and <code>time</code>.</p> <p>If several H5MD elements are sampled at equal times, <code>step</code> and <code>time</code> of one element may be hard links to the <code>step</code> and <code>time</code> datasets of a different element. If two elements are sampled at different times (for instance, if one needs the positions more frequently than the velocities), <code>step</code> and <code>time</code> are unique to each of them.</p> <p>The storage of step and time information follows one of the two modes below, depending on the dataset layout of <code>step</code>.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#explicit-step-and-time-storage","title":"Explicit step and time storage","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[variable]\n \\-- (time: type[variable])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <p><code>step</code> :   A dataset with dimensions <code>[variable]</code> that contains the time steps at     which the corresponding data were sampled. It is of <code>Integer</code> type to allow     exact temporal matching of data from one H5MD element to another. The     values of the dataset are in monotonically increasing order.</p> <p><code>time</code> :   An optional dataset that is the same as the <code>step</code> dataset, except it is     <code>Float</code> or <code>Integer</code>-valued and contains the simulation time in physical units. The     values of the dataset are in monotonically increasing order.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#fixed-step-and-time-storage-currently-not-supported-in-h5md-nomad","title":"Fixed step and time storage (currently not supported in H5MD-NOMAD)","text":"<pre><code>&lt;element&gt;\n \\-- step: Integer[]\n     +-- (offset: type[])\n \\-- (time: type[])\n     +-- (offset: type[])\n \\-- value: &lt;type&gt;[variable][...]\n</code></pre> <p><code>step</code> :   A scalar dataset of <code>Integer</code> type that contains the increment of the     time step between two successive rows of data in <code>value</code>.</p> <pre><code>`offset`\n: A scalar attribute of type `Integer` corresponding to the first sampled\nvalue of `step`.\n</code></pre> <p><code>time</code> :   An optional scalar dataset that is the same as the <code>step</code> dataset, except that     it is <code>Float</code> or <code>Integer</code>-valued and contains the increment in simulation     time, in physical units.</p> <p><code>offset</code>     : A scalar attribute of the same type as <code>time</code> corresponding to the first     sampled value of <code>time</code>.</p> <p>For this storage mode, the explicit value \\(s(i)\\) of the step corresponding to the \\(i\\)-th row of the dataset <code>value</code> is \\(s(i) = i\\times\\mathrm{step} + \\mathrm{offset}\\) where \\(\\mathrm{offset}\\) is set to zero if absent. The corresponding formula for the time \\(t(i)\\) is identical: \\(t(i) = i\\times\\mathrm{time} + \\mathrm{offset}\\). The index \\(i\\) is zero-based.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#time-independent-data","title":"Time-independent data","text":"<p>H5MD defines a time-independent H5MD element as a dataset. As for the <code>value</code> dataset in the case of time-dependent data, data type and array shape are implied by the stored data, where the <code>[variable]</code> dimension is omitted.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#storage-order-of-arrays","title":"Storage order of arrays","text":"<p>All arrays are stored in C-order as enforced by the HDF5 file format. A C or C++ program may thus declare <code>r[N][D]</code> for the array of particle coordinates while the Fortran program will declare a <code>r(D,N)</code> array (appropriate index ordering for a system of <code>N</code> particles in <code>D</code> spatial dimensions), and the HDF5 file will be the same.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#storage-of-particles-and-tuples-lists","title":"Storage of particles and tuples lists","text":""},{"location":"custom_schemas/h5md/quick_H5MD_basics/#storage-of-a-list-of-particles","title":"Storage of a list of particles","text":"<p>A list of particles is an H5MD element:</p> <pre><code>&lt;list_name&gt;: Integer[N]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>list_name</code> is a dataset of <code>Integer</code> type and dimensions <code>[N]</code>, N being the number of particle indices stored in the list. <code>particles_group</code> is an attribute containing an HDF5 Object Reference as defined by the HDF5 file format. <code>particles_group</code> must refer to one of the groups in <code>/particles</code>.</p> <p>If a fill value is defined for <code>list_name</code>, the particles indices in <code>list_name</code> set to this value are ignored.</p> <p>If the corresponding <code>particles_group</code> does not possess the <code>id</code> element, the values in <code>list_name</code> correspond to the indexing of the elements in <code>particles_group</code>. Else, the values in <code>list_name</code> must be put in correspondence with the equal values in the <code>id</code> element.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#storage-of-tuples","title":"Storage of tuples","text":"<p>A list of tuples is an H5MD element:</p> <pre><code>&lt;tuples_list_name&gt;: Integer[N,T]\n +-- particles_group: Object reference\n</code></pre> <p>where <code>N</code> is the length of the list and <code>T</code> is the size of the tuples.  Both <code>N</code> and <code>T</code> may indicate variable dimensions. <code>particles_group</code> is an attribute containing an HDF5 Object Reference, obeying the same rules as for the lists of particles.</p> <p>The interpretation of the values stored within the tuples is done as for a list of particles.</p> <p>If a fill value is defined, tuples with at least one entry set to this value are ignored.</p>"},{"location":"custom_schemas/h5md/quick_H5MD_basics/#time-dependence-time-dependent-particle-lists-currently-not-supported-in-h5md-nomad","title":"Time-dependence (time-dependent particle lists currently not supported in H5MD-NOMAD)","text":"<p>As the lists of particles and tuples above are H5MD elements, they can be stored either as time-dependent groups or time-independent datasets.</p> <p>As an example, a time-dependent list of pairs is stored as:</p> <pre><code>&lt;pair_list_name&gt;\n   +-- particles_group: Object reference\n   \\-- value: Integer[variable,N,2]\n   \\-- step: Integer[variable]\n</code></pre> <p>The dimension denoted by <code>N</code> may be variable.</p>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/","title":"H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD","text":"<ul> <li>H5MD-NOMAD: A flexible data-storage schema for uploading molecular simulations to NOMAD<ul> <li>Notable changes from H5MD to H5MD-NOMAD<ul> <li>New or amended features</li> <li>Unused features</li> <li>Unsupported features</li> </ul> </li> <li>Standardized observables in H5MD-NOMAD<ul> <li>configurational</li> <li>ensemble average</li> <li>time correlation</li> </ul> </li> </ul> </li> </ul>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#notable-changes-from-h5md-to-h5md-nomad","title":"Notable changes from H5MD to H5MD-NOMAD","text":"<p>In order to effectively parse and normalize the molecular simulation data, the H5MD-NOMAD schema extends the original H5MD framework while also enforces various restrictions to the schema. This section contains a list of such additions and restrictions. Here we distinguish between \"unused\" features, i.e., metadata that will be ignored by NOMAD and \"unsupported\" features, i.e., structures that will likely cause an error if used within an H5MD-NOMAD file for upload to NOMAD.</p>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#new-or-amended-features","title":"New or amended features","text":"<ul> <li> <p>additional standardized particles group elements</p> </li> <li> <p>boundary attribute changed to boolean datatype</p> </li> <li> <p>treatment of units</p> </li> </ul>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#unused-features","title":"Unused features","text":"<ul> <li> <p>modules in h5md metadata</p> </li> <li> <p>arbitrary particle groups not parsed, group labeled <code>all</code> required</p> </li> <li> <p>image, species, and id elements of particles group</p> </li> <li> <p>non-standard elements in particles group</p> </li> </ul>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#unsupported-features","title":"Unsupported features","text":"<ul> <li> <p>fixed step and time storage</p> </li> <li> <p>time-dependent particle lists</p> </li> <li> <p>time-dependent model labels for particles</p> </li> <li> <p>only partial support for grouping of observables by particle subgroups</p> </li> <li> <p>time-dependent connectivity elements</p> </li> </ul>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#standardized-observables-in-h5md-nomad","title":"Standardized observables in H5MD-NOMAD","text":""},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#configurational","title":"configurational","text":"<ul> <li> <p><code>energy quantities</code> :</p> </li> <li> <p><code>radius_of_gyration</code> :</p> </li> </ul>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#ensemble-average","title":"ensemble average","text":"<ul> <li><code>radial_distribution_function</code> :</li> </ul>"},{"location":"custom_schemas/h5md/reference-H5MD-NOMAD/#time-correlation","title":"time correlation","text":"<ul> <li><code>mean_squared_displacement</code> :</li> </ul>"},{"location":"custom_schemas/h5md/root/","title":"The root level","text":"<p>The root level of H5MD-NOMAD structure is organized as follows (identical to the original H5MD specification):</p> <pre><code>H5MD-NOMAD root\n \\-- h5md\n \\-- (particles)\n \\-- (observables)\n \\-- (connectivity)\n \\-- (parameters)\n</code></pre> <p><code>h5md</code> :   A group that contains metadata and information on the H5MD structure     itself. It is the only mandatory group at the root level of H5MD.</p> <p><code>particles</code> :   An optional group that contains information on each particle in the system,     e.g., a snapshot of the positions or the full trajectory in phase space.</p> <p><code>observables</code> :   An optional group that contains other quantities of interest, e.g.,     physical observables that are derived from the system state at given points     in time.</p> <p><code>connectivity</code> :   An optional group that contains information about the connectivity between particles.</p> <p><code>parameters</code> :   An optional group that contains application-specific (meta)data such as     control parameters or simulation scripts.</p>"},{"location":"custom_schemas/h5md/structure/","title":"Structure","text":""},{"location":"custom_schemas/h5md/structure/#h5md-root-level","title":"H5MD root level","text":"<p>The root level of an H5MD structure holds a number of groups and is organized as follows:</p> <pre><code>H5MD root\n \\-- h5md\n \\-- (particles)\n \\-- (observables)\n \\-- (connectivity)\n \\-- (parameters)\n</code></pre> <p><code>h5md</code> :   A group that contains metadata and information on the H5MD structure     itself. It is the only mandatory group at the root level of H5MD.</p> <p><code>particles</code> :   An optional group that contains information on each particle in the system,     e.g., a snapshot of the positions or the full trajectory in phase space.     The size of the stored data scales linearly with the number of particles     under consideration.</p> <p><code>observables</code> :   An optional group that contains other quantities of interest, e.g.,     physical observables that are derived from the system state at given points     in time. The size of stored data is typically independent of the system     size.</p> <p><code>connectivity</code> :   An optional group that contains the connectivity between particles.</p> <p><code>parameters</code> :   An optional group that contains application-specific, custom data such as     control parameters or simulation scripts.</p> <p>In subsequent sections, the examples of HDF5 organization may start at the group level, omitting the display of <code>H5MD root</code>.</p>"},{"location":"custom_schemas/h5md/structure/#h5md-metadata","title":"H5MD metadata","text":"<p>A set of global metadata describing the H5MD structure is stored in the <code>h5md</code> group as attributes. The contents of the group is:</p> <pre><code>h5md\n +-- version: Integer[2]\n \\-- author\n |    +-- name: String[]\n |    +-- (email: String[])\n \\-- creator\n      +-- name: String[]\n      +-- version: String[]\n</code></pre> <p><code>version</code> :   An attribute, of <code>Integer</code> datatype and of simple dataspace of rank 1 and     size 2, that contains the major version number and the minor version number     of the H5MD specification the H5MD structure conforms to.</p> <pre><code>The version *x.y.z* of the H5MD specification follows\n[semantic versioning][semver] [@semantic_versioning]: A change of the major\nversion number *x* indicates backwards-incompatible changes to the file\nstructure. A change of the minor version number *y* indicates\nbackwards-compatible changes to the file structure. A change of the patch\nversion number *z* indicates changes that have no effect on the file\nstructure and serves to allow for clarifications or minor text editing of\nthe specification.\n\nAs the *z* component has no impact on the content of a H5MD file, the\n`version` attribute contains only *x* and *y*.\n</code></pre> <p><code>author</code> :   A group that contains metadata on the person responsible for the simulation     (or the experiment) as follows:</p> <pre><code>`name`\n:   An attribute, of fixed-length string datatype and of scalar\n    dataspace, that holds the author's real name.\n\n`email`\n:   An optional attribute, of fixed-length string datatype and\n    of scalar dataspace, that holds the author's email address of\n    the form `email@domain.tld`.\n</code></pre> <p><code>creator</code> :   A group that contains metadata on the program that created the H5MD     structure as follows:</p> <pre><code>`name`\n:   An attribute, of fixed-length string datatype and of scalar\n    dataspace, that stores the name of the program.\n\n`version`\n:   An attribute, of fixed-length string datatype and of scalar\n    dataspace, that yields the version of the program.\n</code></pre>"},{"location":"custom_schemas/h5md/structure/#modules","title":"Modules","text":"<p>The H5MD specification can be complemented by modules specific to a domain of research.  A module may define additional data elements within the H5MD structure, add conditions that the data must satisfy, or define rules for their semantic interpretation. Multiple modules may be present, as long as their prescriptions are not contradictory. Each module is identified by a name and a version number.</p> <p>The modules that apply to a specific H5MD structure are stored as subgroups within the group <code>h5md/modules</code>. Each module holds its version number as an attribute, further module-specific information may be stored:</p> <pre><code>h5md\n \\-- (modules)\n      \\-- &lt;module1&gt;\n      |    +-- version: Integer[2]\n      \\-- &lt;module2&gt;\n      |    +-- version: Integer[2]\n      \\-- ...\n</code></pre> <p><code>version</code> :   An attribute, of <code>Integer</code> datatype and of simple dataspace of rank 1 and     size 2, that contains the major version number and the minor version number     of the module.</p> <pre><code>The version *x.y.z* of an H5MD module follows [semantic versioning][semver]\n[@semantic_versioning] and again only the components *x* and *y* are\nstored, see `h5md/version` in \"[H5MD metadata].\"\n</code></pre>"},{"location":"custom_schemas/h5md/structure/#particles-group","title":"Particles group","text":"<p>Information on each particle, i.e., particle trajectories, is stored in the <code>particles</code> group. The <code>particles</code> group is a container for subgroups that represent different subsets of the system under consideration, and it may hold one or several subgroups, as needed. These subsets may overlap and their union may be incomplete, i.e., not represent all particles of the simulation volume. The subgroups contain the trajectory data for each particle as time-dependent or time-independent data, depending on the situation. Each subgroup contains a specification of the simulation box, see below. For each dataset, the particle index is accommodated by the second (first, in the case of time-independence) array dimension.</p> <p>The contents of the <code>particles</code> group assuming <code>N</code> particles in <code>D</code>-dimensional space could be the following:</p> <pre><code>particles\n \\-- &lt;group1&gt;\n      \\-- box\n      \\-- (position)\n      |    \\-- step: Integer[variable]\n      |    \\-- time: Float[variable]\n      |    \\-- value: &lt;type&gt;[variable][N][D]\n      \\-- (image)\n      |    \\-- step: Integer[variable]\n      |    \\-- time: Float[variable]\n      |    \\-- value: &lt;type&gt;[variable][N][D]\n      \\-- (species: Enumeration[N])\n      \\-- ...\n</code></pre> <p>The following identifiers for H5MD elements are standardized:</p> <p><code>position</code> :   An element that describes the particle positions as coordinate vectors of     <code>Float</code> or <code>Integer</code> type.</p> <pre><code>If the component $k$ of `box/boundary` (see [below](#simulation-box)) is set\nto `none`, the data indicate for each particle the component $k$ of its\nabsolute position in space. If the component $k$ of `box/boundary` is set to\n`periodic`, the data indicate for each particle the component $k$ of the\nabsolute position in space of an *arbitrary* periodic image of that particle.\n</code></pre> <p><code>image</code> :   An element that represents periodic images of the box as coordinate vectors     of <code>Float</code> or <code>Integer</code> type and allows one to compute for each particle its     absolute position in space. If <code>image</code> is present, <code>position</code> must be     present as well. For time-dependent data, the <code>step</code> and <code>time</code> datasets of     <code>image</code> must equal those of <code>position</code>, which must be accomplished by     hard-linking the respective datasets.</p> <pre><code>If the component $k$ of `box/boundary` (see [below](#simulation-box)) is set\nto `none`, the values of the corresponding component $k$ of `image` serve as\nplaceholders. If the component $k$ of `box/boundary` is set to `periodic`,\nfor a cuboid box, the component $k$ of the absolute position of particle $i$\nis computed as $R_{ik} = r_{ik} + L_k a_{ik}$, where $\\vec r_i$ is taken\nfrom `position`, $\\vec a_i$ is taken from `image`, and $\\vec L$ from\n`box/edges`.\n</code></pre> <p><code>velocity</code> :   An element that contains the velocities for each particle as a vector of     <code>Float</code> or <code>Integer</code> type.</p> <p><code>force</code> :   An element that contains the total forces (i.e., the accelerations     multiplied by the particle mass) for each particle as a vector of <code>Float</code>     or <code>Integer</code> type.</p> <p><code>mass</code> :   An element that holds the mass for each particle as a scalar of <code>Float</code>     type.</p> <p><code>species</code> :   An element that describes the species for each particle, i.e., its     atomic or chemical identity, as a scalar of <code>Enumeration</code> or <code>Integer</code>     data type. Particles of the same species are assumed to be identical with     respect to their properties and unbonded interactions.</p> <p><code>id</code> :   An element that holds a scalar identifier for each particle of <code>Integer</code>     type, which is unique within the given particle subgroup. The <code>id</code> serves     to identify particles over the course of the simulation in the case when     the order of the particles changes, or when new particles are inserted and     removed. If <code>id</code> is absent, the identity of the particles is given by their     index in the <code>value</code> datasets of the elements within the same subgroup.</p> <pre><code>A *fill value* (see\n[\u00a7 6.6](http://www.hdfgroup.org/HDF5/doc/UG/11_Datatypes.html#Fvalues) in\n[@HDF5_users_guide]) may be defined for `id/value` upon dataset creation.\nWhen the identifier of a particle is equal to this user-defined value,\nthe particle is considered non-existing, the entry serves as a\nplaceholder. This permits the storage of subsystems whose number of\nparticles varies in time. For the case of varying particle number, the\ndimension denoted by `[N]` above may be variable.\n</code></pre> <p><code>charge</code> :   An element that contains the charge associated to each particle as a     scalar, of <code>Integer</code> or <code>Float</code> type.</p> <pre><code>`charge` has the optional attribute `type` of fixed-length string datatype\nand of scalar dataspace, possible values are `effective` and `formal`. In\nthe case `effective`, the charge is part of an effective description of the\ninteractions with the precise meaning depending on the underlying empirical\nforce fields or coarse-grained models.\n\nIn the case `formal`, the charge is the so-called \"formal charge\" assigned\nto an atom (see &lt;http://en.wikipedia.org/wiki/Formal_charge&gt;) and must be\nof `Integer` type. This case corresponds to the entries in PDB files (see\ndefinition in the PDBx/mmCIF dictionary\n&lt;http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v40.dic/Items/_atom_site.pdbx_formal_charge.html&gt;).\n\nIf none of `effective` or `formal` describes the data properly, the\nattribute `type` may be omitted.\n</code></pre>"},{"location":"custom_schemas/h5md/structure/#simulation-box","title":"Simulation box","text":"<p>The specification of the simulation box is stored in the group <code>box</code>, which must be contained within each of the subgroups of the <code>particles</code> group. Storing the box information at several places reflects the fact that different subgroups may be sampled at different time grids. This way, the box information remains associated to a group of particles.  A specific requirement for <code>box</code> groups inside <code>particles</code> is that the <code>step</code> and <code>time</code> datasets exactly match those of the corresponding <code>position</code> groups, which must be accomplished by hard-linking the respective datasets.</p> <p>The spatial dimension and the boundary conditions of the box are stored as attributes to the <code>box</code> group, e.g., :</p> <pre><code>particles\n \\-- &lt;group1&gt;\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- (edges)\n</code></pre> <p><code>dimension</code> :   An attribute that stores the spatial dimension <code>D</code> of the simulation box     and is of <code>Integer</code> datatype and scalar dataspace.</p> <p><code>boundary</code> :   An attribute, of fixed-length string datatype and of simple dataspace of     rank 1 and size <code>D</code>, that specifies the boundary condition of the box along     each dimension. The values in <code>boundary</code> are either <code>periodic</code> or <code>none</code>:</p> <pre><code>`periodic` The simulation box is periodically continued along the given\ndimension and serves as the unit cell for an infinite tiling of space.\n\n`none` No boundary condition is imposed. This summarizes the situations of\nopen systems (i.e., an infinitely large box) and closed systems (e.g., due\nto an impenetrable wall). For those components where `boundary` is set to\n`none`, the corresponding values of `edges` serve as placeholders.\n</code></pre> <p>Information on the geometry of the box edges is stored as an H5MD element, allowing for the box to be fixed in time or not.  Supported box shapes are the cuboid and triclinic unit cell, for other shapes a transformation to the triclinic shape may be considered [@Bekker:1997]. If all values in <code>boundary</code> are <code>none</code>, <code>edges</code> may be omitted.</p> <p><code>edges</code> :   A <code>D</code>-dimensional vector or a <code>D</code> \u00d7 <code>D</code> matrix, depending on the geometry     of the box, of <code>Float</code> or <code>Integer</code> type. If <code>edges</code> is a vector, it     specifies the space diagonal of a cuboid-shaped box. If <code>edges</code> is a     matrix, the box is of triclinic shape with the edge vectors given by the     rows of the matrix.</p> <pre><code>For a time-dependent box, a cuboid geometry is encoded by a dataset `value`\n(within the H5MD element) of rank 2 (1 dimension for the time and 1 for the\nvector) and a triclinic geometry by a dataset `value` of rank 3 (1\ndimension for the time and 2 for the matrix).\n\nFor a time-independent box, a cuboid geometry is encoded by a dataset\n`edges` of rank 1 and a triclinic geometry by a dataset of rank 2.\n</code></pre> <p>For instance, a cuboid box that changes in time would appear as:</p> <pre><code>particles\n \\-- &lt;group1&gt;\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges\n                \\-- step: Integer[variable]\n                \\-- time: Float[variable]\n                \\-- value: &lt;type&gt;[variable][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>. A triclinic box that is fixed in time would appear as:</p> <pre><code>particles\n \\-- &lt;group1&gt;\n      \\-- box\n           +-- dimension: Integer[]\n           +-- boundary: String[D]\n           \\-- edges: &lt;type&gt;[D][D]\n</code></pre> <p>where <code>dimension</code> is equal to <code>D</code>.</p>"},{"location":"custom_schemas/h5md/structure/#observables-group","title":"Observables group","text":"<p>Macroscopic observables, or more generally, averages of some property over many particles, are stored in the root group <code>observables</code>. Observables representing only a subset of the particles may be stored in appropriate subgroups similarly to the <code>particles</code> tree. Each observable is stored as an H5MD element. The shape of the corresponding dataset (the element itself for time-independent data and <code>value</code> for time-dependent data) is the tensor shape of the observable, prepended by a <code>[variable]</code> dimension for time-dependent data.</p> <p>The contents of the observables group has the following structure:</p> <pre><code>observables\n \\-- &lt;observable1&gt;\n |    \\-- step: Integer[variable]\n |    \\-- time: Float[variable]\n |    \\-- value: &lt;type&gt;[variable]\n \\-- &lt;observable2&gt;\n |    \\-- step: Integer[variable]\n |    \\-- time: Float[variable]\n |    \\-- value: &lt;type&gt;[variable][D]\n \\-- &lt;group1&gt;\n |    \\-- &lt;observable3&gt;\n |         \\-- step: Integer[variable]\n |         \\-- time: Float[variable]\n |         \\-- value: &lt;type&gt;[variable][D][D]\n \\-- &lt;observable4&gt;: &lt;type&gt;[]\n \\-- ...\n</code></pre>"},{"location":"custom_schemas/h5md/structure/#connectivity-group","title":"Connectivity group","text":"<p>The connectivity information is stored as tuples in the group <code>/connectivity</code>. The tuples are pairs, triples, etc. as needed and may be either time-independent or time-dependent. Several connectivity elements may be defined for any particles group. A connectivity element may only refer to a single particle group.</p> <p>The tuples of particles are interpreted according to the section Storage of particles and tuples lists.</p>"},{"location":"custom_schemas/h5md/structure/#parameters-group","title":"Parameters group","text":"<p>The <code>parameters</code> group stores application-specific, custom data such as control parameters or simulation scripts. The group consists of groups, datasets, and attributes; the detailed structure, however, is left unspecified.</p> <p>The contents of the <code>parameters</code> group could be the following:</p> <pre><code>parameters\n +-- &lt;user_attribute1&gt;\n \\-- &lt;user_data1&gt;\n \\-- &lt;user_group1&gt;\n |    \\-- &lt;user_data2&gt;\n |    \\-- ...\n \\-- ...\n</code></pre>"},{"location":"custom_schemas/h5md/structure/#references","title":"References","text":""},{"location":"custom_schemas/h5md/units/","title":"Specifying units of datasets in H5MD-NOMAD","text":"<p>In the original H5MD schema, units were given as string attributes of datasets, e.g., <code>60 m s-2</code>. H5MD-NOMAD amends the treatment of units in 2 ways:</p> <ol> <li> <p>If needed, the leading prefactor is stored as a separate attribute of <code>float</code> datatype called <code>unit_factor</code>.</p> </li> <li> <p>The string that describes the unit should be compatible with the <code>UnitRegistry</code> class of the <code>pint</code> python module.</p> </li> </ol> <p>Generic representation of unit storage in H5MD-NOMAD:</p> <pre><code>&lt;group&gt;\n    \\-- &lt;dataset&gt;\n        +-- (unit: String[])\n        +-- (unit_factor: Float)\n</code></pre>"},{"location":"custom_schemas/h5md/examples/creating_a_topology/","title":"Example - Creating a topology","text":"<p>This page demonstrates how to create a \"standard\" topology in H5MD-NOMAD. The demonstrated organization of molecules and monomers is identical to what other NOMAD parsers do to create a topology from native simulation files (e.g., outputs from Gromacs or Lammps). However, the user is free to deviate from this standard to create arbitrary organizations of particles, as described on the Connectivity page.</p>"},{"location":"custom_schemas/h5md/examples/creating_a_topology/#standard-topology-structure-for-bonded-force-fields","title":"Standard topology structure for bonded force fields","text":"<p><pre><code>topology\n\u251c\u2500\u2500 molecule_group_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502       \u251c\u2500\u2500 monomer_1\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_1\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 monomer_2\n\u2502    \u2502      \u2502       \u2502       \u2514\u2500\u2500 metadata for monomer_2\n\u2502    \u2502      \u2502       \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 monomer_group_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 molecule_group_2\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_1\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_1\n\u2502  \u00a0 \u251c\u2500\u2500 molecule_2\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2514\u2500\u2500 metadata for molecule_2\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> Here, the first level of organization is the \"molecule group\". Molecule groups contain molecules of the same type. In other words, <code>molecule_group_1</code> and <code>molecule_group_2</code> represent distinct molecule types. At the next level of the hierarchy, each molecule within this group is stored (i.e., <code>molecule_1</code>, <code>molecule_2</code>, etc.). In the above example, <code>molecule_group_1</code> represents a polymer (or protein). Thus, below the molecule level, there is a \"monomer group level\". Similar to the molecule group, the monomer group organizes all monomers (of the parent molecule) that are of the same type. Thus, for <code>molecule_1</code> of <code>molecule_group_1</code>, <code>monomer_group_1</code> and <code>monomer_group_2</code> represent distinct types of monomers existing within the polymer. Then, below <code>monomer_group_1</code>, each monomer within this group is stored. Finally, beneath these individual monomers, only the metadata for that monomer is stored (i.e., no further organization levels). Note however, that metadata can be (and is) stored at each level of the hierarchy, but is left out of the illustration for clarity. Notice also that <code>molecule_group_2</code> is not a polymer. Thus, each molecule within this group stores only the corresponding metadata, and no further levels of organization.</p>"},{"location":"custom_schemas/h5md/examples/creating_a_topology/#creating-the-standard-hierarchy-from-an-mdanalysis-universe","title":"Creating the standard hierarchy from an MDAnalysis universe","text":"<p>We start from the perspective of the Example - H5MD file page, with identical imports and assuming that an MDAnalysis <code>universe</code> is already instantiated from the raw simulation files. As in the previous example, the <code>universe</code> containing the topology information is called <code>universe_topology</code>.</p> <p>The following functions will be useful for creating the topology:</p> <pre><code>def get_composition(children_names):\n'''\n    Given a list of children, return a compositional formula as a function of\n    these children. The format is &lt;child_1&gt;(n_child_1)&lt;child_2&gt;(n_child_2)...\n    '''\nchildren_count_tup = np.unique(children_names, return_counts=True)\nformula = ''.join([f'{name}({count})' for name, count in zip(*children_count_tup)])\nreturn formula\ndef get_molecules_from_bond_list(n_particles: int, bond_list: List[int], particle_types: List[str] = None, particles_typeid=None):\n'''\n    Returns a dictionary with molecule info from the list of bonds\n    '''\nimport networkx\nsystem_graph = networkx.empty_graph(n_particles)\nsystem_graph.add_edges_from([(i[0], i[1]) for i in bond_list])\nmolecules = [system_graph.subgraph(c).copy() for c in networkx.connected_components(system_graph)]\nmol_dict = []\nfor i_mol, mol in enumerate(molecules):\nmol_dict.append({})\nmol_dict[i_mol]['indices'] = np.array(mol.nodes())\nmol_dict[i_mol]['bonds'] = np.array(mol.edges())\nmol_dict[i_mol]['type'] = 'molecule'\nmol_dict[i_mol]['is_molecule'] = True\nif particles_typeid is None and len(particle_types) == n_particles:\nmol_dict[i_mol]['names'] = [particle_types[int(x)] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\nif particle_types is not None and particles_typeid is not None:\nmol_dict[i_mol]['names'] = [particle_types[particles_typeid[int(x)]] for x in sorted(np.array(mol_dict[i_mol]['indices']))]\nmol_dict[i_mol]['formula'] = get_composition(mol_dict[i_mol]['names'])\nreturn mol_dict\ndef is_same_molecule(mol_1: dict, mol_2: dict):\n'''\n    Checks whether the 2 input molecule dictionaries represent the same\n    molecule type, i.e., same particle types and corresponding bond connections.\n    '''\nif sorted(mol_1['names']) == sorted(mol_2['names']):\nmol_1_shift = np.min(mol_1['indices'])\nmol_2_shift = np.min(mol_2['indices'])\nmol_1_bonds_shift = mol_1['bonds'] - mol_1_shift\nmol_2_bonds_shift = mol_2['bonds'] - mol_2_shift\nbond_list_1 = [sorted((mol_1['names'][i], mol_1['names'][j])) for i, j in mol_1_bonds_shift]\nbond_list_2 = [sorted((mol_2['names'][i], mol_2['names'][j])) for i, j in mol_2_bonds_shift]\nbond_list_names_1, bond_list_counts_1 = np.unique(bond_list_1, axis=0, return_counts=True)\nbond_list_names_2, bond_list_counts_2 = np.unique(bond_list_2, axis=0, return_counts=True)\nbond_list_dict_1 = {bond[0] + '-' + bond[1]: bond_list_counts_1[i_bond] for i_bond, bond in enumerate(bond_list_names_1)}\nbond_list_dict_2 = {bond[0] + '-' + bond[1]: bond_list_counts_2[i_bond] for i_bond, bond in enumerate(bond_list_names_2)}\nif bond_list_dict_1 == bond_list_dict_2:\nreturn True\nreturn False\nreturn False\n</code></pre> <p>Then, we can create the topology structure from the MDAnalysis universe:</p> <pre><code>bond_list = universe_toponly.bonds._bix\nmolecules = get_molecules_from_bond_list(n_atoms, bond_list, particle_types=universe_toponly.atoms.types, particles_typeid=None)\n# create the topology\nmol_groups = []\nmol_groups.append({})\nmol_groups[0]['molecules'] = []\nmol_groups[0]['molecules'].append(molecules[0])\nmol_groups[0]['type'] = 'molecule_group'\nmol_groups[0]['is_molecule'] = False\nfor mol in molecules[1:]:\nflag_mol_group_exists = False\nfor i_mol_group in range(len(mol_groups)):\nif is_same_molecule(mol, mol_groups[i_mol_group]['molecules'][0]):\nmol_groups[i_mol_group]['molecules'].append(mol)\nflag_mol_group_exists = True\nbreak\nif not flag_mol_group_exists:\nmol_groups.append({})\nmol_groups[-1]['molecules'] = []\nmol_groups[-1]['molecules'].append(mol)\nmol_groups[-1]['type'] = 'molecule_group'\nmol_groups[-1]['is_molecule'] = False\nfor i_mol_group, mol_group in enumerate(mol_groups):\nmol_groups[i_mol_group]['formula'] = molecule_labels[i_mol_group] + '(' + str(len(mol_group['molecules'])) + ')'\nmol_groups[i_mol_group]['label'] = 'group_' + str(molecule_labels[i_mol_group])\nmol_group_indices = []\nfor i_molecule, molecule in enumerate(mol_group['molecules']):\nmolecule['label'] = molecule_labels[i_mol_group]\nmol_indices = molecule['indices']\nmol_group_indices.append(mol_indices)\nmol_resids = np.unique(universe_toponly.atoms.resindices[mol_indices])\nif mol_resids.shape[0] == 1:\ncontinue\nres_dict = []\nfor i_resid, resid in enumerate(mol_resids):\nres_dict.append({})\nres_dict[i_resid]['indices'] = np.where( universe_toponly.atoms.resindices[mol_indices] == resid)[0]\nres_dict[i_resid]['label'] = universe_toponly.atoms.resnames[res_dict[i_resid]['indices'][0]]\nres_dict[i_resid]['formula'] = get_composition(universe_toponly.atoms.names[res_dict[i_resid]['indices']])\nres_dict[i_resid]['is_molecule'] = False\nres_dict[i_resid]['type'] = 'monomer'\nres_groups = []\nres_groups.append({})\nres_groups[0]['residues'] = []\nres_groups[0]['residues'].append(res_dict[0])\nres_groups[0]['label'] = 'group_' + res_dict[0]['label']\nres_groups[0]['type'] = 'monomer_group'\nres_groups[0]['is_molecule'] = False\nfor res in res_dict[1:]:\nflag_res_group_exists = False\nfor i_res_group in range(len(res_groups)):\nif res['label'] == res_groups[i_res_group]['label']:\nres_groups[i_res_group]['residues'].append(res)\nflag_res_group_exists = True\nbreak\nif not flag_res_group_exists:\nres_groups.append({})\nres_groups[-1]['residues'] = []\nres_groups[-1]['residues'].append(res)\nres_groups[-1]['label'] = 'group_' + res['label']\nres_groups[-1]['formula'] = get_composition(universe_toponly.atoms.names[res['indices']])\nres_groups[-1]['type'] = 'monomer_group'\nres_groups[-1]['is_molecule'] = False\nmolecule['formula'] = ''\nfor res_group in res_groups:\nres_group['formula'] = res_group['residues'][0]['label'] + '(' + str(len(res_group['residues'])) + ')'\nmolecule['formula'] += res_group['formula']\nres_group_indices = []\nfor res in res_group['residues']:\nres_group_indices.append(res['indices'])\nres_group['indices'] = np.concatenate(res_group_indices)\nmol_group['indices'] = np.concatenate(mol_group_indices)\nmolecule['residue_groups'] = res_groups\n</code></pre>"},{"location":"custom_schemas/h5md/examples/creating_a_topology/#writing-the-topology-to-an-h5md-nomad-file","title":"Writing the topology to an H5MD-NOMAD file","text":"<p>Here we assume an H5MD-NOMAD file has already been created, as demonstrated on the Example - H5MD file page, and that the <code>connectivity</code> group was created under the root level.</p> <p>Now, create the <code>particles_group</code> group under <code>connectivity</code> within our HDF5-NOMAD file: <pre><code>topology_keys = ['type', 'formula', 'particles_group', 'label', 'is_molecule', 'indices']\ncustom_keys = ['molecules', 'residue_groups', 'residues']\ntopology = connectivity.create_group('particles_group')\nfor i_mol_group, mol_group in enumerate(mol_groups):\nhdf5_mol_group = topology.create_group('group_' + molecule_labels[i_mol_group])\nfor mol_group_key in mol_group.keys():\nif mol_group_key not in topology_keys + custom_keys:\ncontinue\nif mol_group_key != 'molecules':\nhdf5_mol_group[mol_group_key] = mol_group[mol_group_key]\nelse:\nhdf5_molecules = hdf5_mol_group.create_group('particles_group')\nfor i_molecule, molecule in enumerate(mol_group[mol_group_key]):\nhdf5_mol = hdf5_molecules.create_group('molecule_' + str(i_molecule))\nfor mol_key in molecule.keys():\nif mol_key not in topology_keys + custom_keys:\ncontinue\nif mol_key != 'residue_groups':\nhdf5_mol[mol_key] = molecule[mol_key]\nelse:\nhdf5_residue_groups = hdf5_mol.create_group('particles_group')\nfor i_res_group, res_group in enumerate(molecule[mol_key]):\nhdf5_res_group = hdf5_residue_groups.create_group('residue_group_' + str(i_res_group))\nfor res_group_key in res_group.keys():\nif res_group_key not in topology_keys + custom_keys:\ncontinue\nif res_group_key != 'residues':\nhdf5_res_group[res_group_key] = res_group[res_group_key]\nelse:\nhdf5_residues = hdf5_res_group.create_group('particles_group')\nfor i_res, res in enumerate(res_group[res_group_key]):\nhdf5_res = hdf5_residues.create_group('residue_' + str(i_res))\nfor res_key in res.keys():\nif res_key not in topology_keys:\ncontinue\nif res[res_key] is not None:\nhdf5_res[res_key] = res[res_key]\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/testing_h5md-nomad/","title":"Example - Accessing the H5MD-NOMAD file","text":"<p>The following functions are useful for accessing data from your H5MD-NOMAD file: <pre><code>def apply_unit(quantity, unit, unit_factor):\nfrom pint import UnitRegistry\nureg = UnitRegistry()\nif quantity is None:\nreturn\nif unit:\nunit = ureg(unit)\nunit *= unit_factor\nquantity *= unit\nreturn quantity\ndef decode_hdf5_bytes(dataset):\nif dataset is None:\nreturn\nelif type(dataset).__name__ == 'ndarray':\nif dataset == []:\nreturn dataset\ndataset = np.array([val.decode(\"utf-8\") for val in dataset]) if type(dataset[0]) == bytes else dataset\nelse:\ndataset = dataset.decode(\"utf-8\") if type(dataset) == bytes else dataset\nreturn dataset\ndef hdf5_attr_getter(source, path, attribute, default=None):\n'''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\nsection_segments = path.split('.')\nfor section in section_segments:\ntry:\nvalue = source.get(section)\nsource = value[-1] if isinstance(value, list) else value\nexcept Exception:\nreturn\nvalue = source.attrs.get(attribute)\nsource = value[-1] if isinstance(value, list) else value\nsource = decode_hdf5_bytes(source) if source is not None else default\nreturn source\ndef hdf5_getter(source, path, default=None):\n'''\n    Extracts attribute from object based on path, and returns default if not defined.\n    '''\nsection_segments = path.split('.')\nfor section in section_segments:\ntry:\nvalue = source.get(section)\nunit = hdf5_attr_getter(source, section, 'unit')\nunit_factor = hdf5_attr_getter(source, section, 'unit_factor', default=1.0)\nsource = value[-1] if isinstance(value, list) else value\nexcept Exception:\nreturn\nif source is None:\nsource = default\nelif type(source) == h5py.Dataset:\nsource = source[()]\nsource = apply_unit(source, unit, unit_factor)\nreturn decode_hdf5_bytes(source)\n</code></pre></p> <p>Open your H5MD-NOMAD file with <code>h5py</code>: <pre><code>import h5py\nh5_read = h5py.File('test_h5md-nomad.h5', 'r')\n</code></pre></p> <p>Access a particular data set: <pre><code>potential_energies = h5_read['observables']['energy']['potential']['value']\nprint(potential_energies[()])\n</code></pre> result: <pre><code>array([1., 1., 1., 1., 1.])\n</code></pre></p> <p>Get the unit information for this quantity: <pre><code>unit = potential_energies.attrs['unit']\nunit_factor = potential_energies.attrs['unit_factor']\nprint(unit)\nprint(unit_factor)\n</code></pre></p> <p>results: <pre><code>joule\n1000.0\n</code></pre></p> <p>Alternatively, the above functions will return the dataset as python arrays, i.e., already applying <code>[()]</code> to the HDF5 element, and also apply the appropriate units where applicable: <pre><code>potential_energies = hdf5_getter(h5_read, 'observables.energy.potential.value')\nprint(potential_energies)\n</code></pre></p> <p>result: <pre><code>Magnitude\n[1000.0 1000.0 1000.0 1000.0 1000.0]\nUnits   joule\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/","title":"Example - H5MD-NOMAD file","text":"<p>You can write to an HDF5 file via a python interface, using the h5py package. This page provides some practical examples to help you get started.</p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#imports","title":"Imports","text":"<pre><code>import numpy as np\nimport json\nimport h5py\nimport parmed as chem\nimport MDAnalysis as mda\nfrom pint import UnitRegistry\nureg = UnitRegistry()\n</code></pre> <p>h5py : module for reading and writing HDF5 files.</p> <p>UnitRegistry : object from the pint package that provides assistance for working with units. We suggest using this package for easiest compatibility with NOMAD.</p> <p>MDAnalysis : a library to analyze trajectories from molecular dynamics simulations stored in various formats.</p> <p>ParmEd : a tool for aiding in investigations of biomolecular systems using popular molecular simulation packages.</p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#example-data","title":"Example Data","text":"<p>In the following we consider a fictitious set of \"vanilla\" molecular dynamics simulations, run with the OpenMM software. The following definitions set the dimensionality, periodicity, and the units for this simulation.</p> <pre><code>dimension = 3\nperiodicity = [True, True, True]\ntime_unit = 1.0 * ureg.picosecond\nlength_unit = 1.0 * ureg.angstrom\nenergy_unit = 1000. * ureg.joule\nmass_unit = 1.0 * ureg.amu\ncharge_unit = 1.0 * ureg.e\ntemperature_unit = 1.0 * ureg.K\ncustom_unit = 1.0 * ureg.newton / length_unit**2\nacceleration_unit = 1.0 * length_unit / time_unit**2\n</code></pre> <p>In this example, we will assume that the relevant simulation data is compatible with MDAnalysis, such that a <code>universe</code> containing the trajectory and topology information can be created. Note: Knowledge of the MDAnalysis package is not necessary for understanding this example. The dimensions of the supplied quantities will be made clear in each case.</p> <p>Create a universe by supplying a <code>pdb</code> structure file and corresponding <code>dcd</code> trajectory file (MDAnalysis supports many different file formats): <pre><code>universe = mda.Universe('initial_structure.pdb', 'trajectory.dcd')\nn_frames = len(universe.trajectory)\nn_atoms = universe.trajectory[0].n_atoms\n</code></pre> Some topologies can be loaded directly into MDAnalysis. However, for simulations from OpenMM, one can read the topology using <code>parmed</code> and then import it to MDanalysis: <pre><code>pdb = app.PDBFile('initial_structure.pdb')\nforcefield = app.ForceField('force_field.xml')\nsystem = forcefield.createSystem(pdb.topology)\nstruct = chem.openmm.load_topology(pdb.topology, system)\nuniverse_toponly = mda.Universe(struct)\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#h5md-group","title":"H5MD Group","text":"<p>Create an HDF5 file called <code>test_h5md-nomad.h5</code> and create the group <code>h5md</code> under <code>root</code>: <pre><code>h5_write = h5py.File('test_h5md-nomad.h5', 'w')\nh5md = h5_write.create_group('h5md')\n</code></pre></p> <p>Add the h5md version (1.0.x in this case) as an attribute of the <code>h5md</code> group: <pre><code>h5md.attrs['version'] = [1, 0]\n</code></pre></p> <p>Create the <code>author</code> group and add the associated metadata: <pre><code>author = h5md.create_group('author')\nauthor.attrs['name'] = 'author name'\nauthor.attrs['email'] = 'author-name@example-domain.edu'\n</code></pre></p> <p>Create the <code>program</code> group and add the associated metadata: <pre><code>program = h5md.create_group('program')\nprogram.attrs['name'] = 'OpenMM'\nprogram.attrs['version'] = '7.7.0'\n</code></pre></p> <p>Create the <code>creator</code> group and add the associated metadata: <pre><code>program = h5md.create_group('creator')\nprogram.attrs['name'] = h5py.__name__\nprogram.attrs['version'] = str(h5py.__version__)\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#particles-group","title":"Particles Group","text":"<p>Create the <code>particles</code> group and the underlying <code>all</code> group to hold the relevant particle data: <pre><code>particles = h5_write.create_group('particles')\nparticles_group_all = particles.create_group('all')\n</code></pre></p> <p>Get the steps, times, positions, and lattice vectors (i.e., box dimensions) from the MDA universe: <pre><code># quantities extracted from MDAnalysis\nsteps = []\ntimes = []\npositions = []\nlattice_vectors = []\nfor i_frame, frame in enumerate(universe.trajectory):\ntimes.append(frame.time)\nsteps.append(frame.frame)\npositions.append(frame.positions)\nlattice_vectors.append(frame.triclinic_dimensions)\n</code></pre></p> <p>Set the positions and corresponding metadata: <pre><code>position_group_all = particles_group_all.create_group('position')\nposition_group_all['step'] = steps  # shape = (n_frames)\nposition_group_all['time'] = times  # shape = (n_frames)\nposition_group_all['time'].attrs['unit'] = str(time_unit.units)\nposition_group_all['time'].attrs['unit_factor'] = time_unit.magnitude\nposition_group_all['value'] = positions  # shape = (n_frames, n_atoms, dimension)\nposition_group_all['value'].attrs['unit'] = str(length_unit.units)\nposition_group_all['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p> <p>Set the particle-specific metadata: <pre><code>particles_group_all['species_label'] = universe_toponly.atoms.types  # shape = (n_atoms)\nparticles_group_all['force_field_label'] = universe_toponly.atoms.names  # shape = (n_atoms)\nparticles_group_all['mass'] = universe_toponly.atoms.masses  # shape = (n_atoms)\nparticles_group_all['mass'].attrs['unit'] = str(mass_unit.units)\nparticles_group_all['mass'].attrs['unit_factor'] = mass_unit.magnitude\nparticles_group_all['charge'] = universe_toponly.atoms.charges  # shape = (n_atoms)\nparticles_group_all['charge'].attrs['unit'] = str(charge_unit.units)\nparticles_group_all['charge'].attrs['unit_factor'] = charge_unit.magnitude\n</code></pre></p> <p>Create the <code>box</code> group under <code>particles.all</code> and write corresponding data: <pre><code>box_group = particles_group_all.create_group('box')\nbox_group.attrs['dimension'] = dimension\nbox_group.attrs['boundary'] = periodicity\nedges = box_group.create_group('edges')\nedges['step'] = steps\nedges['time'] = times\nedges['time'].attrs['unit'] = str(time_unit.units)\nedges['time'].attrs['unit_factor'] = time_unit.magnitude\nedges['value'] = lattice_vectors\nedges['value'].attrs['unit'] = str(length_unit.units)\nedges['value'].attrs['unit_factor'] = length_unit.magnitude\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#connectivity-group","title":"Connectivity Group","text":"<p>Create the <code>connectivity</code> group under <code>root</code> and add the tuples of bonds, angles, and dihedrals: <pre><code>connectivity = h5_write.create_group('connectivity')\nconnectivity['bonds'] = universe_toponly.bonds._bix  # shape = (n_bonds, 2)\nconnectivity['angles'] = universe_toponly.angles._bix  # shape = (n_angles, 3)\nconnectivity['dihedrals'] = universe_toponly.dihedrals._bix  # shape = (n_dihedrals, 4)\nconnectivity['impropers'] = universe_toponly.impropers._bix  # shape = (n_impropers, 4)\n</code></pre> Here <code>n_bonds</code>, <code>n_angles</code>, <code>n_dihedrals</code>, and <code>n_impropers</code> represent the corresponding number of instances of each interaction within the force field.</p> <p>The creation of the <code>particles_group</code> group (i.e., topology) is discussed HERE</p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#observables-group","title":"Observables Group","text":"<p>For this section, we will consider sets of fabricated observable data for clarity. First, create the <code>observables</code> group under root: <pre><code>observables = h5_write.create_group('observables')\n</code></pre></p> <p>There are 3 types of support observables: <pre><code>types = ['configurational', 'ensemble_average', 'correlation_function']\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#configurational-observables","title":"Configurational Observables","text":"<p>Fabricated data: <pre><code>temperatures = 300. * np.ones(n_frames)\npotential_energies = 1.0 * np.ones(n_frames)\nkinetic_energies = 2.0 * np.ones(n_frames)\n</code></pre></p> <p>Create a <code>temperature</code> group and populate the associated metadata:</p> <pre><code>temperature = observables.create_group('temperature')\ntemperature.attrs['type'] = types[0]\ntemperature['step'] = steps\ntemperature['time'] = times\ntemperature['time'].attrs['unit'] = str(time_unit.units)\ntemperature['time'].attrs['unit_factor'] = time_unit.magnitude\ntemperature['value'] = temperatures\ntemperature['value'].attrs['unit'] = str(temperature_unit.units)\ntemperature['value'].attrs['unit_factor'] = temperature_unit.magnitude\n</code></pre> <p>Create an <code>energy</code> group to hold various types of energies. Add : <pre><code>energies = observables.create_group('energy')\npotential_energy = energies.create_group('potential')\npotential_energy.attrs['type'] = types[0]\npotential_energy['step'] = steps\npotential_energy['time'] = times\npotential_energy['time'].attrs['unit'] = str(time_unit.units)\npotential_energy['time'].attrs['unit_factor'] = time_unit.magnitude\npotential_energy['value'] = potential_energies\npotential_energy['value'].attrs['unit'] = str(energy_unit.units)\npotential_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\nkinetic_energy = energies.create_group('kinetic')\nkinetic_energy.attrs['type'] = types[0]\nkinetic_energy['step'] = steps\nkinetic_energy['time'] = times\nkinetic_energy['time'].attrs['unit'] = str(time_unit.units)\nkinetic_energy['time'].attrs['unit_factor'] = time_unit.magnitude\nkinetic_energy['value'] = kinetic_energies\nkinetic_energy['value'].attrs['unit'] = str(energy_unit.units)\nkinetic_energy['value'].attrs['unit_factor'] = energy_unit.magnitude\n</code></pre></p>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#ensemble-average-observables","title":"Ensemble Average Observables","text":"<p>Fabricated data - the following represents radial distribution function (rdf) data calculated between molecule types <code>X</code> and <code>Y</code>, stored in <code>rdf_MOLX-MOLY.xvg</code>: <pre><code>      0.24 0.000152428\n     0.245 0.00457094\n      0.25  0.0573499\n     0.255   0.284764\n      0.26   0.842825\n     0.265    1.64705\n      0.27    2.37243\n     0.275    2.77916\n      0.28    2.80622\n     0.285    2.60082\n      0.29    2.27182\n      ...\n</code></pre></p> <p>Store the rdf data in a dictionary along with some relevant metadata:</p> <pre><code>rdf_XX = np.loadtxt('rdf_MOLX-MOLX.xvg')\nrdf_XY = np.loadtxt('rdf_MOLX-MOLY.xvg')\nrdf_YY = np.loadtxt('rdf_MOLY-MOLY.xvg')\nrdfs = {\n'MOLX-MOLX': {\n'n_bins': len(rdf_XX[:, 0]),\n'bins': rdf_XX[:, 0],\n'value': rdf_XX[:, 1],\n'type': 'molecular',\n'frame_start': 0,\n'frame_end': n_frames-1\n},\n'MOLX-MOLY': {\n'n_bins': len(rdf_XY[:, 0]),\n'bins': rdf_XY[:, 0],\n'value': rdf_XY[:, 1],\n'type': 'molecular',\n'frame_start': 0,\n'frame_end': n_frames-1\n},\n'MOLY-MOLY': {\n'n_bins': len(rdf_YY[:, 0]),\n'bins': rdf_YY[:, 0],\n'value': rdf_YY[:, 1],\n'type': 'molecular',\n'frame_start': 0,\n'frame_end': n_frames-1\n}\n}\n</code></pre> <p>Now create the <code>radial_distribution_functions</code> group under <code>observables</code> and store each imported rdf: <pre><code>radial_distribution_functions = observables.create_group('radial_distribution_functions')\nfor key in rdfs.keys():\nrdf = radial_distribution_functions.create_group(key)\nrdf.attrs['type'] = types[1]\nrdf['type'] = rdfs[key]['type']\nrdf['n_bins'] = rdfs[key]['n_bins']\nrdf['bins'] = rdfs[key]['bins']\nrdf['bins'].attrs['unit'] = str(length_unit.units)\nrdf['bins'].attrs['unit_factor'] = length_unit.magnitude\nrdf['value'] = rdfs[key]['value']\nrdf['frame_start'] = rdfs[key]['frame_start']\nrdf['frame_end'] = rdfs[key]['frame_end']\n</code></pre></p> <p>We can also store scalar ensemble average observables. Let's consider some fabricated diffusion constant data: <pre><code>Ds = {\n'MOLX': {\n'value': 1.0,\n'error_type': 'Pearson_correlation_coefficient',\n'errors': 0.98\n},\n'MOLY': {\n'value': 2.0,\n'error_type': 'Pearson_correlation_coefficient',\n'errors': 0.95\n}\n}\n</code></pre></p> <p>Create the <code>diffusion constants</code> group under <code>observables</code> and store the correspond (meta)data:</p> <pre><code>diffusion_constants = observables.create_group('diffusion_constants')\nfor key in Ds.keys():\ndiffusion_constant = diffusion_constants.create_group(key)\ndiffusion_constant.attrs['type'] = types[1]\ndiffusion_constant['value'] = Ds[key]['value']\ndiffusion_constant['value'].attrs['unit'] = str(diff_unit.units)\ndiffusion_constant['value'].attrs['unit_factor'] = diff_unit.magnitude\ndiffusion_constant['error_type'] = Ds[key]['error_type']\ndiffusion_constant['errors'] = Ds[key]['errors']\n</code></pre>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#time-correlation-observables","title":"Time Correlation Observables","text":"<p>Fabricated data - the following represents mean squared displacement (msd) data calculated for molecule type <code>X</code>, stored in <code>msd_MOLX.xvg</code>: <pre><code>         0           0\n         2   0.0688769\n         4    0.135904\n         6    0.203573\n         8    0.271162\n        10    0.339284\n        12    0.410115\n        14    0.477376\n        16    0.545184\n        18     0.61283\n        ...\n</code></pre></p> <p>Store the msd data in a dictionary along with some relevant metadata:</p> <pre><code>msd_X = np.loadtxt('msd_MOLX.xvg')\nmsd_Y = np.loadtxt('msd_MOLY.xvg')\nmsds = {\n'MOLX': {\n'n_times': len(msd_X[:, 0]),\n'times': msd_X[:, 0],\n'value': msd_X[:, 1],\n'type': 'molecular',\n'direction': 'xyz',\n'error_type': 'standard_deviation',\n'errors': np.zeros(len(msd_X[:, 0])),\n},\n'MOLY': {\n'n_times': len(msd_Y[:, 0]),\n'times': msd_Y[:, 0],\n'value': msd_Y[:, 1],\n'type': 'molecular',\n'direction': 'xyz',\n'error_type': 'standard_deviation',\n'errors': np.zeros(len(msd_Y[:, 0])),\n}\n}\n</code></pre> <p>Now create the <code>mean_squared_displacements</code> group under <code>observables</code> and store each imported rdf:</p> <pre><code>mean_squared_displacements = observables.create_group('mean_squared_displacements')\nmsd_unit = length_unit * length_unit\ndiff_unit = msd_unit / time_unit\nfor key in msds.keys():\nmsd = mean_squared_displacements.create_group(key)\nmsd.attrs['type'] = types[2]\nmsd['type'] = msds[key]['type']\nmsd['direction'] = msds[key]['direction']\nmsd['error_type'] = msds[key]['error_type']\nmsd['n_times'] = msds[key]['n_times']\nmsd['times'] = msds[key]['times']\nmsd['times'].attrs['unit'] = str(time_unit.units)\nmsd['times'].attrs['unit_factor'] = time_unit.magnitude\nmsd['value'] = msds[key]['value']\nmsd['value'].attrs['unit'] = str(msd_unit.units)\nmsd['value'].attrs['unit_factor'] = msd_unit.magnitude\nmsd['errors'] = msds[key]['errors']\n</code></pre>"},{"location":"custom_schemas/h5md/examples/writing_h5md-nomad/#parameter-group","title":"Parameter Group","text":"<p>Using the json templates for force calculations and molecular dynamics workflows, the (meta)data can be written to the H5MD-NOMAD file using the following code:</p> <p>First, import the templates: <pre><code>with open('force_calculations_metainfo.json') as json_file:\nforce_calculation_parameters = json.load(json_file)\nwith open('workflow_metainfo.json') as json_file:\nworkflow_parameters = json.load(json_file)\n</code></pre></p> <p>Then, create the appropriate container groups: <pre><code>parameters = h5_write.create_group('parameters')\nforce_calculations = parameters.create_group('force_calculations')\nworkflow = parameters.create_group('workflow')\n</code></pre></p> <p>Now, recursively write the (meta)data: <pre><code>def get_parameters_recursive(parameter_group, parameter_dict):\n# Store the parameters from parameter dict into an hdf5 file\nfor key, val in parameter_dict.items():\nif type(val) == dict:\nparam = val.get('value')\nif param is not None:\nparameter_group[key] = param\nunit = val.get('unit')\nif unit is not None:\nparameter_group[key].attrs['unit'] = unit\nelse:  # This is a subsection\nsubsection = parameter_group.create_group(key)\nsubsection = get_parameters_recursive(subsection, val)\nelse:\nif val is not None:\nparameter_group[key] = val\nreturn parameter_group\nforce_calculations = get_parameters_recursive(force_calculations, force_calculation_parameters)\nworkflow = get_parameters_recursive(workflow, workflow_parameters)\n</code></pre></p> <p>It's as simple as that! Now, we can upload our H5MD-NOMAD file directly to NOMAD and all the written (meta)data will be stored according to the standard NOMAD schema.</p>"},{"location":"custom_schemas/h5md/references/standard_observables/","title":"Standardized observables in H5MD-NOMAD","text":""},{"location":"custom_schemas/h5md/references/standard_observables/#configurational","title":"configurational","text":"<ul> <li> <p><code>energy quantities</code> :</p> </li> <li> <p><code>radius_of_gyration</code> :</p> </li> </ul>"},{"location":"custom_schemas/h5md/references/standard_observables/#ensemble-average","title":"ensemble average","text":"<ul> <li><code>radial_distribution_function</code> :</li> </ul>"},{"location":"custom_schemas/h5md/references/standard_observables/#time-correlation","title":"time correlation","text":"<ul> <li><code>mean_squared_displacement</code> :</li> </ul>"},{"location":"custom_schemas/h5md/references/unsupported/","title":"Notable changes from H5MD to H5MD-NOMAD","text":"<p>In order to effectively parse and normalize the molecular simulation data, the H5MD-NOMAD schema extends the original H5MD framework while also enforces various restrictions to the schema. This page contains a list of such additions and restrictions. Here we distinguish between \"unused\" features, i.e., metadata that will be ignored by NOMAD and \"unsupported\" features, i.e., structures that will likely cause an error if used within an H5MD-NOMAD file for upload to NOMAD.</p>"},{"location":"custom_schemas/h5md/references/unsupported/#new-or-amended-features","title":"New or amended features","text":"<ul> <li> <p>additional standardized particles group elements</p> </li> <li> <p>boundary attribute changed to boolean datatype</p> </li> <li> <p>treatment of units</p> </li> </ul>"},{"location":"custom_schemas/h5md/references/unsupported/#unused-features","title":"Unused features","text":"<ul> <li> <p>modules in h5md metadata</p> </li> <li> <p>arbitrary particle groups not parsed, group labeled <code>all</code> required</p> </li> <li> <p>image, species, and id elements of particles group</p> </li> <li> <p>non-standard elements in particles group</p> </li> </ul>"},{"location":"custom_schemas/h5md/references/unsupported/#unsupported-features","title":"Unsupported features","text":"<ul> <li> <p>fixed step and time storage</p> </li> <li> <p>time-dependent particle lists</p> </li> <li> <p>time-dependent model labels for particles</p> </li> <li> <p>only partial support for grouping of observables by particle subgroups</p> </li> <li> <p>time-dependent connectivity elements</p> </li> </ul>"},{"location":"customizing_the_filters/overview/","title":"Overview","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"filtering_and_querying/overview/","title":"Overview","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"glossary/glossary/","title":"Glossary of terms","text":"<p>Warning</p> <p>Under construction.</p> <p>This is a list of terms that have a well-defined and specific meaning, and which are repeatedly used throughout the documentation.</p>"},{"location":"glossary/glossary/#gui","title":"NOMAD Graphical User Interface (GUI)","text":""},{"location":"glossary/glossary/#api","title":"NOMAD Application Programming Interface (API)","text":""},{"location":"glossary/glossary/#entries","title":"Entries","text":""},{"location":"glossary/glossary/#metainfo-sections","title":"Sections","text":""},{"location":"glossary/glossary/#metainfo-quantities","title":"Quantities","text":""},{"location":"normalizing_metadata/overview/","title":"Overview","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/","title":"Explanation - how the processing works?","text":"<p>In the previous section, How-to upload data, we showed that when the data was uploaded in the Uploads page, a processing of the raw data was triggered. </p> <p>Once the data is added to the upload, NOMAD interprets the files and determines which of them is a mainfile. Any other files in the upload can be viewed as auxiliary files. In the same upload, there might be multiple mainfiles and auxiliary files organized in a folder tree structure.</p> <p>The mainfiles are the main output file of a calculation. The presence of a mainfile in the upload is key for NOMAD to recognize a calculation. In NOMAD, we support several computational codes for first principles calculations, molecular dynamics simulations, and lattice modeling, as well as workflow and database managers. For each code, NOMAD recognizes a single file as the mainfile. For example, the VASP mainfile is by default the <code>vasprun.xml</code>, although if the <code>vasprun.xml</code> is not present in the upload NOMAD searches the <code>OUTCAR</code> file and assigns it as the mainfile<sup>1</sup>. </p> <p>The rest of files which are not the mainfile are auxiliary files. These can have several purposes and be supported and recognized by NOMAD in the parser. For example, the <code>band*.out</code> or <code>GW_band*</code> files in FHI-aims are auxiliary files that allows the NOMAD FHI-aims parser to recognize band structures in DFT and GW, respectively.</p> <p>You can see the full list of supported codes, mainfiles, and auxiliary files in the general NOMAD documentation under Supported parsers.</p> <p>We recommend that the user keeps the folder structure and files generated by the simulation code, but without reaching the uploads limits. Please, also check our recommendations on Best Practices: preparing the data and folder structure.</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#structured-data-with-the-nomad-metainfo","title":"Structured data with the NOMAD metainfo","text":"<p>Once the mainfile has been recognized, a new entry in NOMAD is created and a specific parser is called. The auxliary files are searched by and accessed within the parser. You can check more details in Writing a parser plugin on how to add new parsers in order for NOMAD to support new codes.</p> <p>For this new entry, NOMAD generates a NOMAD archive. It will contain all the (meta)information extracted from the unstructured raw data files but in a structured, well defined, and machine readable format. This metadata provides context to the raw data, i.e., what were the input methodological parameters, on which material the calculation was performed, etc. We define the NOMAD Metainfo as all the set of sections, sub-sections, and quantities used to structure the raw data into a structured schema. Further information about the NOMAD Metainfo is available in the general NOMAD documentation page in Learn &gt; Structured data.</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#nomad-sections-for-computational-data","title":"NOMAD sections for computational data","text":"<p>Under the <code>Entry</code> / <code>archive</code> section, there are several sections and quantities being populated by the parsers. For computational data, only the following sections are populated:</p> <ul> <li><code>metadata</code>: contains general and non-code specific metadata. This is mainly information about authors, creation of the entry time, identifiers (id), etc.</li> <li><code>run</code>: contains the parsed and normalized raw data into the structured NOMAD schema. This is all the possible raw data which can be translated into a structured way.</li> <li><code>workflow2</code>: contains metadata about the specific workflow performed within the entry. This is mainly a set of well-defined workflows, e.g., <code>GeometryOptimization</code>, and their parameters.</li> <li><code>results</code>: contains the normalized and search indexed metadata. This is mainly relevant for searching, filtering, and visualizing data in NOMAD.</li> </ul> <code>workflow</code> and <code>workflow2</code> sections: development and refactoring <p>You have probably noticed the name <code>workflow2</code> but also the existence of a section called <code>workflow</code> under <code>archive</code>. This is because <code>workflow</code> is an old version of the workflow section, while <code>workflow2</code> is the new version. Sometimes, certain sections suffer a rebranding or refactoring, in most cases to add new features or to polish them after we receive years of feedback. In this case, the <code>workflow</code> section will remain until all older entries containing such section are reprocessed to transfer this information into <code>workflow2</code>. </p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#parsing","title":"Parsing","text":"<p>A parser is a Python module which reads the code-specific mainfile and auxiliary files and populates the <code>run</code> and <code>workflow2</code> sections of the <code>archive</code>, along with all relevant sub-sections and quantities. We explain them more in detail in Writing a parser plugin. </p> <p>Parsers are added to NOMAD as plugins and are divided in a set of Github sub-projects under the main NOMAD repository. You can find a detailed list of projects in Writing a parser plugin - Parser organization.</p> <p>External contributions</p> <p>We always welcome external contributions for new codes and parsers in our repositories. Furthermore, we are always happy to hear feedback and implement new features into our parsers. Please, check our Contact information to get in touch with us so we can promptly help you!</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#normalizing","title":"Normalizing","text":"<p>After the parsing populates the <code>run</code> and <code>workflow2</code> sections, an extra layer of Python modules is executed on top of the processed NOMAD metadata. This has two main purposes: 1. normalize or homogenize certain metadata parsed from different codes, and 2. populate the <code>results</code> section. For example, this is the case of normalizing the density of states (DOS) to its size intensive value, independently of the code used to calculate the DOS. The set of normalizers relevant for computational data are listed in <code>/nomad/config/models.py</code> and are executed in the specific order defined there. Their roles are explained more in detail in Normalizing metadata.</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#search-indexing","title":"Search indexing (and storing)","text":"<p>The last step is to store the structured metadata and pass some of it to the search index. The metadata which is passed to the search index is defined in the <code>results</code> section. These metadata can then be searched by filtering in the Entries page of NOMAD or by writing a Python script which searches using the NOMAD API, see Filtering and Querying.</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#entries-overview-page","title":"Entries OVERVIEW page","text":"<p>Once the parsers and normalizers finish, the Uploads page will show if the processing of the entry was a <code>SUCCESS</code> or a <code>FAILURE</code>. The entry information can be browsed by clicking on the  icon. </p> <p>You will land on the <code>OVERVIEW</code> page of the entry. On the top menu you can further select the <code>FILES</code> page, the <code>DATA</code> page, and the <code>LOGS</code> page.</p> <p>The overview page contains a summary of the parsed metadata, e.g., tabular information about the material and methodology of the calculation (in the example, a G0W0 calculation done with the code exciting for bulk Si<sub>2</sub>), and visualizations of the system and some relevant properties. We note that all metadata are read directly from <code>results</code>.</p>"},{"location":"uploading_and_publishing/explanation_how_the_processing_works/#logs-page","title":"LOGS page","text":"<p>In the <code>LOGS</code> page, you can find information about the processing. You can read error, warning, and critical messages which can provide insight if the processing of an entry was a <code>FAILURE</code>.</p>      We recommend you to contact us in case you find `FAILURE` situations. These might be due to bugs which we are rapidly fixing, and whose origin might be varied: from a new version     of a code which is not yet supported to wrong handling of potential errors in the parser script. It may also be a problem with the organization of the data in the folders. In order to     minimize these situations, we suggest you reading [References &gt; Best Practices: preparing the data and folder structure](refs.md/#best-practices-preparing-folder-upload).      Please, check our [Contact](../contact.md) information to get in touch with us so we can promptly help you!   ### DATA page  The `DATA` page contains all the structured NOMAD metainfo populated by the parser and normalizers. This is the most important page in the entry, as it contains all the relevant metadata which will allow users to find that specific simulation.     Furthermore, you can click on the :fontawesome-solid-cloud-arrow-down: icon to download the NOMAD `archive` in a JSON format. We explain more in detail how to work with such files in [Filtering and Querying](../filtering_and_querying/overview.md).     <ol> <li> <p>Please, check our note References &gt; VASP POTCAR stripping.\u00a0\u21a9</p> </li> </ol>"},{"location":"uploading_and_publishing/howto_publish_data/","title":"How-to publish data","text":"<p>After uploading and a successful parsing, congratulations! Now you can publish your data and let other users browse through it and re-use it for other purposes.</p> <p>You can define a specific <code>Embargo period</code> of up to 36 months, after which the data will be made publicly available under the CC BY 4.0 license.</p> <p>After publishing by clicking on <code>PUBLISH</code>, the uploaded files cannot be altered. However, you can still edit the metadata fields.</p>"},{"location":"uploading_and_publishing/howto_publish_data/#organize-data-in-datasets","title":"Organizing data in datasets","text":"<p>You can organize your uploads and individual entries by grouping them into common datasets. </p> <p>In the uploads page, click on <code>EDIT AUTHOR METADATA OF ALL ENTRIES</code>. </p> <p>Under <code>Datasets</code> you can either <code>Create a new dataset</code> or <code>Search for an existing dataset</code>. After selecting the dataset, click on <code>SUBMIT</code>.</p> <p>Now, the defined dataset will be defined under <code>PUBLISH &gt; Datasets</code>.</p> <p>The icon  allows you to assign a DOI to a specific dataset. Once a DOI has been assign to a dataset, no more data can be added to it. This can then be added into your publication so that it can be used as a reference, e.g., see the Data availability statement in M. Kuban et al., Similarity of materials and data-quality assessment by fingerprinting, MRS Bulletin 47, 991-999 (2022).</p>"},{"location":"uploading_and_publishing/howto_upload_data/","title":"How-to upload data","text":"<p>Uploading data in NOMAD can be done in several ways: </p> <ul> <li>By dragging-and-dropping your files into the <code>PUBLISH &gt; Uploads</code> page: suitable for users who have a relatively small amount of data or who want to test how the processing works.</li> <li>By using the Python-based NOMAD API: suitable for users who have larger datasets and need to automatize the upload.</li> <li>By using the shell command <code>curl</code> for sending files to the upload: suitable for users who have larger datasets and need to automatize the upload.</li> </ul> <p>You can upload the files one by one or you can zip them in <code>.zip</code> or <code>.tar.gz</code> formats to upload a larger amount of files at once. </p> <p>We suggest you to visit and read the References &gt; Best Practices: preparing the data and folder structure page to see what are the best practices to organize data in a directory tree prior to upload it.</p>"},{"location":"uploading_and_publishing/howto_upload_data/#drag-and-drop-uploads","title":"Drag-and-drop uploads","text":"<p>On the top-left menu, click on <code>PUBLISH &gt; Uploads</code>.</p> <p>You can then click on <code>CREATE A NEW UPLOAD</code> or try one of the example uploads by clicking in <code>ADD EXAMPLE UPLOADS</code> and selecting one of the multiple options. In our case, we use a zip file with some computational data.</p> <p>You can drag-and-drop your files or click on the <code>CLICK OR DROP FILES</code> button to browse through your local directories.</p> <p>After the files are uploaded, a processing is triggered. Visit Explanation - how the processing works to gain further insight into the process.</p> <p>You will receive an email when the upload processing is finished.</p>"},{"location":"uploading_and_publishing/howto_upload_data/#nomad-api-uploads","title":"NOMAD API uploads","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"uploading_and_publishing/howto_upload_data/#command-line-uploads","title":"Command-line uploads","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"uploading_and_publishing/howto_upload_data/#sections-of-the-uploads-page","title":"Sections of the Uploads page","text":"<p>At the top of the uploads page, you can modify certain general metadata fields.</p> <p>The name of the upload can be modify by clicking on the pen icon . The other icons correspond to:</p> <ul> <li> Manage members: allows users to invite collaborators by defining co-authors and reviewers roles.</li> <li> Download files: downloads all files present in the upload.</li> <li> Reload: reloads the uploads page.</li> <li> Reprocess: triggers again the processing of the uploaded data.</li> <li> API: generates a JSON response to use by the NOMAD API. See Filtering and Querying for more information.</li> <li> Delete the upload: deletes completely the upload.</li> </ul> <p>The remainder of the uploads page is divided in 4 sections. </p>"},{"location":"uploading_and_publishing/howto_upload_data/#prepare-and-upload-your-files","title":"Prepare and upload your files","text":"<p>This section shows the files and folder structure in the upload. You can add a <code>README.md</code> in the root directory and its content will be shown above this section..</p>"},{"location":"uploading_and_publishing/howto_upload_data/#process-data","title":"Process data","text":"<p>This section shows the processed data and the generated entries in NOMAD.</p>"},{"location":"uploading_and_publishing/howto_upload_data/#edit-author-metadata","title":"Edit author metadata","text":"<p>This section allows users to edit certain metadata fields from all entries recognized in the upload. This includes comments, where you can add as much extra information as you want, references, where you can add a URL to your upload (e.g., an article DOI), and datasets, where you can create or add the uploaded data into a more general dataset (see How-to publish data &gt; Organizing data in datasets).</p> <p> </p>"},{"location":"uploading_and_publishing/howto_upload_data/#publish","title":"Publish","text":"<p>This section lets the user to publish the data with or without an embargo. This will be explained more in detail in How-to publish data. </p>"},{"location":"uploading_and_publishing/overview/","title":"Overview","text":"<p>One of the main pillars of NOMAD is to enable scientists and researchers to manage, share, and publish data in an easy and efficient way. In this section, we will show you how you can use the central NOMAD Archive (or, equivalently, your local installations or OASIS) for this purpose. Alternatively, you can check the Tutorial and How-to upload/publish data in the general NOMAD documentation.</p>"},{"location":"uploading_and_publishing/overview/#creating-a-nomad-account","title":"Creating a NOMAD account","text":"<p>Before being able to upload and publish data in NOMAD, you need to create your personal account.</p> <p>Go to the NOMAD website and click on the button <code>Open NOMAD</code>. This will take you to the NOMAD GUI. The purpose of this site is to allow users to search, access, and download data using an intuitive and appealing interface.</p> <p>On the top right, click on <code>LOGIN / REGISTER</code>. </p> <p>You can then create an account by clicking on <code>New user? Register</code>.</p> <p> </p> <p>After filling the blanks and clicking on <code>REGISTER</code>, you will receive a verification email. Once you verify your personal account, you can start using NOMAD.</p> <p>Note</p> <p>In practice, you can create as many accounts as you want. However, we recommend you to create a single one for managing your data in the platform. Otherwise, this can interfere with other functionalities, e.g., when a collaborator wants to add you as a member of an upload but instead finds a list of possible accounts.</p>"},{"location":"uploading_and_publishing/refs/","title":"References","text":""},{"location":"uploading_and_publishing/refs/#best-practices-preparing-folder-upload","title":"Best Practices: preparing the data and folder structure","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"uploading_and_publishing/refs/#uploads-limits","title":"Uploads limits","text":"<p>NOMAD limits the number of uploads and size of all its users. The following rules apply:</p> <ol> <li>One upload cannot exceed 32 GB in size.</li> <li>A user can only be co-author of up to 10 non-published uploads at the same time.</li> <li>Only uploads with at least one recognized entry can be published.</li> </ol>"},{"location":"uploading_and_publishing/refs/#vasp-potcar-stripping","title":"VASP POTCAR stripping","text":"<p>For VASP data, NOMAD complies with the licensing of the <code>POTCAR</code> files. In agreement with Georg Kresse, NOMAD extracts the most important information of the <code>POTCAR</code> file and stores them in a stripped version called <code>POTCAR.stripped</code>. The <code>POTCAR</code> files are then automatically removed from the upload, so that you can safely publish your data. </p>"},{"location":"visualizing_properties/overview/","title":"Overview","text":"<p>Warning</p> <p>Under construction.</p>"},{"location":"writing_a_parser_plugin/computational/","title":"Parser structure for computation","text":""},{"location":"writing_a_parser_plugin/computational/#overview-of-metadata-organization-for-computation","title":"Overview of metadata organization for computation","text":"<p>NOMAD stores all processed data in a well defined, structured, and machine readable format, known as the <code>archive</code>. The schema that defines the organization of (meta)data within the archive is known as the <code>MetaInfo</code>. More information can be found in the NOMAD docs: An Introduction to Schemas and Structured Data in NOMAD. The following diagram is an overarching visualization of the most important archive sections for computational data:</p> <pre><code>archive\n\u251c\u2500\u2500 run\n\u2502  \u00a0 \u251c\u2500\u2500 method\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atom_parameters\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 dft\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 forcefield\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u251c\u2500\u2500 system\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u251c\u2500\u2500 atoms\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 positions\n\u2502  \u00a0 \u2502\u00a0\u00a0    \u2502     \u251c\u2500\u2500 lattice_vectors\n\u2502  \u00a0 \u2502      \u2502     \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2502      \u2514\u2500\u2500 ...\n\u2502  \u00a0 \u2514\u2500\u2500 calculation\n\u2502  \u00a0        \u251c\u2500\u2500 energy\n\u2502  \u00a0        \u251c\u2500\u2500 forces\n\u2502  \u00a0        \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 workflow\n \u00a0\u00a0  \u251c\u2500\u2500 method\n \u00a0\u00a0  \u251c\u2500\u2500 inputs\n \u00a0\u00a0  \u251c\u2500\u2500 tasks\n \u00a0\u00a0  \u251c\u2500\u2500 outputs\n \u00a0\u00a0  \u2514\u2500\u2500 results\n</code></pre> <p>The most important section of the archive for computational data is the <code>run</code> section, which is divided into three main subsections: <code>method</code>, <code>system</code>, and <code>calculation</code>. <code>method</code> stores information about the computational model used to perform the calculation.</p> <p><code>system</code> stores attributes of the atoms involved in the calculation, e.g., atom types, positions, lattice vectors, etc. <code>calculation</code> stores the output of the calculation, e.g., energy, forces, etc.</p> <p>The <code>workflow</code> section of the archive then stores information about the series of tasks performed to accumulate the (meta)data in the run section. The relevant input parameters for the workflow are stored in <code>method</code>, while the <code>results</code> section stores output from the workflow beyond observables of single configurations.</p> <p>I think this better summarizes the general rule. For example, any ensemble-averaged quantity from a molecular dynamics simulation would be stored under <code>workflow/results</code>. Then, the <code>inputs</code>, <code>outputs</code>, and <code>tasks</code> sections define the specifics of the workflow.</p> <p>For some standard workflows, e.g., geometry optimization and molecular dynamics, the NOMAD normalizers  will automatically populate these specifics. The parser must only create the appropriate workflow section. </p> <p>For non-standard workflows, the parser (or more appropriately the corresponding normalizer) must populate these sections accordingly. More information about the structure of the workflow section, as well as instructions on how to upload custom workflows to link individual Entries in NOMAD, can be found HERE</p>"},{"location":"writing_a_parser_plugin/computational/#recommended-parser-layout","title":"Recommended parser layout","text":"<p>The following represents the recommended core structure for a computational parser,  typically implemented within <code>&lt;parserproject&gt;/parser.py</code>.</p>"},{"location":"writing_a_parser_plugin/computational/#imports","title":"Imports","text":"<p>The imports typically include not only the necessary generic python modules, but also the required MetaInfo classes from nomad and additional nomad utilities:</p> <pre><code>&lt;license&gt;\nimport os\nimport numpy as np\nfrom nomad.datamodel.metainfo.simulation.system import System, Atoms\nfrom nomad.atomutils import get_molecules_from_bond_list\nfrom nomad.units import ureg\n</code></pre> <p>For example, above the classes <code>System</code> and <code>Atoms</code> are imported from the nomad MetaInfo definitions in order to appropriately build and populate the NOMAD archive. The <code>atomutils</code> NOMAD module contains many useful functions for processing computational data. Finally, the <code>UnitRegistry</code> module of <code>pint</code> (imported directly from NOMAD in this case), provides support to defining and converting units.</p>"},{"location":"writing_a_parser_plugin/computational/#parser-classes","title":"Parser Classes","text":"<pre><code>class &lt;Parsername&gt;&lt;Mainfiletype&gt;Parser(FileParser):\ndef __init__(self):\nsuper().__init__(None)\n@property\ndef file&lt;mainfiletype&gt;(self):\nif self._file_handler is None:\ntry:\nself._file_handler = &lt;openfilefunction&gt;(self.mainfile, 'rb')\nexcept Exception:\nself.logger.error('Error reading &lt;mainfiletype&gt; file.')\nreturn self._file_handler\nclass &lt;Parsername&gt;Parser:\ndef __init__(self):\nself.&lt;mainfiletype&gt;_parser = &lt;Parsername&gt;&lt;Mainfiletype&gt;Parser()\n</code></pre> <p>The main class for your parser, <code>&lt;Parsername&gt;Parser</code>, will contain the bulk of the parsing routine, described further below. It may be useful to create a distinct class, <code>&lt;Parsername&gt;&lt;Mainfiletype&gt;Parser</code>, for dealing with various filetypes that may be parsed throughout the entirety of the routine. However, in the simplest case of a single file type parsed, the entire parser can of course be implemented within a single class.</p> <p>In the following, we will walk through the layout of the <code>&lt;Parsername&gt;Parser</code> class. First, every parser class should have a \"main\" function called <code>parse()</code>, which will be called by NOMAD when the appropriate mainfile is found:</p> <pre><code>def parse(self, filepath, archive, logger):\nself.filepath = os.path.abspath(filepath)\nself.archive = archive\nself.logger = logging.getLogger(__name__) if logger is None else logger\nself._maindir = os.path.dirname(self.filepath)\nself._&lt;parsername&gt;_files = os.listdir(self._maindir)  # get the list of files in the same directory as the mainfile\nself._basename = os.path.basename(filepath).rsplit('.', 1)[0]\nself.init_parser()\nif self.&lt;mainfileparser&gt;_parser is None:\nreturn\nsec_run = self.archive.m_create(Run)\nsec_run.program = Program(name='&lt;PARSERNAME&gt;', version='unknown')\nself.parse_method()\nself.parse_system()\nself.parse_calculation()\nself.parse_workflow()\n</code></pre> <p>Then, the individual functions to populate that various MetaInfo sections can be defined:</p> <pre><code>def parse_calculation(self):\nsec_run = self.archive.run[-1]\nsec_calc = sec_run.m_create(Calculation)\n# populate calculation metainfo\n# ...\ndef parse_system(self, frame):\nsec_run = self.archive.run[-1]\nsec_system = sec_run.m_create(System)\nsec_atoms = sec_system.m_create(Atoms)\n# populate system metainfo\n# ...\ndef parse_method(self, frame):\nsec_method = self.archive.run[-1].m_create(Method)\nsec_force_field = sec_method.m_create(ForceField)\nsec_model = sec_force_field.m_create(Model)\n# populate method metainfo\n# ...\ndef parse_workflow(self):\nsec_workflow = self.archive.m_create(Workflow)  # for old workflow, should update for workflow2\n# populate workflow metainfo\n# ...\n</code></pre> <p>For more information, see Examples - populating the NOMAD archive .</p>"},{"location":"writing_a_parser_plugin/creating_new_metainfo/","title":"Creating new MetaInfo","text":""},{"location":"writing_a_parser_plugin/creating_new_metainfo/#nomad-metainfo","title":"NOMAD MetaInfo","text":""},{"location":"writing_a_parser_plugin/creating_new_metainfo/#code-specific-metainfo","title":"Code-specific MetaInfo","text":""},{"location":"writing_a_parser_plugin/parser_plugin_overview/","title":"How to write a parser","text":"<p>NOMAD uses parsers to convert raw data (for example, output from computational software, instruments, or electronic lab notebooks) into NOMAD's common Archive format. The following pages describe how to develop such a parser and integrate it within the NOMAD software. The goal is equip users with the required knowledge to contribute to and extend NOMAD.</p>"},{"location":"writing_a_parser_plugin/parser_plugin_overview/#getting-started","title":"Getting started","text":"<p>In principle, it is possible to develop a \"local parser\" that uses the nomad-lab package to parse raw data, without changing the NOMAD software itself. This allows a quick start for focusing on the parsing of the data itself, but is not relevant for full integration of your new parser into NOMAD. Here we are focused on developing parsers that will be integrated into the NOMAD software. For this, you will have to install a development version of NOMAD.</p>"},{"location":"writing_a_parser_plugin/parser_plugin_overview/#parser-organization","title":"Parser organization","text":"<p>The NOMAD parsers can be found within your local NOMAD git repo under <code>dependencies/parsers/</code>. The parsers are organized into the following individual projects (<code>dependencies/parsers/&lt;parserproject&gt;</code>) with their own corresponding repositories:</p> <ul> <li>atomistic - Parsers for output from classical molecular simulations, e.g., from Gromacs, Lammps, etc.</li> <li>database - Parsers for various databases, e.g., OpenKim.</li> <li>eelsdb - Parser for the EELS database (https://eelsdb.eu/; to be integrated in the database project).</li> <li>electronic - Parsers for output from electronic structure calculations, e.g., from Vasp, Fhiaims, etc. </li> <li>nexus - Parsers for combining various instrument output formats and electronic lab notebooks.</li> <li>workflow - Parsers for output from task managers and workflow schedulers.</li> </ul> <p>Within each project folder you will find a <code>test/</code> directory, containing the parser tests, and also a directory containing the parsers' source code, <code>&lt;parserproject&gt;parser</code> or <code>&lt;parserproject&gt;parsers</code>, depending on if one or more parsers are contained within the project, respectively. In the case of multiple parsers, the files for individual parsers are contained within a corresponding subdirectory: <code>&lt;parserproject&gt;parsers/&lt;parsername&gt;</code> For example, the Quantum Espresso parser files are found in <code>dependencies/parsers/electronic/electronicparsers/quantumespresso/</code>.</p>"},{"location":"writing_a_parser_plugin/parser_plugin_overview/#setting-up-your-development-branches","title":"Setting up your development branches","text":"<p>We will first focus on the case of adding a new parser to an existing parser project. Creating a new parser project will require a few extra steps.</p> <p>The existing parser projects are stored within their own git repositories and then linked to the NOMAD software. All current parser projects are available at nomad-coe (see also individual links above).</p> <p>You will first need to create new branches within both the NOMAD project and also within the corresponding parser project. Ideally, this should be done following the best practices for NOMAD development. Here, we briefly outline the procedure:</p> <p>Create a new issue within the NOMAD project at NOMAD gitlab. On the page of the new issue, in the top right, click the arrow next to the <code>Create merge request</code> button and select <code>Create branch</code>. The branch name should be automatically generated with the corresponding issue number and the title of the issue (copy the branch name to the clipboard for use below), and the default source branch should be <code>develop</code>. Click the <code>Create branch</code> button.</p> <p>Now, run the following commands in your local NOMAD directory:</p> <p><code>git fetch --all</code> \u00a0\u00a0\u00a0\u00a0 (to sync with remote)</p> <p><code>git checkout origin/&lt;new_branch_name&gt; -b &lt;new_branch_name&gt;</code> \u00a0\u00a0\u00a0\u00a0 (to checkout the new branch and create a local copy of the branch)</p> <p>Unless you just installed the NOMAD development version, you should rerun <code>./scripts/setup_dev_env.sh</code> within the NOMAD directory to reinstall with the newest development branch.</p> <p>Now we need to repeat this process for the parser project that we plan to extend. As above, create a new issue at the relevant parser project GitHub page. Using the identical issue title as you did for the NOMAD project above is ideal for clarity. On the page of the new issue, in the right sidebar under the subsection <code>Development</code>, click <code>Create a branch</code>. As above, the branch name should be automatically generated with the corresponding issue number and the title of the issue, and the default source branch should be <code>develop</code> (which can be seen by clicking <code>change source branch</code>). Under <code>What's next</code>, the default option should be <code>Checkout locally</code>, which is what we want in this case. Click <code>Create branch</code>, and then copy the provided commands to the clipboard, and run them within the parser project folder within your local NOMAD repo, i.e., <code>dependencies/parsers/&lt;parserproject&gt;</code>.</p>"},{"location":"writing_a_parser_plugin/parser_shell/","title":"Creating the framework for your parser","text":"<p>First, create a directory for your parser under the relevant parser project directory:</p> <pre><code>cd dependencies/parsers/&lt;parserproject&gt;\nmkdir &lt;parsername&gt;\n</code></pre> <p>In the following please note the naming conventions:</p> <ul> <li> <p>&lt;parsername&gt; - flat case, e.g., quantumespresso</p> </li> <li> <p>&lt;ParserName&gt; - Pascal case, e.g., QuantumEspresso</p> </li> <li> <p>&lt;parser_name&gt; - snake case, e.g., quantum_espresso</p> </li> <li> <p>&lt;PARSERNAME&gt; - upper flat case, e.g., QUANTUMESPRESSO</p> </li> </ul>"},{"location":"writing_a_parser_plugin/parser_shell/#parser-files","title":"Parser files","text":"<p>In the following, &lt;license&gt; represents the insertion of the following license agreement statement:</p> <pre><code>#\n# Copyright The NOMAD Authors.\n#\n# This file is part of NOMAD.\n# See https://nomad-lab.eu for further info.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n</code></pre> <p>The following are typical files found within a parser source directory:</p> <ul> <li> <p><code>__init__.py</code></p> <pre><code>&lt;license&gt;\nfrom .parser import &lt;ParserName&gt;Parser\n</code></pre> </li> <li> <p><code>__main__.py</code></p> <pre><code>&lt;license&gt;\nimport sys\nimport json\nimport logging\n\nfrom nomad.utils import configure_logging\nfrom nomad.datamodel import EntryArchive\nfrom atomisticparsers.&lt;parsername&gt; import &lt;ParserName&gt;Parser\n\nif __name__ == \"__main__\":\n    configure_logging(console_log_level=logging.DEBUG)\n    archive = EntryArchive()\n    &lt;ParserName&gt;Parser().parse(sys.argv[1], archive, logging)\n    json.dump(archive.m_to_dict(), sys.stdout, indent=2)\n</code></pre> </li> <li> <p><code>metainfo/__init__.py</code></p> <pre><code>&lt;license&gt;\nfrom nomad.metainfo import Environment\n\nfrom . import &lt;parsername&gt;\n\nm_env = Environment()\nm_env.m_add_sub_section(Environment.packages, &lt;parsername&gt;.m_package)\n</code></pre> </li> <li> <p><code>metainfo/&lt;parser_name&gt;.py</code> - contains code-specific metadata definitions.</p> </li> <li> <p><code>nomad_plugin.yaml</code></p> <pre><code>code_category: &lt;Parserproject&gt; code\ncode_homepage: &lt;&gt;\ncode_name: &lt;PARSERNAME&gt;\nmetadata:\ncodeCategory: &lt;Parserproject&gt; code\ncodeLabel: &lt;PARSERNAME&gt;\ncodeLabelStyle: All in capitals\ncodeName: &lt;parsername&gt;\ncodeUrl: &lt;&gt;\nparserDirName: dependencies/parsers/&lt;parserproject&gt;/&lt;parserproject&gt;parsers/parsername/\nparserGitUrl: https://github.com/nomad-coe/&lt;parserproject&gt;-parsers.git\nparserSpecific: ''\npreamble: ''\nstatus: production\ntableOfFiles: ''\nname: parsers/&lt;parsername&gt;\nparser_class_name: &lt;parserproject&gt;parsers.&lt;parsername&gt;.parser.&lt;ParserName&gt;Parser\npython_package: &lt;parserproject&gt;parsers.&lt;parsername&gt;\n</code></pre> </li> <li> <p><code>parser.py</code> - the parser source code.</p> </li> <li> <p><code>README.md</code> - a short description of the functionality of this parser.</p> </li> </ul>"},{"location":"writing_a_parser_plugin/parser_shell/#integration-into-nomads-parser-matching-interface","title":"Integration into NOMAD's parser-matching interface","text":"<p>The <code>nomad_plugin.yaml</code> file enables the parser to be recognized as a plugin to the NOMAD software. However, in order for NOMAD to identify that it should use this parser, we still need to add configuration details within NOMAD's parser-matching interface. These options are specified within the plugins options (<code>plugins = Plugins(options={}))</code>)  in the file <code>nomad/config/__init__.py</code>. There are many examples to follow here. In short, we need to create a dictionary key within <code>options</code> for our new parser:</p> <pre><code>'parsers/&lt;parsername&gt;': Parser(\n    python_package='&lt;parserproject&gt;parsers.&lt;parsername&gt;',\n    &lt;args&gt;\n    )\n</code></pre> <p>Here, <code>&lt;args&gt;</code> can be one or several of the following:</p> <ul> <li><code>mainfile_name_re=</code> - regex string for matching mainfile name</li> <li><code>mainfile_contents_re=</code> - regex string for matching mainfile contents</li> <li><code>mainfile_mime_re=</code> -</li> <li><code>supported_compressions=</code> -</li> <li><code>mainfile_alternative=</code> -</li> <li><code>mainfile_binary_header_re=</code> -</li> <li><code>mainfile_contents_dict={'__has_all_keys': ['&lt;key1&gt;', '&lt;key2&gt;'}),</code></li> <li><code>parser_as_interface=</code> -</li> </ul> <p>You can find further information about this topic here: Examples of parser-matching</p>"},{"location":"writing_a_parser_plugin/parser_tests/","title":"Creating parser tests","text":"<p>Each parser should have a series of associated (py)tests to ensure that future developments do not affect the intended parsing. These tests can be found under directly under  <code>&lt;parserproject&gt;/tests/</code> directory, labeled by the corresponding parser name: <code>test_parsername.py</code>.</p> <p>Here is an example template of such a <code>pytest</code> code: <pre><code>&lt;license&gt;\n\nimport pytest\nimport numpy as np\n\nfrom nomad.datamodel import EntryArchive\nfrom &lt;parserproject&gt;.&lt;parsername&gt; import ParserName\n\ndef approx(value, abs=0, rel=1e-6):\n    return pytest.approx(value, abs=abs, rel=rel)\n\n@pytest.fixture(scope='module')\ndef parser():\n    return &lt;ParserName&gt;()\n\ndef test_parser(parser):\n    archive = EntryArchive()\n    parser.parse(&lt;path_to_test_mainfile&gt;, archive, None)\n\n    sec_run = archive.run[0]\n\n    assert sec_run.program.name == '&lt;PARSERNAME&gt;'\n    assert sec_run.program.version == 'x.x.x'\n\n    sec_method = sec_run.method\n    assert len(sec_method) == 1\n    assert len(sec_method[0].force_field.model[0].contributions) == 1127\n    assert sec_method[0].force_field.model[0].contributions[0].type == 'angle'\n\n    sec_systems = sec_run.system\n    assert len(sec_systems) == 2\n    assert np.shape(sec_systems[0].atoms.positions) == (1516, 3)\n    assert sec_systems[1].atoms.positions[800][1].magnitude == approx(2.4740036e-09)\n    assert sec_systems[0].atoms.velocities[500][0].magnitude == approx(869.4773)\n    assert sec_systems[1].atoms.lattice_vectors[2][2].magnitude == approx(2.469158e-09)\n\n    sec_calc = sec_run.calculation\n    assert len(sec_calc) == 5\n    assert sec_calc[3].temperature.magnitude == approx(291.80401611328125)\n    assert sec_calc[0].energy.total.value.magnitude == approx(-1.1863129365544755e+31)\n\n    sec_workflow = archive.workflow2\n    assert sec_workflow.m_def.name == 'WorkflowName'\n</code></pre></p>"},{"location":"writing_a_parser_plugin/references/advanced_new_parser_project/","title":"Creating a new parser project","text":""},{"location":"writing_a_parser_plugin/references/examples_parser_matching/","title":"Examples of NOMAD's parser-matching interface","text":""},{"location":"writing_a_parser_plugin/references/examples_populating_archive/","title":"Examples of populating the NOMAD archive","text":""},{"location":"writing_a_parser_plugin/references/old_docs/","title":"How to write a parser","text":"<p>NOMAD uses parsers to convert raw code input and output files into NOMAD's common Archive format. This is documentation on how to develop such a parser.</p>"},{"location":"writing_a_parser_plugin/references/old_docs/#getting-started","title":"Getting started","text":"<p>Let's assume we need to write a new parser from scratch.</p> <p>First we need the install nomad-lab Python package to get the necessary libraries: <pre><code>pip install nomad-lab\n</code></pre></p> <p>We prepared an example parser project that you can work with. <pre><code>git clone https://github.com/nomad-coe/nomad-parser-example.git --branch hello-world\n</code></pre></p> <p>Alternatively, you can fork the example project on GitHub to create your own parser. Clone your fork accordingly.</p> <p>The project structure should be <pre><code>example/exampleparser/__init__.py\nexample/exampleparser/__main__.py\nexample/exampleparser/metainfo.py\nexample/exampleparser/parser.py\nexample/LICENSE.txt\nexample/README.md\nexample/setup.py\n</code></pre></p> <p>Next you should install your new parser with pip. The <code>-e</code> parameter installs the parser in development. This means you can change the sources without the need to re-install. <pre><code>cd example\npip install -e .\n</code></pre></p> <p>The main code file <code>exampleparser/parser.py</code> should look like this: <pre><code>class ExampleParser(MatchingParser):\ndef __init__(self):\nsuper().__init__(name='parsers/example', code_name='EXAMPLE')\ndef run(self, mainfile: str, archive: EntryArchive, logger):\n# Log a hello world, just to get us started. TODO remove from an actual parser.\nlogger.info('Hello World')\nrun = archive.m_create(Run)\nrun.program_name = 'EXAMPLE'\n</code></pre></p> <p>A parser is a simple program with a single class in it. The base class <code>MatchingParser</code> provides the necessary interface to NOMAD. We provide some basic information about our parser in the constructor. The main function <code>run</code> simply takes a filepath and empty archive as input. Now its up to you, to open the given file and populate the given archive accordingly. In the plain hello world, we simple create a log entry and populate the archive with a root section <code>Run</code> and set the program name to <code>EXAMPLE</code>.</p> <p>You can run the parser with the included <code>__main__.py</code>. It takes a file as argument and you can run it like this: <pre><code>python -m exampleparser tests/data/example.out\n</code></pre></p> <p>The output should show the log entry and the minimal archive with one <code>section_run</code> and the respective <code>program_name</code>. <pre><code>INFO     root                 2020-12-02T11:00:52 Hello World\n- nomad.release: devel\n- nomad.service: unknown nomad service\n{\n\"section_run\": [\n{\n\"program_name\": \"EXAMPLE\"\n}\n]\n}\n</code></pre></p>"},{"location":"writing_a_parser_plugin/references/old_docs/#parsing-test-files","title":"Parsing test files","text":"<p>Let's do some actual parsing. Here we demonstrate how to parse ASCII files with some structure information in it. As it is typically used by materials science codes.</p> <p>The on the <code>master</code> branch of the example project, we have a more 'realistic' example: <pre><code>git checkout master\n</code></pre></p> <p>This example imagines a potential code output that looks like this (<code>tests/data/example.out</code>): <pre><code>2020/05/15\n               *** super_code v2 ***\n\nsystem 1\n--------\nsites: H(1.23, 0, 0), H(-1.23, 0, 0), O(0, 0.33, 0)\nlatice: (0, 0, 0), (1, 0, 0), (1, 1, 0)\nenergy: 1.29372\n\n*** This was done with magic source                                ***\n***                                x\u00b042                            ***\n\nsystem 2\n--------\nsites: H(1.23, 0, 0), H(-1.23, 0, 0), O(0, 0.33, 0)\ncell: (0, 0, 0), (1, 0, 0), (1, 1, 0)\nenergy: 1.29372\n</code></pre></p> <p>There is some general information at the top and then a list of simulated systems with sites and lattice describing crystal structures, a computed energy value, an example for a code specific quantity from a 'magic source'.</p> <p>In order to convert the information  from this file into the archive, we first have to parse the necessary quantities: the date, system, energy, etc. The nomad-lab Python package provides a <code>text_parser</code> module for declarative text file parsing. You can define text file parsers like this: <pre><code>def str_to_sites(string):\nsym, pos = string.split('(')\npos = np.array(pos.split(')')[0].split(',')[:3], dtype=float)\nreturn sym, pos\ncalculation_parser = UnstructuredTextFileParser(quantities=[\nQuantity('sites', r'([A-Z]\\([\\d\\.\\, \\-]+\\))', str_operation=str_to_sites),\nQuantity(\nSystem.lattice_vectors,\nr'(?:latice|cell): \\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*\\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*\\((\\d)\\, (\\d), (\\d)\\)\\,?\\s*',\nrepeats=False),\nQuantity('energy', r'energy: (\\d\\.\\d+)'),\nQuantity('magic_source', r'done with magic source\\s*\\*{3}\\s*\\*{3}\\s*[^\\d]*(\\d+)', repeats=False)])\nmainfile_parser = UnstructuredTextFileParser(quantities=[\nQuantity('date', r'(\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d)', repeats=False),\nQuantity('program_version', r'super\\_code\\s*v(\\d+)\\s*', repeats=False),\nQuantity(\n'calculation', r'\\s*system \\d+([\\s\\S]+?energy: [\\d\\.]+)([\\s\\S]+\\*\\*\\*)*',\nsub_parser=calculation_parser,\nrepeats=True)\n])\n</code></pre></p> <p>The quantities to be parsed can be specified as a list of <code>Quantity</code> objects with a name and a regular expression (re) pattern. The matched value should be enclosed in a group(s) denoted by <code>(...)</code>. By default, the parser uses the findall method of <code>re</code>, hence overlap between matches is not tolerated. If overlap cannot be avoided, one should switch to the finditer method by passing findall=False to the parser. Multiple matches for the quantity are returned if repeats=True (default). The name, data type, shape and unit for the quantity can also intialized by passing a metainfo Quantity. An external function str_operation can be also be passed to perform more specific string operations on the matched value. A local parsing on a matched block can be carried out by nesting a sub_parser. This is also an instance of the <code>UnstructuredTextFileParser</code> with a list of quantities to parse. To access a parsed quantity, one can use the get method.</p> <p>We can apply these parser definitions like this: <pre><code>mainfile_parser.mainfile = mainfile\nmainfile_parser.parse()\n</code></pre></p> <p>This will populate the <code>mainfile_parser</code> object with parsed data and it can be accessed like a Python dict with quantity names as keys: <pre><code>run = archive.m_create(Run)\nrun.program_name = 'super_code'\nrun.program_version = str(mainfile_parser.get('program_version'))\ndate = datetime.datetime.strptime(\nmainfile_parser.get('date'),\n'%Y/%m/%d') - datetime.datetime(1970, 1, 1)\nrun.program_compilation_datetime = date.total_seconds()\nfor calculation in mainfile_parser.get('calculation'):\nsystem = run.m_create(System)\nsystem.lattice_vectors = calculation.get('lattice_vectors')\nsites = calculation.get('sites')\nsystem.atom_labels = [site[0] for site in sites]\nsystem.atom_positions = [site[1] for site in sites]\nscc = run.m_create(SCC)\nscc.single_configuration_calculation_to_system_ref = system\nscc.energy_total = calculation.get('energy') * units.eV\nscc.single_configuration_calculation_to_system_ref = system\nmagic_source = calculation.get('magic_source')\nif magic_source is not None:\nscc.x_example_magic_value = magic_source\n</code></pre></p> <p>You can still run the parse on the given example file: <pre><code>python -m exampleparser tests/data/example.out\n</code></pre></p> <p>Now you should get a more comprehensive archive with all the provided information from the <code>example.out</code> file.</p> <p>** TODO more examples an explanations for: unit conversion, logging, types, scalar, vectors, multi-line matrices **</p>"},{"location":"writing_a_parser_plugin/references/old_docs/#extending-the-metainfo","title":"Extending the Metainfo","text":"<p>The NOMAD Metainfo defines the schema of each archive. There are pre-defined schemas for all domains (e.g. <code>common_dft.py</code> for electron-structure codes; <code>common_ems.py</code> for experiment data, etc.). The sections <code>Run</code>, <code>System</code>, an single configuration calculations (<code>SCC</code>) in the example are taken fom <code>common_dft.py</code>. While this covers most data that is usually provide in code input/output files, some data is typically format specific and only applies to a certain code or method. For these cases, we allow to extend the Metainfo like this (<code>exampleparser/metainfo.py</code>): <pre><code># We extend the existing common definition of a section \"single configuration calculation\"\nclass ExampleSCC(SCC):\n# We alter the default base class behavior to add all definitions to the existing\n# base class instead of inheriting from the base class\nm_def = Section(extends_base_section=True)\n# We define an additional example quantity. Use the prefix x_&lt;parsername&gt;_ to denote\n# non common quantities.\nx_example_magic_value = Quantity(type=int, description='The magic value from a magic source.')\n</code></pre></p>"},{"location":"writing_a_parser_plugin/references/old_docs/#testing-a-parser","title":"Testing a parser","text":"<p>Until now, we simply run our parse on some example data and manually observed the output. To improve the parser quality and ease the further development, you should get into the habit of testing the parser.</p> <p>We use the Python unit test framework pytest: <pre><code>pip install pytest\n</code></pre></p> <p>A typical test, would take one example file, parse it, and make assertions about the output. <pre><code>def test_example():\nparser = rExampleParser()\narchive = EntryArchive()\nparser.run('tests/data/example.out', archive, logging)\nrun = archive.section_run[0]\nassert len(run.section_system) == 2\nassert len(run.section_single_configuration_calculation) == 2\nassert run.section_single_configuration_calculation[0].x_example_magic_value == 42\n</code></pre></p> <p>You can run all tests in the <code>tests</code> directory like this: <pre><code>pytest -svx tests\n</code></pre></p> <p>You should define individual test cases with example files that demonstrate certain features of the underlying code/format.</p>"},{"location":"writing_a_parser_plugin/references/old_docs/#structured-data-files-with-numpy","title":"Structured data files with numpy","text":"<p>TODO: examples</p> <p>The <code>DataTextParser</code> uses the numpy.loadtxt function to load an structured data file. The loaded data can be accessed from property data.</p>"},{"location":"writing_a_parser_plugin/references/old_docs/#xml-parser","title":"XML Parser","text":"<p>TODO: examples</p> <p>The <code>XMLParser</code> uses the ElementTree module to parse an xml file. The parse method of the parser takes in an xpath style key to access individual quantities. By default, automatic data type conversion is performed, which can be switched off by setting convert=False.</p>"},{"location":"writing_a_parser_plugin/references/old_docs/#add-the-parser-to-nomad","title":"Add the parser to NOMAD","text":"<p>NOMAD has to manage multiple parsers and during processing needs to decide what parsers to run on what files. To decide what parser is use, NOMAD processing relies on specific parser attributes.</p> <p>Consider the example, where we use the <code>MatchingParser</code> constructor to add additional attributes that determine for what files the parser is indented: <pre><code>class ExampleParser(MatchingParser):\ndef __init__(self):\nsuper().__init__(\nname='parsers/example', code_name='EXAMPLE', code_homepage='https://www.example.eu/',\nmainfile_mime_re=r'(application/.*)|(text/.*)',\nmainfile_contents_re=(r'^\\s*#\\s*This is example output'))\n</code></pre></p> <ul> <li><code>mainfile_mime_re</code>: A regular expression on the mime type of files. The parser is only run on files with matching mime type. The mime-type is guessed with libmagic.</li> <li><code>mainfile_contents_re</code>: A regular expression that is applied to the first 4k of a file. The parser is only run on files where this matches.</li> <li><code>mainfile_name_re</code>: A regular expression that can be used to match against the name and path of the file.</li> </ul> <p>Not all of these attributes have to be used. Those that are given must all match in order to use the parser on a file.</p> <p>The nomad infrastructure keep a list of parser objects (in <code>nomad/parsing/parsers.py::parsers</code>). These parser are considered in the order they appear in the list. The first matching parser is used to parse a given file.</p> <p>While each parser project should provide its own tests, a single example file should be added to the infrastructure parser tests (<code>tests/parsing/test_parsing.py</code>).</p> <p>Once the parser is added, it become also available through the command line interface and normalizers are applied as well: <pre><code>nomad parser tests/data/example.out\n</code></pre></p>"},{"location":"writing_a_parser_plugin/references/old_docs/#developing-an-existing-parser","title":"Developing an existing parser","text":"<p>To develop an existing parser, you should install all parsers: <pre><code>pip install nomad-lab[parsing]\n</code></pre></p> <p>Close the parser project on top: <pre><code>git clone &lt;parser-project-url&gt;\ncd &lt;parser-dir&gt;\n</code></pre></p> <p>Either remove the installed parser and pip install the cloned version: <pre><code>rm -rf &lt;path-to-your-python-env&gt;/lib/python3.7/site-packages/&lt;parser-module-name&gt;\npip install -e .\n</code></pre></p> <p>Or use <code>PYTHONPATH</code> so that the cloned code takes precedence over the installed code: <pre><code>PYTHONPATH=. nomad parser &lt;path-to-example-file&gt;\n</code></pre></p> <p>Alternatively, you can also do a full developer setup of the NOMAD infrastructure and develop the parser there.</p>"},{"location":"writing_a_parser_plugin/references/quick_NOMAD_best_practices/","title":"Quick guide for best practices for NOMAD development","text":""},{"location":"writing_a_parser_plugin/references/quick_installing_nomad_dev/","title":"Quick start to installing a development version of NOMAD","text":"<p>Detailed instructions can be found here: Developing NOMAD.</p> <p>The NOMAD repository is located on the MPCDF gitlab. To collaborate on the project, you will first need an invitation. You can send invitation requests to <code>fairmat@physik.hu-berlin.de</code>.</p> <p>Once you have access to MPCDF gitlab:</p> <p>Clone the NOMAD gitlab repo and navigate to the resulting directory:</p> <pre><code>git clone https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR.git nomad\ncd nomad/\n</code></pre> <p>You should be on the branch <code>origin/develop</code> by default. Create a virtual environment for your nomad installation:</p> <pre><code>pip install virtualenv\nvirtualenv -p `which python3` .pyenv\nsource .pyenv/bin/activate\n</code></pre> <p>Now install nomad:</p> <pre><code>./scripts/setup_dev_env.sh\n</code></pre>"},{"location":"writing_a_parser_plugin/references/quick_parser_setup/","title":"Quick start for setting up a local parser","text":""},{"location":"writing_a_parser_plugin/references/standard_workflows/","title":"Standard workflows in NOMAD","text":""}]}